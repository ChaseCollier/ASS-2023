# Thursday October 20, 2022 {.unnumbered}

**"Give someone a program, you frustrate them for a day; teach them how to program, you frustrate them for a lifetime."** - David Leinweber

Today

-   Spatial data as point patterns
-   Theoretical framework for analyzing/modeling point pattern data
-   Working with point pattern data using functions from the {spatstat} package
-   Quantifying the spatial intensity of events

## Spatial data as point patterns {.unnumbered}

We now turn our attention to analyzing and modeling point pattern data. We start with some theory, then learn how to work with functions from the {spatstat} package before focusing on estimating spatial intensity.

As humans we naturally seek patterns within a collection of events. A pattern that quickly catches our attention is clustering. Clusters of stars in the night sky were defined by the ancients as a constellation.

A cluster of events in a particular region begs for an explanation. Why do events occur more often in one region rather than another region?

Consider tornado reports in the state of Kansas. Let the start position of a tornado be an *event location*. And let the damage rating (EF scale) provide a *mark* on the event.

Here you consider only events since 2007 with marks of 1, 2, 3, 4, and 5. More about the damage rating scale is available here <https://en.wikipedia.org/wiki/Enhanced_Fujita_scale>

Import and filter the data accordingly.

```{r}
Torn.sf <- sf::st_read(dsn = here::here("data", "1950-2020-torn-initpoint")) |>
  dplyr::filter(st == "KS", 
                yr >= 2007,
                mag > 0) 
```

Create a map of the event locations using functions from the {tmap} package. The state border is obtained as a simple feature data frame. The polygon geometry is plotted first with `tm_borders()` then the event locations are plotted with the `tm_bubbles()` and `size = "mag"`.

```{r}
KS.sf <- USAboundaries::us_states(states = "Kansas")

tmap::tm_shape(KS.sf) +
   tmap::tm_borders(col = "grey70") +
tmap::tm_shape(Torn.sf) +
   tmap::tm_bubbles(size = "mag", 
                    col = "red",
                    alpha = .4,
                    title.size = "EF Rating") +
tmap::tm_layout(legend.position = c("left", "top"),
                legend.outside = TRUE)
```

Based on this map you ask: (1) Are certain areas of the state more likely to get a tornado? (2) Do tornadoes tend to cluster? (3) Are there places in the state that are safe from tornadoes?

These questions are similar but they are not identical. We will explore these questions about point pattern data.

To make things more precise, it is helpful to begin with some definitions.

-   Event: An occurrence of interest (e.g., tornado, accident, wildfire).

-   Event location: Location of event (e.g., latitude/longitude).

-   Point: Any location in the study area where an event *could* occur. Across the state of Kansas a tornado can occur anywhere. However, in a forest with a lake, the lake is a place where a tree can not occur.

Note: Event location is a particular point where an event *did* occur.

-   Point pattern data: A collection of observed (or simulated) event locations together with a domain of interest. The event locations may contain additional information about the event. For example, the age of a tree.

-   Domain: Study area that is often defined by data availability (e.g., state or county boundary) or by the extent of the events.

-   Complete spatial randomness: Or CSR (not to be confused with CRS--coordinate reference system) defines the situation where an event has an equal chance of occurring at any point in the domain regardless of other nearby events.

Under the situation of CSR you say the event locations have a uniform probability distribution across space. Note: uniform chance does not mean that the events have an ordered pattern (e.g., trees in an orchard).

Consider a set of event locations that are randomly distributed within the unit plane. First create two vectors containing the x and y coordinates, then create a data frame that includes the name of the sample, and then graph the locations using `ggplot()`.

```{r}
library(ggplot2)

x <- runif(n = 50, min = 0, max = 1)
y <- runif(n = 50, min = 0, max = 1)
df1 <- data.frame(x, y, name = "Point Pattern 1")
ggplot(data = df1, 
       mapping = aes(x, y)) +
  geom_point(size = 2)
```

The plot shows a sample from a spatial point pattern process. A *spatial point process* is a mechanism (physical or synthetic) for producing a set of event locations. The pattern of locations produced by the point process is described as CSR. There are areas with relatively few events and areas with relative many events.

Let's repeat this procedure to create three more samples. First combine them into a single data frame with the `rbind()` function and then plot a four-panel figure using the `facet_wrap()` function.

```{r}
df2 <- data.frame(x = runif(n = 30, min = 0, max = 1),
                  y = runif(n = 30, min = 0, max = 1),
                  name = "Point Pattern 2")
df3 <- data.frame(x = runif(n = 30, min = 0, max = 1),
                  y = runif(n = 30, min = 0, max = 1),
                  name = "Point Pattern 3")
df4 <- data.frame(x = runif(n = 30, min = 0, max = 1),
                  y = runif(n = 30, min = 0, max = 1),
                  name = "Point Pattern 4")
df <- rbind(df1, df2, df3, df4)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_point() +
  facet_wrap(~ name)
```

Groups of nearby events illustrate the fact that a certain amount of *clustering* will occur simply by chance (without cause). This chance cluster confounds visual attempts to assess whether clustering is causal or not.

Complete spatial randomness sits on a spectrum between regularity and clustered.

To illustrate this here you generate point pattern data that have *more* regularity than CSR and point pattern data that are *more* clustered than CSR. You do this using the `rMaternI()` and `rMaternClust()` functions from the {spatstat} package.

```{r}
m1 <- spatstat.random::rMaternI(kappa = 200, r = .04)
df1 <- data.frame(x = m1$x, y = m1$y, name = "Regularity Pattern 1")
m2 <- spatstat.random::rMaternI(kappa = 200, r = .04)
df2 <- data.frame(x = m2$x, y = m2$y, name = "Regularity Pattern 2")
m3 <- spatstat.random::rMatClust(kappa = 30, r = .15, mu = 4)
df3 <- data.frame(x = m3$x, y = m3$y, name = "Clustered Pattern 1")
m4 <- spatstat.random::rMatClust(kappa = 30, r = .15, mu = 4)
df4 <- data.frame(x = m4$x, y = m4$y, name = "Clustered Pattern 2")
df <- rbind(df1, df2, df3, df4)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_point() +
  facet_wrap(~ name)
```

The difference in the arrangement of events between the two pattern types (clustered in the top two panels) can be visually detected. For example, given an event in a regularity pattern, there appears to always be more distance to the nearest neighbor event.

But the difference in the arrangement of events between a CSR and regular process and the difference in the arrangement of events between a CSR and cluster process is not.

And spatial scale matters. A set of event locations can be regular on a small scale but clustered on a larger scale.

Probability models for spatial patterns motivate methods for detecting event clustering. A probability model generates a point pattern process. For example, you can think of crime as a point pattern process defined by location and influenced by environmental factors. The probability of a crime occurring at a particular location is the random variable and you can estimate the probability of a crime event at any location given factors that influence crime.

## Theoretical framework for analyzing/modeling point pattern data {.unnumbered}

More formally, a spatial point pattern process is a *stochastic* (statistical) process where event location is the random variable. A sample of the process is a collection of events generated under the probability model.

A spatial point process is said to be *stationary* if the statistical properties of the resulting events are invariant to translation. This means that the relationship between two events depends on (1) how much distance separates them and on (2) their relative orientation to each other but not on where the two events are located in the domain.

A spatial point process is said to be *isotropic* if the statistical properties of the resulting events are invariant to rotation. This means that the relationship between two events depends only on how much distance separates them. The inter-event distance between events is called the *spatial lag*.

The properties of *stationarity* and *isotropy* allow you to treat each region within the domain as *independent*. Assuming a stationary and isotropic point process, two event pairs that are separated by the same distance will have the same relatedness regardless of where the event pairs are located in the domain.

This is analogous to the assumption you make when you define a weights matrix for spatially aggregated data. The assumptions of stationarity and isotropy are starting points for modeling point pattern data.

The Poisson distribution defines a model for complete spatial randomness (CSR). A point process is said to be *homogeneous Poisson* under the following two criteria:

1.  The number of events (N) occurring within a finite domain A is a random variable described by a Poisson distribution with mean $\lambda$ times \|A\| for some positive constant $\lambda$, with \|A\| denoting the area of the domain, and

2.  The locations of the N events represent a sample where each point in A is *equally likely* to be chosen as an event location.

The first criteria of a Poisson distribution refers to a probability model describing the number of events. It expresses the probability of a given number of events occurring in a fixed interval of space (and time) when the events occur with a known constant rate.

The Poisson parameter (lambda) defines the *intensity* of the point process. Given a set of events, an estimate for the mean (rate) parameter of the Poisson distribution is given by the number of events divided by the domain area. The second criteria ensures the events are scattered about the domain without clustering or regularity.

A procedure to create a homogeneous Poisson point process follows from its definition. Step 1: Choose the number of events from a Poisson distribution with a mean that is proportional to the domain area. Step 2: Place each event within the domain with coordinates given by a *uniform distribution*.

For example, let area \|A\| = 1, and the rate of occurrence $\lambda$ = 20, then the code below generates a CSR set of events.

```{r}
lambda <- 20
N <- rpois(1, lambda)
x <- runif(N)
y <- runif(N)
df <- data.frame(x, y)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_point(size = 2) 
```

The set of events represents a sample from a homogeneous Poisson point process. The intensity of the events is specified first then the locations are placed uniformly inside the domain. The domain need not be regular. The actual number of events varies from one realization to the next. On average the number of events is 20 but it could vary between 10 and 35 or more.

This point pattern exhibits CSR by construction. However, you are typically in the opposite position. You *observe* a set of events (e.g., tornadoes on the Plains) and you want to know if the events are regular, clustered, or random.

The null hypothesis is CSR and you need a test statistic that will summarize the evidence against this hypothesis. The null models are simple so you can use Monte Carlo methods to generate samples and compare summary statistics from those samples with your observed data.

In some cases the homogeneous Poisson model is not restrictive enough. This means that you can easily reject the null hypothesis but not learn anything interesting about your data. For example, with health events (locations of people with heart disease) CSR is not an appropriate model because a null hypothesis that incidences are equally likely does not consider that people cluster (locations at risk are not uniform).

Each person may have the same risk of heart disease, for instance, regardless of location. But you expect more cases in areas with more people at risk. Clusters of cases in high population areas violate the CSR but not necessarily the constant risk hypothesis. The constant risk hypothesis requires the intensity of the spatial process be defined as a spatially varying function. That is, you define the intensity as $\lambda(s)$, where $s$ denotes location.

The intensity (density) function is a first-order property of point pattern data. If intensity varies (significantly) across the domain, the process generating the data is said to be heterogeneous. The intensity function describes the expected number of events at any location. Events might be independent of one another, but groups of events appear because of the changing intensity.

It helps to visualize these concepts. We do that with functions from the {spatstat} package.

## Working with point pattern data using functions from the spatstat package {.unnumbered}

The {spatstat} family of packages contain functions to analyze and model point pattern data. The individual packages have different namespace names beginning with `spatstat` so it is best to load the family of packages with the `library()` function before beginning.

```{r}
library(spatstat)
```

Point pattern data are defined in {spatstat} by an object of class `ppp` (for planar point pattern) which contains the spatial coordinates of the events (event locations), optional values attached to the events (called 'marks'), and a description of the domain or 'window' over which the events are observed. See `?ppp.object()` for details.

Spatial statistics computed on a `ppp` object will be sensitive to the choice of the window (domain), so some thought should go into the choice of the domain.

As an example, the data `swedishpines` is available in the package as a `ppp` object.

```{r}
class(swedishpines)
swedishpines
```

The data is a planar point pattern object with 71 events.

Note: Unfortunately events in a `ppp` object are called 'points' rather than events. This is in contrast to the terminology used by the theoreticians who define a point as a *potential* event not an *observed* event.

All the events are contained within a rectangle window of size 9.6 by 10 meters.

There is a `plot()` method for `ppp` objects that provides a quick view of the data and the domain window.

```{r}
swedishpines |>
  plot()
```

Events are plotted as open circles inside a bounding box. The plot is labeled with the name of the `ppp` object.

The function `convexhull()` from the {spatstat.geom} package as part of the {spatstat} family of packages creates a convex hull around the events. A convex hull defines the minimum-area convex polygon that contains all the events.

Here you create the hull and add it to the plot.

```{r}
swedishpines |>
  plot()
swedishpines |>
  convexhull() |>
  plot(add = TRUE)
```

The domain (window) for analysis and modeling should be somewhat larger than the convex hull. The function `ripras()` computes a spatial domain based on the event locations alone assuming the locations are independent and identically distributed.

Here you also overlay this polygon on the plot.

```{r}
swedishpines |>
  plot()
swedishpines |>
  convexhull() |>
  plot(add = TRUE)
swedishpines |>
  ripras() |>
  plot(add = TRUE, lty = "dotted")
```

The window can have an arbitrary shape. A rectangle, a polygon, a collection of polygons including holes, or a binary image (mask). A window can be stored as a separate object of class `owin`. See `?owin.object()` for details.

Each event may carry information called a 'mark'. A mark can be continuous (e.g. tree height) or discrete (tree species).

Multitype point patterns are point patterns in which the events have a discrete mark (e.g., tree species). The mark values are given in a vector of the same length as the vector of locations. That is, `marks[i]` is the mark attached to the location (`x[i]`, `y[i]`).

Consider the `ppp` object `demopat` from the {spatstat} package.

```{r}
demopat |>
 plot()
```

Here the domain is defined as an irregular concave polygon with a hole. The distinction between inside and outside is important for spatial statistics computed using the events.

You print the marks with the `marks()` function.

```{r}
demopat |>
  marks()
```

For a multitype pattern (where the marks are factors) you can use the `split()` function to separate the point pattern objects by mark type. Consider the Lansing Woods data set (`lansing`) with marks corresponding to tree species.

```{r}
data(lansing)
LW <- lansing

LW |>
  split() |>
  plot()
```

## Quantifying the spatial intensity of events {.unnumbered}

The average intensity of events is defined as the number of events per unit area of the domain. The `summary()` method applied to a `ppp` object gives the average intensity.

```{r}
swedishpines |>
  summary()
```

There are 71 events over a window area (spatial domain) of 9600 square units giving an average intensity of 71/9600 = .0074.

Average intensity depends on the number of events and on the domain area defined by the `owin` object.

The `owin` object is the first element labeled `window` in a `ppp` object.

```{r}
W0 <- swedishpines$window
class(W0)
```

The default window is a rectangle.

```{r}
plot(W0)
```

Here you change the window to that defined by the event locations using the method of Ripley-Rasson (`ripras()` function) and then make that the window for the Swedish pines by using the subset operator `[`.

```{r}
W1 <- swedishpines |>
  ripras()

SP2 <- swedishpines[W1]
```

Since the new window area is slightly larger than the default window the summary method shows that the spatial intensity is lower.

```{r}
SP2 |>
  summary()
```

The average intensity might not represent the intensity of events locally. We need a way to describe the expected number of events at any location of the region.

Counting the number of events in equal areas is one way. The quadrat method divides the domain into a grid of rectangular cells and the number of events in each cell is counted. Quadrat counting is done with the `quadratcount()` function.

```{r}
swedishpines |>
  quadratcount()
```

By default the function divides the data into a 5 x 5 grid of cells. The event count in each cell is produced. To change the default number of cells in x and y directions you use the `nx =` and `ny =` arguments.

```{r}
swedishpines |>
  quadratcount(nx = 2, 
               ny = 3)
```

The plot method applied to the results of the `quadratcount()` functions adds the counts to a plot. Here you add the counts and include the event locations.

```{r}
swedishpines |>
  quadratcount() |>
  plot()
swedishpines |>
  plot(pty = 19, col = "red", 
       add = TRUE, main = "")
```

Grid cell areas will not be all equal when the domain boundaries are irregular like with the `demopat` ppp object.

```{r}
demopat |>
  quadratcount() |>
  plot()
```

Areas near the borders are smaller than areas completely within the domain.

When the number of events is large, hexagon grid cells provide a useful alternative to rectangular grid cells.

The process is: (1) tessellate the domain by a regular grid of hexagons, (2) count the number of events in each hexagon, and (3) use a color ramp to display the events per hexagon.

As an example here you generate 20K values from the standard normal distribution for the x coordinate and the same number of values for the y coordinate. You then use the `hexbin()` function from the {hexbin} package and specify 10 bins in the x direction to count the number of events in each hexagon and assign the result to the object `hbin`.

```{r}
if(!require(hexbin)) install.packages(pkgs = "hexbin", repos = "http://cran.us.r-project.org")

x <- rnorm(20000)
y <- rnorm(20000)
hbin <- hexbin::hexbin(x, y, xbins = 10) 
str(hbin)
```

The {hexbin} package uses S4 data classes so the output is stored in slots. Use the `plot()` method to make a graph.

```{r}
hbin |>
  plot()
```

Hexagons have symmetric nearest neighbors (there is only rook contiguity). They have the most sides of any polygon that can tessellate the plane.

They are generally more efficient than rectangles at covering the events. In other words it takes fewer of them to cover the same number of events.

They are visually less biased for displaying local event intensity compared to squares/rectangles.

Here you generate a large number of random events in the two-dimensional plane. Use a normal distribution in the x-direction and a student t-distribution in the y-direction.

```{r}
set.seed(131)
x <- rnorm(7777)
y <- rt(7777, df = 3)
hexbin::hexbin(x, y, xbins = 25) |>
  plot()
```

The {ggplot2} package has the `stat_binhex()` function so that also can be used for displaying spatial intensity.

```{r}
df <- data.frame(x, y)
ggplot(data = df, 
       mapping = aes(x, y)) +
  stat_binhex()
```

Another way to quantify the spatial intensity is with kernel density estimation (KDE). Let $s_i$ for $i$ = 1...$n$ be event locations, then an estimate for the intensity of the events at any location $s$ is given by

$$
\hat \lambda (s) = \frac{1}{nh}\sum_{i=1}^nK\Big(\frac{s - s_i}{h}\Big)
$$

where $K()$ is the kernel function and $h > 0$ is a smoothing parameter called the bandwidth. Typically the kernel function is a Gaussian probability density function.

To help visualize KDE you generate 25 event locations (`el`) uniformly on the real number line representing a one-dimensional spatial domain between 0 and 1 and then use kernel density estimation to get a continuous intensity function. The density estimation is is done using the function `density()` and here you compare the intensity function for increasing bandwidths specified with the `bw =` argument.

```{r}
el <- runif(25)
dd1 <- density(el, bw = .025)
dd2 <- density(el, bw = .05)
dd3 <- density(el, bw = .1)
df <- data.frame(x = c(dd1$x, dd2$x, dd3$x), 
                 y = c(dd1$y, dd2$y, dd3$y),
                bw = c(rep("h = .025", 512), 
                       rep("h = .05", 512),
                       rep("h = .1", 512)))
df2 <- data.frame(x = el, y = 0)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_line() +
  facet_wrap(~ bw, nrow = 3) +
  geom_point(mapping = aes(x, y), 
             data = df2, 
             color = "red")
```

As the bandwidth increases the curve (black line) representing the local intensity becomes smoother. The intensity is estimated at every location, not just at the location of the event.

The density is a summation of the kernels with one kernel centered on top of each event location. Event locations are marked with a point along the x-axis and the kernel is a Gaussian probability density function. The kernel is placed on each event and the bandwidth specifies the distance between the inflection points of the kernel. The one-dimensional KDE extends to two (or more) dimensions.

Example: The distribution of trees in a tropical forest

The object `bei` is a planar point pattern object from the {spatstat} package containing the locations of trees in a tropical rain forest.

```{r}
bei |>
  summary()
```

There are 3604 events (trees) over an area of 500,000 square meters giving an average intensity of .0073 trees per unit area.

The distribution of trees is not uniform (heterogeneous) as can be seen with a plot

```{r}
bei |>
  plot()
```

The plot shows clusters of trees and large areas with few if any trees.

Elevation and elevation slope are factors associated with tree occurrence.

The point pattern data is accompanied by data (`bei.extra`) on elevation (`elev`) and slope of elevation (`grad`) across the region.

```{r}
bei.extra |>
  plot()
```

These data are stored as `im` (image) objects.

```{r}
class(bei.extra$elev)
```

The image object contains a list with 10 elements including the matrix of values (`v`).

```{r}
str(bei.extra$elev)
```

Specifying a spatial domain (window) allows focuses the analysis on a particular region. Suppose you want to model locations of a certain tree type, but only for trees located at elevations above 145 meters. The `levelset()` function creates a window from an image object using `thresh =` and `compare =` arguments.

```{r}
W <- levelset(bei.extra$elev,
              thresh = 145, 
              compare = ">")
class(W)
```

The result is an object of class `owin`. The plot method displays the window as a mask, which is the region in black.

```{r}
W |>
  plot()
```

You subset the `ppp` object by the window using the bracket operator (`[]`). Here you assign the reduced `ppp` object to `beiW` and then make a plot.

```{r}
beiW <- bei[W]

beiW |>
  plot()
```

Now the analysis window is white and the event locations are plotted on top.

As another example you create a window where altitude is lower than 145 m and slope exceeds .1 degrees. In this case you use the `solutionset()` function.

```{r}
V <- solutionset(bei.extra$elev <= 145 & 
                 bei.extra$grad > .1)
beiV <- bei[V]

beiV |>
  plot()
```

You compute the spatial intensity function over the domain with the `density()` method using the default Gaussian kernel and fixed bandwidth determined by the window size.

```{r}
beiV |>
  density() |>
  plot()
```

The units of intensity are events per unit area (here square meters). The intensity values are computed on a grid ($v$) and are returned as a pixel image.

There are over 16K cells that have a value of `NA` as a result of the masking by elevation and slope.

```{r}
den <- beiV |>
  density()

sum(is.na(den$v))
```

Summarizing: Spatial point patterns play a big role in the analysis of natural occurring systems where the spatial location of events is examined through the lens of statistics in an attempt to understand physical processes.

Some of the terminology and theory of point pattern data analysis was introduced in this lesson. Including the concept of complete spatial randomness (CSR) and the twin assumptions of stationarity and isotropy.

The {spatstat} family of packages provides a comprehensive set of functions for analyzing, plotting, and modeling point pattern data. The package requires the data be of spatial class `ppp`.

The typical work flow includes importing and munging data as a simple feature data frame and then converting the simple feature data frame to a `ppp` object for analysis and modeling. But it is sometimes convenient to do at least part of the data munging after conversion to a `ppp` object.

The first-order property of spatial point pattern data is spatial intensity. It is defined as the number of events per unit area of the domain. The spatial intensity can vary with domain area even if the number of events is constant. It can also vary across the domain. Quadrat counting and kernel density estimates are used to quantify the spatial varying intensity.
