# Tuesday April 18, 2023 {-}

**"Weeks of coding can save you hours of planning."** - Unknown

## Models for point pattern data {-}

Event interaction means that an event at one location changes the probability that another event will be nearby. Models for point pattern data can help you understand the processes that led to the event locations especially when event interaction is suspected

Cluster models can be derived by starting with a  Poisson specification. For example, you begin by describing a set of events $Y$ as homogeneous Poisson. Then each individual event $y_i$ in $Y$ is considered a 'parent' that produces 'offspring' events ($x_i$) according to some mechanism

The resulting set of offspring events forms a clustered point pattern data $X$. The model is homogeneous Poisson at an unobserved level $Y$ (latent level) but clustered at the level of the observations ($X$)

One example of this parent-child process is the Matérn cluster model. Parent events come from a homogeneous Poisson process with intensity $\kappa$ and each parent has a Poisson ($\mu$) number of offspring that are independent and identically distributed (iid) within a radius $r$ centered around the parent

For instance here you use the `rMatClust()` function from the {spatstat} package to produce a clustered `ppp` object. You set the intensity equal to 10 (`kappa = 10`) and use a disc radius of .1 units (`r = .1`) and an offspring rate equal to 5 (`mu = 5`)

```{r}
library(spatstat)

rMatClust(kappa = 10, 
          r = .1, 
          mu = 5) |>
  plot(main = "")
```

The result is a set of event locations and the process that produced them is described as _doubly Poisson_. You can vary $\kappa$, $r$, and $\mu$ to generate more or fewer events

Other clustered Poisson models include
- Thomas model: each cluster consists of a Poisson number of random events with each event having an isotropic Gaussian displacement from its parent
- Gauss-Poisson model: each cluster is either a single event or a pair of events. 
- Neyman-Scott model: the cluster mechanism is arbitrary

A Cox model is a homogeneous Poisson model with a random intensity function. Let $\Lambda(s)$ be a function with non-negative values defined at all points $s$ inside the domain. Then at each point let $X$ be a Poisson model with an intensity $\Lambda$. Then $X$ will be a sample from a Cox model

A Cox model is doubly Poisson since $\Lambda$ is generated from some distribution that allow only positive numbers and then, conditional on $\Lambda$, a homogeneous point process is generated

Following are two samples from a Cox point process

```{r}
set.seed(3042)
par(mfrow = c(1, 2))
for (i in 1:2){
  lambda <- rexp(n = 1, rate = 1/100)
  X <- rpoispp(lambda)
  plot(X)
}
par(mfrow = c(1, 1))
```

The statistical moments of Cox models are defined in terms of the moments of $\Lambda$. For instance, the intensity function of $X$ is $\lambda(s)$ = E[$\Lambda(s)$], where E[] is the expected value

Cox models are convenient for describing clustered point pattern data. A Cox model is over-dispersed relative to a Poisson model (i.e. the variance of the number of events falling in any region of size A, is greater than the mean number of events in those regions)

The Matérn cluster model and the Thomas models are types of Cox models. Another common type of Cox model is the log-Gaussian Cox processes (LGCP) model in which the logarithm of $\Lambda(s)$ is a Gaussian random function

If you have a way of generating samples from a random function $\Lambda$ of interest, then you can use the `rpoispp()` function to generate the Cox process. The intensity argument `lambda` of `rpoispp()` can be a function of x or y or a pixel image

Another way to generate clustered point pattern data is by 'thinning'. Thinning refers to deleting some of the events. With 'independent thinning' the fate of each event is independent of the fate of the other events. When independent thinning is applied to a homogeneous Poisson point pattern, the resulting point pattern consisting of the retained events is also Poisson

An example of this is Matérn's Model I model. Here a homogeneous Poisson model first generates a point pattern $Y$, then any event in $Y$ that lies closer than a distance $r$ from another event is deleted. This results in point pattern data whereby close neighbor events do not exist

```{r}
plot(rMaternI(kappa = 7, 
              r = .05), 
     main = "")

X <- rMaternI(kappa = 70, 
              r = .05)

X |>
  Kest() |>
  plot()
```

Changing $\kappa$ and $r$ will change the event intensity

The various spatial models for event locations can be described with math. For instance, expanding on the earlier notation you write that a homogeneous Poisson model with intensity $\lambda > 0$ has intensity

$$\lambda(s, x) = \lambda$$ where $s$ is any location in the window W and $x$ is the set of events

Then the inhomogeneous Poisson model has conditional intensity

$$\lambda(s, x) = \lambda(s)$$

The intensity $\lambda(s)$ depends on a spatial trend or on an explanatory variable

There is also a class of 'Markov' point process models that allow for clustering (or inhibition) due to event interaction. Markov refers to the fact that the interaction is limited to nearest neighbors. Said another way, a Markov point process generalizes a Poisson process in the case where events are pairwise dependent

A Markov process with parameters $\beta > 0$ and $0 < \gamma < \infty$ with interaction radius $r > 0$ has conditional intensity $\lambda(s, x)$ given by

$$
\lambda(s, x) = \beta \gamma^{t(s, x)}
$$

where $t(s, x)$ is the number of events that lie within a distance $r$ of location $s$

Three cases:
- If $\gamma = 1$, then $\lambda(s, x) = \beta$ No interaction between events,  $\beta$ can vary with $s$
- If $\gamma < 1$, then $\lambda(s, x) < \beta$. Events inhibit nearby events
- If $\gamma > 1$, then $\lambda(s, x) > \beta$. Events encourage nearby events

Note the distinction between the interaction term $\gamma$ and the trend term $\beta$. A similar distinction exists between autocorrelation $\rho$ and trend $\beta$ in spatial regression models

More generally, you write the logarithm of the conditional intensity $\log[\lambda(s, x)]$ as linear expression with two components

$$
\log\big[\lambda(s, x)\big] = \theta_1 B(s) + \theta_2 C(s, x)
$$

where the $\theta$'s are model parameters that need to be estimated

The term $B(s)$ depends only on location so it represents trend and explanatory variable (covariate) effects. It is the 'systematic component' of the model. The term $C(s, x)$ represents stochastic interactions (dependency) between events

## Fitting and interpreting inhibition models {-}

The {spatstat} family of packages contains functions for fitting statistical models to point pattern data. Models can include trend, explanatory variables, and event interactions of any order (not restricted to pairwise). Models are fit with the method of maximum likelihood and the method of minimum contrasts

The method of maximum likelihood estimates the probability of the empirical $K$ curve given the theoretical curve for various parameter values. Parameter values are chosen so as to maximize the likelihood of the empirical curve

The method of minimum contrasts derives a cost function as the difference between the theoretical and empirical $K$ curves. Parameter values for the theoretical curve are those that minimize this cost function

The `ppm()` function from {spatstat} is used to fit a spatial point pattern model. The syntax has the form `ppm(X, formula, interaction, ...)` where `X` is the point pattern object of class `ppp`, `formula` describes the systematic (trend and covariate) part of the model, and `interaction` describes the stochastic dependence between events (e.g., Matérn process)

Recall a plot the Swedish pine saplings. There was no indication of a trend (no systematic variation in the intensity of saplings)

```{r}
SP <- swedishpines

SP |>
  plot()

SP |>
  intensity()
```

There is no obvious spatial trend in the distribution of saplings and the average intensity is .0074 saplings per unit area

A plot of the Ripley's $K$ function indicated some regularity relative to CSR for distances between .5 and 1.2 meters

```{r}
SP |>
  Kest(correction = "iso") |>
  plot()
```

The red dashed line is the $K$ curve under CSR. The black line is the empirical curve. At lag distances of between 5 and 15 units the empirical curve is below the CSR curve indicating there are fewer events within other events at those scales than would be expected by chance

This suggests a process of between-event inhibition. A simple inhibition model is a Strauss process when the inhibition is constant with a fixed radius (r) around each event. The amount of inhibition ranges between zero (100% chance of a nearby event) to complete (0% chance of a nearby event). In the case of no inhibition the process is equivalent to a homogeneous Poisson process

If you assume the inhibition process is constant across the domain with a fixed interaction radius (r), then you can fit a Strauss model to the data. You use the `ppm()` function from the {spatstat} package and include the point pattern data as the first argument. You set the trend term to a constant (implying a stationary process) with the argument `trend = ~ 1` and the interaction radius to 10 units with the argument `interaction = Strauss(r = 10)`. Finally you use a border correction out to a distance of 10 units from the window with the `rbord =` argument

Save the output in the object called `model.in` (inhibition model)

```{r}
model.in <- ppm(SP, 
                trend = ~ 1, 
                interaction = Strauss(r = 10), 
                rbord = 10)
```

The value for `r` in the `Strauss()` function is based on our visual inspection of the plot of `Kest()`. A value is chosen to be the distance at which there is the largest departure from a CSR model

You inspect the model parameters by typing the object name

```{r}
model.in
```

The first-order term (`beta`) has a value of .0757. This is the intensity of the parent ('proposal') events. Beta exceeds the average intensity by a factor of ten

Recall the intensity of the events is obtained as

```{r}
SP |>
  intensity()
```

The interaction parameter (`gamma`) is .275. It is less than one, indicating an inhibition process. The logarithm of gamma, called the interaction coefficient (`Interaction`), is -1.29. Interaction coefficients less than zero imply inhibition

A table with the coefficients including the standard errors and uncertainty ranges is obtained with the `coef()` method

```{r}
model.in |>
  summary() |>
  coef()
```

The output includes the `Interaction` coefficient along with it's standard error (`S.E.`) and the associated 95% uncertainty interval. The ratio of the `Interaction` coefficient to its standard error is the `Zval`. A large z-value (in absolute magnitude) translates to a low $p$-value and a rejection of the null hypothesis of no interaction between events

Output also shows the value for the `(Intercept)` term. It is the logarithm of the beta value, so exp(-2.58) = .0757 is the intensity of the proposal events

You interpret the model output as follows. The process producing the spatial pattern of pine saplings is such that you should see .0757 saplings per unit area [unobserved (latent) rate] 

But because of event inhibition, where saplings nearby other saplings fail to grow, the number of saplings is reduced to .0074 per unit area. Thus the spatial pattern is suggestive of sibling-sibling interaction. Adults have many offspring, but only some survive due to limited resources

## Fitting and interpreting cluster models {-}

Let's compare this inhibition model with a cluster model for describing the Lansing Woods maple trees (in the `ppp` object called `lansing` from the {spatstat} package)

Start by extracting the events marked as `maple` and putting them in a separate `ppp` object called `MT`

```{r}
data(lansing)
summary(lansing)

MT <- lansing |>
  subset(marks == "maple") |>
  unmark()

summary(MT)
```

There are 514 maple trees over this square region (924 x 924 square feet).

Plots of tree locations and the local intensity function help you examine the first-order property of these data.

```{r}
MT |>
  density() |>
  plot()

plot(MT, add = TRUE)
```

There are maple trees across the southern and central parts of the study domain

A plot of the $G$ function summarizes the second-order properties under the assumption of no trend

```{r}
G.df <- MT |>
  Gest() |>
  as.data.frame() |>
  dplyr::filter(r < .033) |>
  dplyr::mutate(r = r * 924)

library(ggplot2)

ggplot(G.df, aes(x = r, y = km)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  geom_vline(xintercept = 18, lty = 'dashed') +
  xlab("Lag distance (ft)") + 
  ylab("G(r): Cumulative % of events within a distance r of another maple") +
  theme_minimal()
```

The plot provides evidence that the maple trees are clustered. The empirical curve is above the theoretical curve. About 74% of the maple trees are within 18 feet of another maple tree (vertical line). If the trees were arranged as CSR then only 49% of the trees would be within 18 feet of another maple

Is the clustering due to interaction or trends (or both)?

You start the modeling process by investigating event interaction using a stationary Strauss model with interaction radius of .019 units (18 ft)

```{r}
ppm(MT, 
    trend = ~ 1, 
    interaction = Strauss(r = .019))
```

Here the first order term beta is 345. It is the 'latent' rate (intensity) of maple trees per unit area. This rate is less than the 514 actual maple trees. The fitted interaction parameter (gamma) is 1.72. It is greater than one since the trees are clustered. The logarithm of gamma is positive at .545

The model is interpreted as follows. The process producing the maple trees is such that you expect to see about 345 maples. Because of clustering where maple trees are more likely in the vicinity of other maple trees, the number of maples increases to the observed 514 per unit area

Here the physical explanation could be event interaction. But it also could be inhibition with hickory trees. You can model this using a term for cross event type interaction

The Strauss process is for inhibition models. So although you use it here for diagnostics, you need to fit a cluster model (thus the `*** Model is not valid ***` warning)

For a cluster model the spatial intensity $$\lambda(s) = \kappa \mu(s)$$ where $\kappa$ is the average number of clusters and where $\mu(s)$ is the spatial varying cluster size (number events per cluster)

Cluster models are fit using the `kppm()` function from the {spatstat} package. Here you specify the cluster process with `clusters = "Thomas"`

That means each cluster consists of a Poisson number of maple trees and where each tree in the cluster is placed randomly about the 'parent' tree with intensity that varies inversely with distance from the parent as a Gaussian function

```{r}
( model.cl <- kppm(MT, 
                   trend = ~ 1,
                   clusters = "Thomas") )
```

Here $\kappa$ is 21.75 and $\bar \mu(s)$ (mean cluster size) is 23.6 trees. The product of kappa and the mean cluster size is the number of events. The cluster model describes a parent-child process. The number of parents is about 22. The distribution of the parents can be described as CSR. Each parent produces about 24 offspring distributed randomly about the location of the parent within a characteristic distance. Note: The physical process might be different from the statistical process used to describe it

The cluster scale parameter indicating the characteristic size (area units) of the clusters is $\sigma^2$ 

A `plot()` method verifies that the cluster process statistically 'explains' the spatial correlation

```{r}
plot(model.cl, 
     what = "statistic")
```

The model (black line) is very close to the cluster process line (red dashed line). Also note that it is far from the CSR model (green line)

The spatial scale of the clustering is visualized with the `what = "cluster"` argument

```{r}
plot(model.cl, 
     what = "cluster")
```

The color ramp is the spatial intensity (number of events per unit area) about an arbitrary single event revealing the spatial scale and extent of clustering

## Fitting and interpreting a log-Gaussian Cox (cluster) model {-}

The Thomas cluster model is a type of Cox process where the logarithm of the spatial intensity is a sample from a non-negative random variable. If the dispersion of events around other events is uniform rather than Gaussian than you fit a Matérn cluster model

A limitation of the Thomas and Matérn cluster models is that the samples are assumed to be _spatially independent_. Nearby locations might have different spatial intensities

If there is a systematic trend or covariate influence on the spatial intensity then you can include this in the model

A related but more flexible process is the log-Gaussian Cox process (LGCP). A LGCP has a hierarchical structure, where at the first level the events are assumed to be drawn from a Poisson distribution conditional on the intensity function, and at the second level the log of the intensity function is assumed to be drawn from a Gaussian process. That is, the log spatial intensity values are _spatially correlated_

The flexibility of the model arises from the Gaussian process prior specified over the log-intensity function. Given this hierarchical structure with a Gaussian process at the second level, fitting this model to observed spatial point pattern data is a computational challenge

One way is through the method of stochastic partial differential equations (SPDE), which involves a probability (Bayesian) framework to approximate posterior distributions

To see how this works and to get a glimpse of the Bayesian framework, here you consider a 1D space and you fit the model using functions from the {inlabru} and {INLA} packages

Example modified from <https://inlabru-org.github.io/inlabru/articles/web/1d_lgcp.html>

Install and make the packages available to this session

```{r}
library(inlabru)
library(INLA)
library(mgcv)
library(ggplot2)
```

Get the data to model from the {inlabru} package using the `data(Poisson2_1D)` function. The data are in the object `pts2`

```{r}
data(Poisson2_1D)

pts2 |>
  dplyr::glimpse()
```

The object `pts2` is a one column data frame with column name `x`

```{r}
pts2 |>
  range()
```

The values of `x` are strictly positive between .33 and 51

Plot the data as points along a horizontal line together with a histogram estimating the 1D spatial intensity. Here you choose about 20 bins across the range of values from 0 to 55

```{r}
ggplot(data = pts2) +
  geom_histogram(mapping = aes(x = x), 
                 binwidth = 55 / 20, 
                 boundary = 0, 
                 fill = NA, 
                 color = "black") +
  geom_point(mapping = aes(x = x), 
             y = 0, pch = "|", cex = 4) 
```

The histogram is a discrete version of the spatial intensity. It shows that events along the horizontal axis tend to be most common near the value of 20

Your goal is a smoothed estimate of the 1-D spatial intensity taking into account event clustering

First create a 1D mesh of 50 points (`length.out =`) across the range of values (from 0 to 55). The end points of the mesh are unconstrained by setting `boundary = "free"`. Assign the mesh to an object with name `mesh1D`

```{r}
x <- seq(from = 0, 
         to = 55, 
         length.out = 50)

mesh1D <- inla.mesh.1d(loc = x, 
                       boundary = "free")
```


Then specify the _prior_ spatial correlation as a Matérn cluster model

The first argument is the mesh onto which the model will be built and the next two arguments are the prior distributions for the range and standard deviation of the spatial correlation

This allows you to control the priors of the parameters by supplying information on the scale of the problem. What is a reasonable upper magnitude for the spatial effect and what is a reasonable lower scale at which the spatial effect can operate?

```{r}
Matern <- inla.spde2.pcmatern(mesh = mesh1D, 
                              prior.range = c(150, .75),
                              prior.sigma = c(.1, .75))
```

The argument `prior.range =` accepts a vector of length two with the first element the lag distance (range) of the spatial correlation and the second element the probability that the range will be less than that value. If the second value is `NA`, the value of the first element is used as a fixed range

The argument `prior.sigma =` accepts a vector of length two with the first element the marginal standard deviation of the spatial intensity and the second element the probability that the standard deviation will be greater than that value

Here you are non-committal on the range of spatial correlation so you specify a large distance (150) with a 75% chance that it will be less than that. That is you give a broad range to the prior

Values for `prior.range` and `prior.sigma` are called hyper-parameters

_Key idea_: In the frequentist context you are interested in the likelihood of the data given the model P(D | M). In a Bayesian context, you are interested in the probability of the model given the data P(M | D). This is called the posterior. You get from P(D | M) to P(M | D) by multiplying the likelihood by the priors P(M). This requires you to specify the priors

Next specify the full model and assign it to the object `f`

```{r}
f <- x ~ spde1D(x, model = Matern) + Intercept(1)
```

Next fit the model to the actual event locations in `pts2`. You use the log Gaussian Cox process `lgcp()` function from the {inlabru} package. The `domain =` argument specifies the 1D mesh as a list object

```{r}
model.lgcp <- lgcp(components = f, 
                   data = pts2, 
                   domain = list(x = mesh1D))
```

You look at the output posterior distributions of the model parameters using the `spde.posterior()` function. The function returns x and y values for a plot of the posterior probability density function (PDF) as a data frame, which you plot with the `plot.bru()` function (`plot()` method)

Start with the probability density function for the range parameter by specifying `what = "range"`

```{r}
spde.posterior(result = model.lgcp, 
               name = "spde1D", 
               what = "range") |>
  plot()
```

The prior range value was specified broadly but the posterior range is focused on values between 2.5 and 5. The output is better viewed on a logarithmic scale by specifying `what = "log.range"`

```{r}
spde.posterior(model.lgcp, 
               name = "spde1D", 
               what = "log.range") |>
  plot()
```

Next you plot the probability density function for the Matérn correlation component of the model

```{r}
spde.posterior(model.lgcp, 
               name = "spde1D", 
               what = "matern.correlation") |>
  plot()
```

The black line is the posterior median correlation as a function of lag distance. The maximum correlation of 1 at zero lag distance decays to .5 correlation out at a distance of about 20 units

You can get a feel for sensitivity to priors by specifying different priors and looking at these posterior plots. Always a good idea when fitting models using Bayesian methods

For example, change the prior range from 150 to 30 and refit the model. Compare the probability density function of the Matérn correlation

You predict on the 'response' scale [i.e. the intensity function $\lambda$(s)] as follows. First set up a data frame of explanatory values at which to predict (here `grid.df`). Then use the `predict()` method with `data = grid.df` and `formula = ~ exp(spde1D + Intercept)`. It takes a few seconds to make predictions at each grid point location

```{r}
grid.df <- data.frame(x = seq(from = 0, to = 55, by = 1)) 
pred.df <- predict(model.lgcp, 
                   data = grid.df, 
                   formula = ~ exp(spde1D + Intercept))
```

The output is a data frame containing the locations on the grid (`x`) and the corresponding summary statistics (mean, median, standard deviation, and quantiles) on the posterior predictions at those location

```{r}
pred.df |>
  dplyr::glimpse()
```

You pass this data frame to the `plot()` method to produce the following prediction plot using the grammar of graphics

```{r}
plot(pred.df, color = "red") +
  geom_point(data = pts2, 
             mapping = aes(x = x), 
             y = 0, pch = "|", cex = 2) +
  xlab("x") + 
  ylab("Spatial intensity\n number of events per unit interval") +
  theme_minimal()
```

The LGCP model provides a smoothed spatial intensity of the events and a 95% credible interval about the intensity at each grid location. The intensity values are the number of events per unit interval

How does this compare with the intensity function that generated the data? 

The function `lambda2_1D( ) `in the data object `Poission2_1D` calculates the true intensity that was used in simulating the data

To plot this function you make a data frame with x- and y-coordinates giving the true intensity function, $\lambda(s)$. Here you use 150 x-values to get a smooth plot

```{r}
xs <- seq(from = 0, to = 55, length = 150)
true.lambda <- data.frame(x = xs, 
                          y = lambda2_1D(xs))
```

Now plot the LGCP model predicted values together with the true intensity function

```{r}
plot(pred.df, color = "red") +
  geom_point(data = pts2, 
             mapping = aes(x = x), 
             y = 0, pch = "|", cex = 2) +
  geom_line(data = true.lambda, 
            mapping = aes(x, y)) +
  xlab("x") + 
  ylab("Spatial intensity") +
  theme_minimal()
```

The match is pretty good. Keep in mind that the data represents just one sample generated from the model

You can look at the goodness-of-fit of the model using the function `bincount( )`, which plots the 95% credible intervals in a set of bins along the x-axis together with the observed count in each bin

```{r}
bc <- bincount(
  result = model.lgcp,
  observations = pts2,
  breaks = seq(from = 0, to = max(pts2), length = 12),
  predictor = x ~ exp(spde1D + Intercept)
)

attributes(bc)$ggp
```

The credible intervals are shown as red rectangles, the mean fitted value as a short horizontal blue line, and the observed data as black points

Abundance is the integral of the intensity over the entire space. Here space is 1D and you estimate the abundance by integrating the predicted intensity over the range of x

Integration is done as a weighted sum of the intensities. The locations along the x axis and their weights are constructed using the `ipoints()` function

Here you create 50 equally-space integration points cover the 1D range. The weights are all equal to 55/100

```{r}
ips <- ipoints(c(0, 55), 100, name = "x")

head(ips)
```

Then compute the abundance over the entire domain with the `predict()` method

```{r}
( Lambda <- predict(model.lgcp, 
                    ips, 
                    ~ sum(weight * exp(spde1D + Intercept))) )
```

* `mean` is the posterior mean abundance
* `sd` is the estimated standard error of the posterior of the abundance
* `q0.025` and `q0.975` are the 95% credible interval bounds
* `q0.5` is the posterior median abundance

The mean number of events is just over 130 with a standard deviation of 11.5 events

Recall that the LGCP has a hierarchical structure, where at the first level the process is assumed Poisson conditional on the intensity function, and at the second level the log of the intensity function is assumed to be drawn from a Gaussian process

The above posterior values for the abundance takes into account only the variance due to the parameters of the intensity function (2nd level). It neglects the variance in the number of events, given the intensity function (first level)

To include both variances you need to modify the input to the `predict( )` method. You include a data frame that samples from a Poisson density for each value of the abundance (here `N = 50:250`)

```{r}
Nest <- predict(model.lgcp, 
                ips,
                ~ data.frame(N = 50:250,
                             dpois = dpois(50:250,
                             lambda = sum(weight * exp(spde1D + Intercept)))))
```

The result shows the same set of statistics as were calculated for `Lambda`, but here for every abundance value between 50 and 250, rather than for the posterior mean abundance alone.

```{r}
Nest |>
  head()
```

You compute the 95% prediction interval and the median with the `inla.qmarginal()` function

```{r}
inla.qmarginal(c(.025, .5, .975), 
               marginal = list(x = Nest$N, 
                               y = Nest$mean))
```

Now compare `Lambda` to `Nest` using a plot

First calculate the posterior distribution conditional on the mean of `Lambda`

```{r}
Nest$plugin_estimate <- dpois(Nest$N, 
                              lambda = Lambda$mean)
```

Then plot it together with the unconditional posterior distribution

```{r}
ggplot(data = Nest) +
  geom_line(aes(x = N, 
                y = mean, 
                color = "Posterior")) +
  geom_line(aes(x = N, 
                y = plugin_estimate, 
                color = "Plugin"))
```

The posterior distribution takes into account both levels of uncertainty so it is less peaked and broader than the plugin distribution

An example of LGCP model using SPDE in two-dimensions with real data (gorilla nesting sites) is available here <https://inlabru-org.github.io/inlabru/index.html>