# Tuesday November 29, 2022 {.unnumbered}

**"The problem of nonparametric estimation consists in estimation, from the observations, of an unknown function belonging to a sufficiently large class of functions."** - A.B. Tsybakov

## Comparing interpolation methods {-}

Let's start by reviewing the steps involved with statistical spatial interpolation.

Here you consider a data set of monthly average near-surface air temperatures during April across the Midwest. The data set is available on my website in the file `MidwestTemps.txt`.

Start by importing the data as a data frame.

```{r}
L <- "http://myweb.fsu.edu/jelsner/temp/data/MidwestTemps.txt"
t.df <- readr::read_table(L)
```

The data frame contains three columns. The first two are longitude (`lon`) and latitude (`lat`) and the third is average air temperatures (`temp`) in tens of 째F. These are the climate observations at specified locations and you want a continuous field of climate values across the domain.

Convert the data frame to a simple feature data frame by specifying which columns you want as coordinates (first X then Y).

```{r}
t.sf <- t.df |>
  sf::st_as_sf(coords = c("lon", "lat"),
               crs = 4326)
```

Next include the spatial coordinates as attributes (`X` and `Y`) in the simple feature data frame. They were removed as attributes by the `sf::st_as_sf()` function.

```{r}
t.sf$X <- t.df$lon
t.sf$Y <- t.df$lat
```

Check to see if there are duplicated coordinates.

```{r}
t.sf$geometry |>
  duplicated() |>
  any()
```

Plot the climatological temperatures at the observation locations on a map.

```{r}
sts <- USAboundaries::us_states()

tmap::tm_shape(t.sf) +
  tmap::tm_text(text = "temp", 
                size = .6) +
tmap::tm_shape(sts) +
  tmap::tm_borders() 
```

There is a clear trend in temperatures with the coolest air to the north. Besides this north-south trend, there appears to be some clustering (spatial autocorrelation) of the temperatures due to local variations.

Next, compute and plot the sample variogram (omni-directional) using the residuals after removing the trend. The trend term is specified in the formula as `temp ~ X + Y`.

```{r}
library(gstat)

t.v <- variogram(temp ~ X + Y, 
                 data = t.sf)
plot(t.v)
```

The sample variogram values confirm spatial autocorrelation as there is an increase in the semi-variance for increasing lag distance out to about 150 km.

Next, check for anisotropy. Specify four directions and compute the corresponding directional sample variograms.

```{r}
t.vd <- variogram(temp ~ X + Y, 
                  data = t.sf,
                  alpha = c(0, 45, 90, 135))
df <- t.vd |>
  as.data.frame() |>
  dplyr::mutate(direction = factor(dir.hor))

library(ggplot2)

ggplot(data = df, 
              mapping = aes(x = dist, y = gamma, color = direction)) + 
  geom_point() + 
  geom_smooth(alpha = .2) +
  scale_y_continuous(limits = c(0, NA)) +
  ylab(expression(paste("Variogram [", gamma,"(h)]"))) +
  xlab("Lag distance (h)") +
  theme_minimal()
```

The four sample variograms are all quite similar providing no strong evidence to reject the assumption of isotropy.

Next, fit a variogram model to the variogram sample. Plot the sample variogram again to eyeball initial estimates of the model parameters.

```{r}
plot(t.v)
```
Choose a nugget of .5, a partial sill of 2.5, and a range of 150.

Next set the initial parameters for an exponential model then fit the model. Plot the sample variogram and the variogram model.

```{r}
t.vmi <- vgm(model = "Exp", 
             psill = 2.5, 
             range = 150, 
             nugget = .5)
t.vmi

t.vm <- fit.variogram(object = t.v, 
                      model = t.vmi)
t.vm

plot(t.v, t.vm)
```

Next, make a grid of locations at which values will be interpolated. Add the coordinates as attributes to the resulting `sfc`.

```{r}
grid.sfc <- sf::st_make_grid(t.sf,
                             n = c(100, 100),
                             what = "centers")
XY <- grid.sfc |>
  sf::st_coordinates()

grid.sf <- grid.sfc |>
  sf::st_as_sf() |>
  dplyr::mutate(X = XY[, 1],
                Y = XY[, 2])
```

Next, interpolate the observed temperatures to the grid locations using the method of universal kriging.

```{r}
t.int <- krige(temp ~ X + Y,
               locations = t.sf,
               newdata = grid.sf,
               model = t.vm)
```

The interpolated values at the grid locations are returned in the simple feature data frame you assigned as `t.int`. Take a glimpse of the the contents of the file.

```{r}
t.int |>
  dplyr::glimpse()
```

There are 10,000 rows (100 by 100 grid locations). The first column labeled `var1.pred` contains the interpolated temperatures. The second column contains the variance of the interpolated temperatures and the third column is the simple feature column.

The trend term captures the north-south temperature gradient and the variogram captures the local spatial autocorrelation. Together they make up the interpolated values.

To see this, you refit the interpolation first without the variogram model and second without the trend.

First, rename the columns.

```{r}
t.int <- t.int |>
  dplyr::rename(uk.pred = var1.pred,
                uk.var = var1.var)
```

Next, use the `krige()` function but do not include the `model = ` argument.

```{r}
t.trend <- krige(temp ~ X + Y,
                 locations = t.sf,
                 newdata = grid.sf) 
```
Add the interpolated temperature trend (located in `t.trend$var1.pred`) to the `t.int` simple feature data frame.

```{r}
t.int <- t.int |>
  dplyr::mutate(trend.pred = t.trend$var1.pred)
```

Next, again use the `krige()` function but do not include the trend term. That is interpolate using ordinary kriging.

```{r}
t.ok <- krige(temp ~ 1,
              locations = t.sf,
              newdata = grid.sf,
              model = t.vm)
```
Again add the interpolated temperatures from ordinary kriging to the `t.int` simple feature data frame.

```{r}
t.int <- t.int |>
  dplyr::mutate(ok.pred = t.ok$var1.pred)
```

Now we have three interpolations of the temperatures in the `t.int` simple feature data frame all labeled with `.pred`.

```{r}
t.int |>
  dplyr::glimpse()
```
Map the interpolations

```{r}
tmap::tm_shape(t.int) +
  tmap::tm_dots(title = "째F",
                shape = 15, 
                size = 2,
                col = c("uk.pred", "trend.pred", "ok.pred"), 
                n = 9, 
                palette = "OrRd") +
tmap::tm_shape(sts) +
  tmap::tm_borders() +
tmap::tm_shape(t.sf) +
  tmap::tm_text("temp", 
                col = "white", 
                size = .5) +
tmap::tm_layout(legend.outside = TRUE,
                legend.outside.position = "bottom")
```

The trend term (middle panel) captures the north-south temperature gradient and ordinary kriging (right panel) captures the local spatial autocorrelation. Together they make the universal kriging (left panel) interpolated surface.

The pattern obtained with ordinary kriging is similar to that obtained using inverse distance weighting. 

Inverse distance weighting (IDW) is a deterministic method for interpolation. The values assigned to locations are calculated with a weighted average of the values available at the observed locations. The weights are proportional to the inverse of the distance to each location.

The function `krige()` performs IDW when there is no trend term and no variogram model given as arguments to the function.

```{r}
t.idw <- krige(temp ~ 1,
               locations = t.sf,
               newdata = grid.sf) 
```

The IDW interpolation is not statistical so there is no estimate of the uncertainty on the interpolated values. This shows up as `NA` values in the `var1.pred` column.

```{r}
t.idw |>
  dplyr::glimpse()
```
Put the IDW interpolated values into the `t.int` simple feature data frame and compare them to the universal kriging interpolated values on a map.

```{r}
t.int <- t.int |>
  dplyr::mutate(idw.pred = t.idw$var1.pred)

tmap::tm_shape(t.int) +
  tmap::tm_dots(title = "째F",
                shape = 15, 
                size = 2,
                col = c("uk.pred", "idw.pred"), 
                n = 9, 
                palette = "OrRd") +
tmap::tm_shape(sts) +
  tmap::tm_borders() +
tmap::tm_layout(legend.outside = TRUE)
```

IDW tends to create more 'bulls-eye' patterns in the interpolations compared with universal kriging. It also tends to over smooth at the larger scales.

```{r}
t.int <- t.int |>
  dplyr::mutate(diff.pred = idw.pred - uk.pred)

tmap::tm_shape(t.int) +
  tmap::tm_dots(title = "째F",
                shape = 15, 
                size = 2,
                col = "diff.pred", 
                n = 9, 
                palette = "BrBG") +
tmap::tm_shape(sts) +
  tmap::tm_borders()
```
Relative to universal kriging, IDW over estimates the temperatures in the coldest regions and under estimates the temperatures in the warmest regions. At the largest scales IDW is too smooth and at the smallest scales it is too coarse.

By taking into account to different models (trend at the largest scale and autocorrelation at the smallest scales) universal kriging produces a 'goldilocks' surface.

Finally, simple kriging is ordinary kriging with a known mean value. This is done by specifying a value for the `beta =` argument. Here you specify the average value over all observed temperatures.

```{r}
krige(temp ~ 1,
      beta = mean(t.sf$temp),
      locations = t.sf,
      newdata = grid.sf,
      model = t.vm)
```

## Evaluating the accuracy of the interpolation {-}

How do you evaluate how good the interpolated surface is? If you use the variogram model to predict at the observation locations, you will get the observed values back when the nugget is zero.

For example, here you interpolate to the observation locations by setting `newdata = t.sf` instead of `grid.sf`. You then compute the correlation between the interpolated value and the observed value.

```{r}
t.int2 <- krige(temp ~ X + Y,
                locations = t.sf,
                newdata = t.sf,
                model = t.vm)

cor(t.int2$var1.pred, t.sf$temp)
```

So this is not helpful. 

Instead you use cross validation. Cross validation is a procedure for assessing how well a model does at interpolating the values when observations specific to the interpolation are removed from the model fitting procedure. 

Cross validation partitions the data into two disjoint subsets and the model is fit to one subset of the data (training set) and the model is validated on the other subset (testing set).

Leave-one-out cross validation (LOOCV) uses all but one observation for fitting and the left-out observation for testing. The procedure is repeated with every observation taking turns being left out. 

```{r}
krige.cv(temp ~ X + Y,
         locations = t.sf,
         model = t.vm) |>
  sf::st_drop_geometry() |>
  dplyr::summarize(r = cor(var1.pred, observed),
                   rmse = sqrt(mean((var1.pred - observed)^2)),
                   mae = mean(abs(var1.pred - observed)))

krige.cv(temp ~ 1,
         locations = t.sf,
         model = t.vm) |>
  sf::st_drop_geometry() |>
  dplyr::summarize(r = cor(var1.pred, observed),
                   rmse = sqrt(mean((var1.pred - observed)^2)),
                   mae = mean(abs(var1.pred - observed)))

krige.cv(temp ~ X + Y,
         locations = t.sf) |>
  sf::st_drop_geometry() |>
  dplyr::summarize(r = cor(var1.pred, observed),
                   rmse = sqrt(mean((var1.pred - observed)^2)),
                   mae = mean(abs(var1.pred - observed)))

krige.cv(temp ~ 1,
         locations = t.sf) |>
  sf::st_drop_geometry() |>
  dplyr::summarize(r = cor(var1.pred, observed),
                   rmse = sqrt(mean((var1.pred - observed)^2)),
                   mae = mean(abs(var1.pred - observed)))
```

All four interpolations result in high correlation between observed and interpolated values that exceed .9 and root-mean-squared errors (RMSE) less than 1.8. But the universal kriging interpolation gives the highest correlation and the lowest RMSE and mean-absolute errors.

For a visual representation of the goodness of fit you plot the observed versus interpolated values from the cross validation procedure.

```{r}
krige.cv(temp ~ X + Y,
               locations = t.sf,
               model = t.vm) |>
  dplyr::rename(interpolated = var1.pred) |>
ggplot(mapping = aes(x = observed, y = interpolated)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  geom_smooth(method = lm, color = "red") +
  ylab("Interpolated temperatures (째F)") +
  xlab("Observed temperatures (째F)") +
  theme_minimal()
```

The black line represents a perfect prediction and the red line is the best fit line when you regress the interpolated temperatures onto the observed temperatures. The fact that the two lines nearly coincide indicates the interpolation is good.

The `nfold =` argument, which by default is set to the number of observations and does a LOOCV, allows you to divide the data into different size folds (instead of N-1).

Note that these performance metrics are biased toward the sample of data because cross validation is done only on the interpolation (kriging) and not on the variogram model fitting.

That is, with kriging the data is used in two ways (1) to fit the variogram model, and (2) to interpolate the values. 

To perform a full LOOCV you need to refit the variogram after removing the observation for which you want the interpolation.
```{r, eval=FALSE}
vmi <- vgm(model = "Sph", 
           psill = 2, 
           range = 200, 
           nugget = 1)
int <- NULL
for(i in 1:nrow(t.sf)){
  t <- t.sf[-i, ]
  v <- variogram(temp ~ X + Y, 
                 data = t)
  vm <- fit.variogram(object = v, 
                      model = vmi)
  int[i] <- krige(temp ~ X + Y,
                  locations = t,
                  newdata = t[i, ],
                  model = vm)$var1.pred
}
```

The interpolation error using full cross validation will always be larger than the interpolation error using a fixed variogram model.

## Block cross validation {-}

One final note about cross validation in the context of spatial data is the observations are not independent. As such it is better to create spatial areas for training separate from the spatial areas for testing.

A nice introduction to so-called 'block' cross validation in the context of species distribution modeling is available here
<https://cran.r-project.org/web/packages/blockCV/vignettes/BlockCV_for_SDM.html>