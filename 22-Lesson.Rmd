# Thursday November 3, 2022 {.unnumbered}

Today

- Evaluating the accuracy of the interpolated surface 


## Evaluating the accuracy of the interpolated surface {-}

Interpolating surface air temperatures across the Midwest. The data in `MidwestTemps.txt` are average temperatures in and around the state of Iowa for the month of April.

Start by examining the data for spatial trends.

```{r}
L <- "http://myweb.fsu.edu/jelsner/temp/data/MidwestTemps.txt"
t.sf <- readr::read_table(L, show_col_types = FALSE) |>
  sf::st_as_sf(coords = c("lon", "lat"),
               crs = 4326)

XY <- t.sf |>
  sf::st_coordinates() 
t.sf$X <- XY[, 1]
t.sf$Y <- XY[, 2]

t.sf$geometry |>
  duplicated() |>
  any()
```

Plot the values on a map.

```{r}
sts <- USAboundaries::us_states()

tmap::tm_shape(t.sf) +
  tmap::tm_text(text = "temp", 
                size = .6) +
tmap::tm_shape(sts) +
  tmap::tm_borders() 
```

There is a clear trend in temperatures with the coolest values to the north Besides the trend there is likely some local spatial autocorrelation.

Compute and plot the empirical variogram on the trend residuals.

```{r}
t.v <- variogram(temp ~ X + Y, 
                 data = t.sf)
plot(t.v)
```

Check for anisotropy. Specify four directions and compute the corresponding directional variograms.

```{r}
t.vd <- variogram(temp ~ X + Y, 
                  data = t.sf,
                  alpha = c(0, 45, 90, 135))
df <- t.vd |>
  as.data.frame() |>
  dplyr::mutate(direction = factor(dir.hor))

ggplot(data = df, 
              mapping = aes(x = dist, y = gamma, color = direction)) + 
  geom_point() + 
  geom_smooth(alpha = .2) +
  scale_y_continuous(limits = c(0, NA)) +
  ylab(expression(paste("Variogram [", gamma,"(h)]"))) +
  xlab("Lag distance (h)") +
  theme_minimal()
```

There is no strong evidence to reject the assumption of isotropy.

Use the `autofitVariogram()` function to get initial estimates.

```{r}
t.sp <- as(t.sf, "Spatial")
t.vm <- automap::autofitVariogram(formula = temp ~ X + Y, 
                                     input_data = t.sp)
plot(t.vm)
```

Set the initial parameters for a Gaussian model then fit the model.

```{r}
t.vmi <- vgm(model = "Gau", 
             psill = 2, 
             range = 100, 
             nugget = 1)
t.vmi

t.vm <- fit.variogram(object = t.v, 
                      model = t.vmi)
t.vm

plot(t.v, t.vm)
```

Make a grid for the interpolated values and add the coordinates as attributes.

```{r}
grid.sfc <- sf::st_make_grid(t.sf,
                             n = c(100, 100),
                             what = "centers")
XY <- grid.sfc |>
  sf::st_coordinates() 
grid.sf <- grid.sfc |>
  sf::st_as_sf() |>
  dplyr::mutate(X = XY[, 1],
                Y = XY[, 2])
```

Interpolate with universal kriging.

```{r}
t.int <- krige(temp ~ X + Y,
               locations = t.sf,
               newdata = grid.sf,
               model = t.vm)
```

Map the output.

```{r}
tmap::tm_shape(t.int) +
  tmap::tm_dots(title = "Â°F",
                shape = 15, 
                size = 2,
                col = "var1.pred", 
                n = 9, 
                palette = "OrRd") +
tmap::tm_shape(sts) +
  tmap::tm_borders() +
tmap::tm_shape(t.sf) +
  tmap::tm_text("temp", 
                col = "white", 
                size = .5) +
tmap::tm_layout(legend.outside = TRUE)
```

The trend captures the large scale feature while the variogram captures the local spatial autocorrelation

How do we evaluate how good the interpolated surface is? If we use the variogram model to predict at the observation locations, we will get the observed values back (when the nugget is fixed at zero). So this is not helpful. Instead we use cross validation.

Cross validation is a procedure for assessing how well a model will do at predicting values when observations specific to the prediction are removed. The procedure first partitions the data into disjoint subsets. The model is then fit to one subset of the data (training set) and the model is validated on a different subset (testing set). 

Leave-one-out cross validation uses all but one observation for fitting and the left-out observation for testing. The procedure is repeated with every observation taking turns being left out. 

K-fold cross validation uses K observations for fitting and N-K for testing. With large K there are many ways to slice the sample so the procedure is not exhaustive like hold-one-out. 

With kriging, the data is used in two ways (1) to fit the variogram model, and (2) to interpolate the values. Thus cross validation has two cases: weak and strong. Weak cross validation uses the entire dataset to estimate the variogram model. Then kriging is performed N times using a leave-one-out strategy with the predicted value saved only for the observation left out.

The `xvalid()` function from the {geoR} package computes the cross-validated prediction error of the Wolfcamp aquifer interpolation in this weak sense.
```{r}
xv.wk <- xvalid(wca.gdf, 
                model = wca.vm)
df <- data.frame(observed = xv.wk$data, 
                 predicted = xv.wk$predicted)

ggplot(df, aes(x = observed, y = predicted)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  geom_smooth(method = lm, color = "red") +
  ylab("Predicted head heights (ft)") +
  xlab("Observed head heights (ft)") +
  ggtitle("Weak Cross Validation") +
  theme_minimal()
```

The black line represents a perfect prediction and the red line is the best fit line when we regress the predicted head heights onto the observed head heights. The fact that the two lines nearly coincide indicates the interpolation is good.

We quantify how good using the mean squared error and mean absolute error as follows.
```{r}
mean(xv.wk$error^2)
mean(abs(xv.wk$error))
```

The mean squared cross-validated prediction error is 30636 ft^2 and the mean absolute cross-validated prediction error is 137 ft.

In contrast to weak cross validation, strong cross validation requires that the variogram be re-estimated each time an observation is removed. The model must be fit using the `variofit()` function and the call must include the empirical variogram object. This is done with the argument `reestimate = TRUE`.
```{r}
wca.vm2 <- variofit(wca.v, 
                    ini = c(33000, .8),
                    nug = 10000, 
                    cov.model = "spherical")

xv.st <- xvalid(wca.gdf, 
                model = wca.vm2,
                variog.obj = wca.v,
                reestimate = TRUE)
df <- data.frame(observed = xv.st$data, 
                 predicted = xv.st$predicted)

ggplot(df, aes(x = observed, y = predicted)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  geom_smooth(method = lm, color = "red") +
  ylab("Predicted head heights (ft)") +
  xlab("Observed head heights (ft)") +
  ggtitle("Strong Cross Validation") +
  theme_minimal()

mean(xv.st$error^2)
mean(abs(xv.st$error))
```

The prediction error estimated using the procedure of strong cross validation will be larger than the prediction error estimated using the procedure of weak cross validation.