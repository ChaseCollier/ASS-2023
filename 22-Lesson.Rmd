# Thursday November 3, 2022 {.unnumbered}

Today

- Comparing interpolation methods
- Evaluating the accuracy of the interpolation

## Comparing interplation methods {-}

Here you consider a data set of monthly average surface air temperatures for April across the Midwest. The data are available on my website in the file `MidwestTemps.txt`.

Start by examining the data for spatial trends.

```{r}
L <- "http://myweb.fsu.edu/jelsner/temp/data/MidwestTemps.txt"
t.sf <- readr::read_table(L, show_col_types = FALSE) |>
  sf::st_as_sf(coords = c("lon", "lat"),
               crs = 4326)

XY <- t.sf |>
  sf::st_coordinates() 
t.sf$X <- XY[, 1]
t.sf$Y <- XY[, 2]

t.sf$geometry |>
  duplicated() |>
  any()
```

Plot the values on a map.

```{r}
sts <- USAboundaries::us_states()

tmap::tm_shape(t.sf) +
  tmap::tm_text(text = "temp", 
                size = .6) +
tmap::tm_shape(sts) +
  tmap::tm_borders() 
```

There is a clear trend in the temperature field with the coolest values to the north. Besides the trend there is some local clustering of similar values (spatial autocorrelation).

Compute and plot the empirical variogram on the residuals after removing the trend. The trend term is specified in the formula as `temp ~ X + Y`.

```{r}
t.v <- variogram(temp ~ X + Y, 
                 data = t.sf)
plot(t.v)
```

Check for anisotropy. Specify four directions and compute the corresponding directional variograms.

```{r}
t.vd <- variogram(temp ~ X + Y, 
                  data = t.sf,
                  alpha = c(0, 45, 90, 135))
df <- t.vd |>
  as.data.frame() |>
  dplyr::mutate(direction = factor(dir.hor))

ggplot(data = df, 
              mapping = aes(x = dist, y = gamma, color = direction)) + 
  geom_point() + 
  geom_smooth(alpha = .2) +
  scale_y_continuous(limits = c(0, NA)) +
  ylab(expression(paste("Variogram [", gamma,"(h)]"))) +
  xlab("Lag distance (h)") +
  theme_minimal()
```

There is no strong evidence to reject the assumption of isotropy.

Use the `autofitVariogram()` function to get initial estimates.

```{r}
t.sp <- as(t.sf, "Spatial")
t.vm <- automap::autofitVariogram(formula = temp ~ X + Y, 
                                     input_data = t.sp)
plot(t.vm)
```

Set the initial parameters for a Gaussian model then fit the model.

```{r}
t.vmi <- vgm(model = "Gau", 
             psill = 2, 
             range = 100, 
             nugget = 1)
t.vmi

t.vm <- fit.variogram(object = t.v, 
                      model = t.vmi)
t.vm

plot(t.v, t.vm)
```

Make a grid for the interpolated values and add the coordinates as attributes.

```{r}
grid.sfc <- sf::st_make_grid(t.sf,
                             n = c(100, 100),
                             what = "centers")
XY <- grid.sfc |>
  sf::st_coordinates() 
grid.sf <- grid.sfc |>
  sf::st_as_sf() |>
  dplyr::mutate(X = XY[, 1],
                Y = XY[, 2])
```

Interpolate with universal kriging.

```{r}
t.int <- krige(temp ~ X + Y,
               locations = t.sf,
               newdata = grid.sf,
               model = t.vm)
```

Map the output.

```{r}
tmap::tm_shape(t.int) +
  tmap::tm_dots(title = "°F",
                shape = 15, 
                size = 2,
                col = "var1.pred", 
                n = 9, 
                palette = "OrRd") +
tmap::tm_shape(sts) +
  tmap::tm_borders() +
tmap::tm_shape(t.sf) +
  tmap::tm_text("temp", 
                col = "white", 
                size = .5) +
tmap::tm_layout(legend.outside = TRUE)
```

The trend term captures the north-south temperature gradient and the variogram captures the local spatial autocorrelation. Together they provide the best interpolated surface.

To see this, you refit the interpolation without the variogram model.

```{r}
krige(temp ~ X + Y,
      locations = t.sf,
      newdata = grid.sf) |>
  tmap::tm_shape() +
  tmap::tm_dots(title = "°F",
                shape = 15, 
                size = 2,
                col = "var1.pred", 
                n = 9, 
                palette = "OrRd") +
tmap::tm_shape(sts) +
  tmap::tm_borders() +
tmap::tm_shape(t.sf) +
  tmap::tm_text("temp", 
                col = "white", 
                size = .5) +
tmap::tm_layout(legend.outside = TRUE)
```

The result is that the variation in temperatures is interpolated as a simple trend surface.

For another comparison, here you interpolate assuming all variation is spatial autocorrelation (no trend term). This is called ordinary kriging.

```{r}
krige(temp ~ 1,
      locations = t.sf,
      newdata = grid.sf,
      model = t.vm) |>
  tmap::tm_shape() +
  tmap::tm_dots(title = "°F",
                shape = 15, 
                size = 2,
                col = "var1.pred", 
                n = 9, 
                palette = "OrRd") +
tmap::tm_shape(sts) +
  tmap::tm_borders() +
tmap::tm_shape(t.sf) +
  tmap::tm_text("temp", 
                col = "white", 
                size = .5) +
tmap::tm_layout(legend.outside = TRUE)
```

The result is that all variation is local autocorrelation. This produces patches of higher and lower temperatures. 

The pattern obtained with ordinary kriging is similar to that obtained using inverse distance weighting. Inverse distance weighting (IDW) is a deterministic method for interpolation. The values assigned to locations are calculated with a weighted average of the values available at the known locations, where the weights are the inverse of the distance to each known location.

The `krige()` function does IDW when there is no trend and no variogram model given.

```{r}
krige(temp ~ 1,
      locations = t.sf,
      newdata = grid.sf) |>
  tmap::tm_shape() +
  tmap::tm_dots(title = "°F",
                shape = 15, 
                size = 2,
                col = "var1.pred", 
                n = 9, 
                palette = "OrRd") +
tmap::tm_shape(sts) +
  tmap::tm_borders() +
tmap::tm_shape(t.sf) +
  tmap::tm_text("temp", 
                col = "white", 
                size = .5) +
tmap::tm_layout(legend.outside = TRUE)
```

Simple kriging is ordinary kriging with a specified mean. This is done with the `beta =` argument.

```{r}
krige(temp ~ 1,
      beta = mean(t.sf$temp),
      locations = t.sf,
      newdata = grid.sf,
      model = t.vm) |>
  tmap::tm_shape() +
  tmap::tm_dots(title = "°F",
                shape = 15, 
                size = 2,
                col = "var1.pred", 
                n = 9, 
                palette = "OrRd") +
tmap::tm_shape(sts) +
  tmap::tm_borders() +
tmap::tm_shape(t.sf) +
  tmap::tm_text("temp", 
                col = "white", 
                size = .5) +
tmap::tm_layout(legend.outside = TRUE)
```

## Evaluating the accuracy of the interpolation {-}

How do you evaluate how good the interpolated surface is? If you use the variogram model to predict at the observation locations, you will get the observed values back. 

For example, here you interpolate to the observation locations by setting `newdata = t.sf` instead of `grid.sf`. You then compute the correlation between the interpolated value and the observed value.

```{r}
t.int2 <- krige(temp ~ X + Y,
                locations = t.sf,
                newdata = t.sf,
                model = t.vm)
cor(t.int2$var1.pred, t.sf$temp)
```

So this is not helpful. Instead we use cross validation.

Cross validation is a procedure for assessing how well a model will do at predicting (interpolating) values when observations specific to the prediction are removed. The procedure first partitions the data into disjoint subsets. The model is then fit to one subset of the data (training set) and the model is validated on a different subset (testing set).

Leave-one-out cross validation uses all but one observation for fitting and the left-out observation for testing. The procedure is repeated with every observation taking turns being left out. 

K-fold cross validation uses K observations for fitting and N-K for testing. With large K there are many ways to slice the sample so the procedure is not exhaustive like hold-one-out. 

With kriging, the data is used in two ways (1) to fit the variogram model, and (2) to interpolate the values. Thus cross validation has two cases: weak and strong. Weak cross validation uses the entire data to estimate the variogram model. Then kriging is performed N times using a leave-one-out strategy with the predicted value saved only for the observation left out.

The `xvalid()` function from the {geoR} package computes the cross-validated prediction error of the Wolfcamp aquifer interpolation in this weak sense.
```{r}
xv.wk <- xvalid(wca.gdf, 
                model = wca.vm)
df <- data.frame(observed = xv.wk$data, 
                 predicted = xv.wk$predicted)

ggplot(df, aes(x = observed, y = predicted)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  geom_smooth(method = lm, color = "red") +
  ylab("Predicted head heights (ft)") +
  xlab("Observed head heights (ft)") +
  ggtitle("Weak Cross Validation") +
  theme_minimal()
```

The black line represents a perfect prediction and the red line is the best fit line when we regress the predicted head heights onto the observed head heights. The fact that the two lines nearly coincide indicates the interpolation is good.

We quantify how good using the mean squared error and mean absolute error as follows.
```{r}
mean(xv.wk$error^2)
mean(abs(xv.wk$error))
```

The mean squared cross-validated prediction error is 30636 ft^2 and the mean absolute cross-validated prediction error is 137 ft.

In contrast to weak cross validation, strong cross validation requires that the variogram be re-estimated each time an observation is removed. The model must be fit using the `variofit()` function and the call must include the empirical variogram object. This is done with the argument `reestimate = TRUE`.
```{r}
wca.vm2 <- variofit(wca.v, 
                    ini = c(33000, .8),
                    nug = 10000, 
                    cov.model = "spherical")

xv.st <- xvalid(wca.gdf, 
                model = wca.vm2,
                variog.obj = wca.v,
                reestimate = TRUE)
df <- data.frame(observed = xv.st$data, 
                 predicted = xv.st$predicted)

ggplot(df, aes(x = observed, y = predicted)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  geom_smooth(method = lm, color = "red") +
  ylab("Predicted head heights (ft)") +
  xlab("Observed head heights (ft)") +
  ggtitle("Strong Cross Validation") +
  theme_minimal()

mean(xv.st$error^2)
mean(abs(xv.st$error))
```

The prediction error estimated using the procedure of strong cross validation will be larger than the prediction error estimated using the procedure of weak cross validation.