# Tuesday March 21, 2023 {-}

**"Doubt is not a pleasant condition, but certainty is an absurd one.**" - Voltaire

## Evaluating interpolation accuracy {-}

Let's return to the Midwest April temperature example from last time. Get the data into your R session as a data frame and convert it to a simple feature data frame. Attach the spatial coordinates as attributes to the sf

```{r, message=FALSE}
( t.df <- "http://myweb.fsu.edu/jelsner/temp/data/MidwestTemps.txt" |>
  readr::read_table() )

t.sf <- t.df |>
  sf::st_as_sf(coords = c("lon", "lat"),
               crs = 4326)

t.sf$X <- t.df$lon
t.sf$Y <- t.df$lat
```

Plot the temperatures at the observation locations on a map

```{r}
sts <- USAboundaries::us_states()

tmap::tm_shape(t.sf) +
  tmap::tm_text(text = "temp", 
                size = .6) +
tmap::tm_shape(sts) +
  tmap::tm_borders() 
```

Compute and plot the sample variogram (omni-directional) using the residuals after removing the trend

```{r}
library(gstat)

t.v <- variogram(temp ~ X + Y, 
                 data = t.sf)
t.v |>
  plot()
```

Next set the initial parameters for an exponential model then fit an exponential variogram model

```{r}
t.vmi <- vgm(model = "Exp", 
             psill = 2.5, 
             range = 150, 
             nugget = .5)

t.vm <- fit.variogram(object = t.v, 
                      model = t.vmi)
t.v |>
  plot(t.vm)
```

How do you evaluate how good the interpolated surface is? You visually compared them across a grid of interpolations last time, but can you get a quantitative comparison at the observation locations?

If you use the variogram model to predict at the observation locations, you will get the observed values back when the nugget is zero

For example, here you interpolate to the observation locations by setting `newdata = t.sf` (instead of `grid.sf` as you did last time). You then compute the correlation between the interpolated value and the observed value

```{r}
t.int <- krige(temp ~ X + Y,
               locations = t.sf,
               newdata = t.sf,
               model = t.vm)

cor(t.int$var1.pred, t.sf$temp)
```

This is not helpful. Instead you use cross validation. Cross validation in this context is a procedure to assess how well the interpolation does at estimating the values at the observed locations when those values are not used in setting the interpolation procedure

Cross validation partitions the data into disjoint subsets and the interpolation procedure is set using one subset of the data (training set) and interpolations are made using the procedure on the other subset (testing set)

Leave-one-out cross validation (LOOCV) uses all but one observation for setting the procedure and the left-out observation is used for interpolation. This process is repeated with every observation taking turns being left out

The `krige.cv()` function from the {gstat} package is used for cross validating the kriging procedure. Interpolations are made at the observation locations

The arguments are the same as in `krige()` except the `nfold =` argument. Values for the argument range between 2 and the number of observations (here 131). The default is 131 which is LOOCV

For example with `nfold = 3` cross validation cuts the set of observations into 3rds (3 folds). Each observation gets put into one of the three folds with the interpolation procedure set using observations from the two folds and interpolations made on the remaining observations. This is repeated three times with each third taking turns being left out

```{r}
xv3 <- krige.cv(temp ~ X + Y,
                locations = t.sf,
                model = t.vm,
                nfold = 3)

xv3 |>
  head()

xv3 |>
  tail()
```
The output is the same as before but now the data frame has a column indicating the fold (the set of observations that were left out). It also has a column labeled `observed` which is the value of `temp` at that location

Using cross validation you are able to compare the interpolated value against the observed value at the observation locations. Here you use the three commonly used statistics for comparing statistical model skill, the correlation (r), the root-mean-squared error (rmse), and the mean absolute error (mae). These statistics provide a way for you to evaluate how skillful kriging is in producing the interpolated surface

Here you repeat the cross validation using `nfold = 131` (the number of observations in the data, LOOCV which is the default)

```{r}
krige.cv(temp ~ X + Y,
         locations = t.sf,
         model = t.vm,
         nfold = 131) |>
  sf::st_drop_geometry() |>
  dplyr::summarize(r = cor(var1.pred, observed),
                   rmse = sqrt(mean((var1.pred - observed)^2)),
                   mae = mean(abs(var1.pred - observed)))
```

The correlation is .94, the rmse is 1.32°F and the mae is 1.04°F

How do these skill metrics compare to interpolations from ordinary kriging

```{r}
krige.cv(temp ~ 1,
         locations = t.sf,
         model = t.vm) |>
  sf::st_drop_geometry() |>
  dplyr::summarize(r = cor(var1.pred, observed),
                   rmse = sqrt(mean((var1.pred - observed)^2)),
                   mae = mean(abs(var1.pred - observed)))
```

With ordinary kriging the skill values are worse. The correlation is lower and the rmse and mae are larger

How do these skill metrics compare to interpolations from a trend-only interpolation

```{r}
krige.cv(temp ~ X + Y,
         locations = t.sf) |>
  sf::st_drop_geometry() |>
  dplyr::summarize(r = cor(var1.pred, observed),
                   rmse = sqrt(mean((var1.pred - observed)^2)),
                   mae = mean(abs(var1.pred - observed)))
```

Even worse. The correlation is lower and the rmse and mae are higher

What about inverse-distance weighting interpolation?

```{r}
krige.cv(temp ~ 1,
         locations = t.sf) |>
  sf::st_drop_geometry() |>
  dplyr::summarize(r = cor(var1.pred, observed),
                   rmse = sqrt(mean((var1.pred - observed)^2)),
                   mae = mean(abs(var1.pred - observed)))
```

Better than the trend-only interpolation but not as good as universal kriging.

All four interpolations result in high correlation between observed and interpolated values that exceed .9 and root-mean-squared errors (RMSE) less than 1.8. But the universal kriging interpolation gives the highest correlation and the lowest RMSE and mean-absolute errors

For a visual representation of the goodness of fit you plot the observed versus interpolated values from the cross validation procedure

```{r}
library(ggplot2)

krige.cv(temp ~ X + Y,
               locations = t.sf,
               model = t.vm) |>
  dplyr::rename(interpolated = var1.pred) |>
ggplot(mapping = aes(x = observed, y = interpolated)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  geom_smooth(method = lm, color = "red") +
  ylab("Interpolated temperatures (°F)") +
  xlab("Observed temperatures (°F)") +
  theme_minimal()
```

The black line represents a perfect prediction and the red line is the best fit line when you regress the interpolated temperatures onto the observed temperatures. The fact that the two lines nearly coincide indicates the interpolation is very good

As you saw, the `nfold =` argument, which by default is set to the number of observations and does a LOOCV, allows you to divide the data into different size folds

These skill metrics are based on a fixed variogram model that uses all the observations when fitting. Thus cross validation using the `krige.cv()` function is a _partial_ cross validation

With kriging the data is used in two ways (1) to fit the variogram model, and (2) to interpolate the values. To perform a _full_ LOOCV you need to _refit_ the variogram after removing the observation for which you want the interpolation

Here is one way to do that using a `for()` loop

```{r}
vmi <- vgm(model = "Exp", 
             psill = 2.5, 
             range = 150, 
             nugget = .5)
int <- NULL
for(i in 1:nrow(t.sf)){
  t <- t.sf[-i, ]
  v <- variogram(temp ~ X + Y, 
                 data = t)
  vm <- fit.variogram(object = v, 
                      model = vmi)
  int[i] <- krige(temp ~ X + Y,
                  locations = t,
                  newdata = t.sf[i, ],
                  model = vm)$var1.pred
}
```

Now compare the observed values with the interpolated values

```{r}
data.frame(interpolated = int,
           observed = t.sf$temp) |>
   dplyr::summarize(r = cor(interpolated, observed),
                    rmse = sqrt(mean((interpolated - observed)^2)),
                    mae = mean(abs(interpolated - observed)))
```

These values are slightly worse (r is lower, and rmse and mae are larger). This will always be the case when using full cross validation but these skill estimates represent how well the procedure will perform on a new set of _independent_ observations

## Block cross validation {-}

With cross validation in the context of spatial data, the observations are not independent. As such it is better to create spatial areas for training separate from the spatial areas for testing rather than randomly selecting observations to withhold

The function `blockCV::spatialBlock()` function creates spatially separated folds based on a pre-specified distance (cell size of the blocks) from raster and vector spatial data objects. It assigns blocks to the training and testing folds with random, checkerboard, or systematic patterns (default is random). The range must be specified in units of meters. The argument `k =` specifies the number of folds with a default value of 5

```{r}
sb <- t.sf |>
  blockCV::cv_spatial()
```

The output shows how many observations are in the training and testing sets for each fold

The `$plot` list is a `ggplot2` object. Render the plot then add the observation locations to see the block assignments

```{r}
  blockCV::cv_plot(sb) +
  geom_sf(data = t.sf, 
          alpha = .5)
```

The area of the hexagons (default spatial geometry) should be larger (but not too much larger) than the area of a circle defined by the variogram model range as the radius 

```{r}
sf::st_area(sb$blocks$geometry) / 10^6

pi * (t.vm$range[2] / 2)^2
```

Now you need to repeat the full cross validation `for()` loop with some minor changes. First you need to add the fold and observation identification for each observation. Then rearrange the observations by these two indices

```{r}
t.sf2 <- t.sf |>
  dplyr::mutate(foldID = sb$folds_ids,
                obsID = 1:nrow(t.sf)) |>
  dplyr::arrange(foldID, obsID)

t.sf2 |>
  head()
```

Now repeat the loop on the new `t.sf2` spatial data frame. This time subset on `foldID`. Keep all folds not equal to `i` for training and then use the ith fold for interpolation

```{r}
vmi <- vgm(model = "Exp", 
             psill = 2.5, 
             range = 150, 
             nugget = .5)
int <- NULL
for(i in 1:5){
  t <- t.sf2[t.sf2$foldID != i, ]
  v <- variogram(temp ~ X + Y, 
                 data = t)
  vm <- fit.variogram(object = v, 
                      model = vmi)
  int_i <- krige(temp ~ X + Y,
                 locations = t,
                 newdata = t.sf2[t.sf2$foldID == i, ],
                 model = vm)$var1.pred
  int <- c(int, int_i)
}
```

Finally create a data frame and compute the skill metrics.

```{r}
data.frame(interpolated = int,
           observed = t.sf2$temp) |>
   dplyr::summarize(r = cor(interpolated, observed),
                   rmse = sqrt(mean((interpolated - observed)^2)),
                   mae = mean(abs(interpolated - observed)))
```

The skill metrics are worse but more representative of how well the interpolation will work with a different but similarly spatial correlated temperature field

An introduction to 'block' cross validation in the context of species distribution modeling is available here
<https://cran.r-project.org/web/packages/blockCV/vignettes/tutorial_1.html>

## Interpolating to areal units (block kriging) {-}

Let's consider at another application of spatial statistical interpolation to highlight additional aspects of the procedure like making better plots, constructing and interpreting variogram maps, working with anisotropy, and kriging to polygons

In 2008 tropical cyclone (TC) Fay formed from a tropical wave near the Dominican Republic, passed over the island of Hispaniola, Cuba, and the Florida Keys, then crossed the Florida peninsula and moved westward across portions of the Panhandle producing heavy rains in parts of the state

Rainfall is an example of geostatistical data. In principle it can be measured anywhere, but typically you have rainfall amounts at scattered measurement sites. Interest centers on making inferences about how much rain fell at certain locations or over averaged over a region (e.g., watershed basin)

Storm total rainfall amounts from sites in and around the state are in `FayRain.txt` on my website. The reports are a mix of official weather stations and cooperative sites. The cooperative sites are the Community Collaborative Rain, Hail and Snow Network (CoCoRaHS), a community-based, high density precipitation network made up of volunteers who take measurements of precipitation on their property. The data are from the Florida Climate Center

Get the data into your R sesson

```{r}
( FR.df <- "http://myweb.fsu.edu/jelsner/temp/data/FayRain.txt" |>
  readr::read_table() )
```

The data frame contains 803 rainfall measurement sites. Longitude and latitude coordinates are given in the first two columns and total rainfall in inches and millimeters are given in the second two columns. 

Create a spatial points data frame by specifying columns that contain the spatial coordinates. Then assign a geographic coordinate system and convert the rainfall from millimeters to centimeters.

```{r}
FR.sf <- sf::st_as_sf(x = FR.df,
                      coords = c("lon", "lat"),
                      crs = 4326) |>
  dplyr::mutate(tpm = tpm / 10)

summary(FR.sf$tpm)
```

The median rainfall across all available observations is 15.8 cm and the highest is 60.2 cm

Get the Florida county boundaries from the {USAboundaries} package

```{r}
FL.sf <- USAboundaries::us_counties(states = "Florida")
```

Transform the geographic coordinates of the site locations and map polygons to projected coordinates. Here you use Florida GDL Albers (EPSG:3087) with meter as the length unit

```{r}
FR.sf <- sf::st_transform(FR.sf, crs = 3087)
FL.sf <- sf::st_transform(FL.sf, crs = 3087)
sf::st_crs(FR.sf)
```

As always start by making a map. Here a map of the storm total rainfall at the observation locations that includes the state border

```{r}
tmap::tm_shape(FR.sf) +
  tmap::tm_dots(col = "tpm", size = .5) +
tmap::tm_shape(FL.sf) +
  tmap::tm_borders()
```

Two areas of very heavy rainfall are noted. One running north-south along the east coast and another across the north centered on Tallahassee

Rainfall reporting sites are clustered in and around cities and are located only over land. This clustering of station locations will make it hard for deterministic interpolation methods (e.g., IDW or splines) to produce a reasonable surface

The empirical variogram is computed using the `variogram()` function from the {gstat} package. The first argument is the model formula specifying the rainfall column from the data frame and the second argument is the data frame name.  Here `~ 1` in the model formula indicates no covariates or trends in the data. Trends are included by specifying coordinate names through the `st_coordinates()` function.

Compute the empirical variogram for these set of rainfall values. Use a cutoff distance of 400 km (400,000 m). The cutoff is the separation distance up to which point pairs are included in the semivariogram. The smaller the cutoff value the more the variogram is focused on nearest neighbor locations.

```{r}
library(gstat)

v <- variogram(tpm ~ 1, 
               data = FR.sf,
               cutoff = 400000)
```

Plot the variogram values as a function of lag distance and add text indicating the number of point pairs for each lag distance. Save a copy of the plot.

```{r}
library(ggplot2)

v.df <- data.frame(dist = v$dist/1000,
                   gamma = v$gamma,
                   np = v$np)

( pv <- ggplot(v.df, aes(x = dist, y = gamma)) +
  geom_point() +
  geom_text(aes(label = np), nudge_y = -5) +
  scale_y_continuous(limits = c(0, 220)) +
  scale_x_continuous(limits = c(0, 400)) +
  xlab("Lagged distance (h) [km]") +
  ylab(expression(paste("Semivariance (", gamma, ") [", cm^2, "]"))) +
  theme_minimal() )
```

Values start low (around 50 cm$^2$) at the shortest lag distance and increase to greater than 200 cm$^2$ at lag distances of 200 km and longer.

The semivariance at lag zero is called the 'nugget' and the semivariance at a level where the variogram values no longer increase is called the 'sill.' The difference between the sill and the nugget is called the 'partial sill'.  The lag distance out to where the sill is reached is called the 'range.'  These three parameters (nugget, partial sill, and range) are used to model the variogram.

Next fit a model to the empirical variogram. The model is a mathematical relationship that defines the semivariance as a function of lag distance. First save the family and the initial parameter guesses in a variogram model (`vmi`) object.

```{r}
vmi <- vgm(model = "Gau", 
           psill = 150, 
           range = 200 * 1000, 
           nugget = 50)
vmi
```

The `psill` argument is the partial sill along the vertical axis. Estimate the parameter values by looking at the empirical variogram. 

Next use the `fit.variogram()` function to improve the fit over these initial values. Given a set of initial parameter values the method of weighted least squares is used to improve the parameter estimates.

```{r}
vm <- fit.variogram(object = v, 
                    model = vmi)
vm
```

The result is a variogram model with a nugget of 46.6 cm$^2$, a partial sill of 156 cm$^2$, and a range on the sill of 128 km. 

Plot the model on top of the empirical variogram. Let $r$ be the range, $c$ the partial sill and $c_o$ the nugget, then the equation defining the function over the set of lag distances $h$ is

$$
\gamma(h)=c\left(1-\exp\left(-\frac{h^2}{r^2}\right)\right)+c_o
$$

Create a data frame with values of h and gamma using this equation.

```{r}
nug <- vm$psill[1]
ps <- vm$psill[2]
r <- vm$range[2] / 1000
h <- seq(0, 400, .2)
gamma <- ps * (1 - exp(-h^2 / (r^2))) + nug

vm.df <- data.frame(dist = h,
                    gamma = gamma)

pv + geom_line(data = vm.df,
               mapping = aes(x = dist, y = gamma))
```

Check for anisotropy. Anisotropy refers to a dependence of the variogram shape on the direction of the location pairs used to compute semivariances. Isotropy refers to a directional independence.

```{r}
plot(variogram(tpm ~ 1, 
               data = FR.sf, 
               alpha = c(0, 45, 90, 135),
               cutoff = 400000), 
     xlab = "Lag Distance (m)")
```

The semivariance values reach the sill at a longer range  (about 300 km) in the north-south direction (0 degrees) compared to the other three directions.

Another way to look at directional dependence in the variogram is through a variogram map. Instead of classifying point pairs Z(s) and Z(s + h) by direction and distance class separately, you classify them jointly.

If h = {x, y} is the two-dimensional coordinates of the separation vector, in the variogram map the variance contribution of each point pair (Z(s) − Z(s + h))^2 is attributed to the grid cell in which h lies. The map is centered at (0, 0) and h is lag distance. Cutoff and width correspond to map extent and cell size; the semivariance map is point-wise symmetric around (0, 0), as γ(h) = γ(−h).

The variogram map is made with the `variogram()` function by adding the `map = TRUE` argument. Here you set the cutoff to be 200 km (200,000 m) and the width (cell size) to be 20 km.

```{r}
vmap <- variogram(tpm ~ 1, 
                  data = FR.sf,
                  cutoff = 200000,
                  width = 20000,
                  map = TRUE)
plot(vmap)
```

Along the dx = 0 vertical line in the north-south direction (top-to-bottom on the plot), the semivariance values increase away from dy = 0, but the increase is less compared to along the dy = 0 horizontal line in the east-west direction (left-to-right on the plot) indicative of directional dependency.

You refit the variogram model defining an anisotropy ellipse with the `anis =` argument. The first value is the direction of longest range (here north-south) and the second value is the ratio of the shortest to longest ranges. Here about (200/300 = .67, see the directional variogram plot above).

```{r}
vmi <- vgm(model = "Gau", 
           psill = 150, 
           range = 300 * 1000, 
           nugget = 50,
           anis = c(0, .67))
vm <- fit.variogram(v, vmi)
```

Use the variogram model together with the rainfall values at the observation sites to create an interpolated surface. Here you use ordinary kriging as there are no spatial trends in the rainfall.

Interpolation is done using the `krige()` function. The first argument is the model specification and the second is the data. Two other arguments are needed. One is the variogram model using the argument name `model =` and the other is a set of locations identifying where the interpolations are to be made. This is specified with the argument name `newdata =`.

Here you interpolate to locations on a regular grid. You create a grid of locations within the borders of the state using the `st_sample()` function.

```{r}
grid.sfc <- sf::st_sample(FL.sf,
                          size = 5000,
                          type = "regular")
```

You specify the number of grid locations using the argument `size =`. Note that the actual number of locations will be somewhat different because of the irregular boundary.

```{r}
grid.sfc |>
  plot(pch = ".")
```

First use the `krige()` function to interpolate the observed rainfall to the grid locations. For a given location, the interpolation is a weighted average of the rainfall across the entire region where the weights are determined by the variogram model.

```{r}
r.int <- krige(tpm ~ 1, 
               locations = FR.sf, 
               newdata = grid.sfc,
               model = vm)
```

If the variogram model is not included then inverse distance-weighted interpolation is performed. The function will not work if a location has more than one value.

The saved object (`r.int`) inherits the spatial geometry specified in the `newdata =` argument but extends it to a spatial data frame. The column `var1.pred` in the data frame is the interpolated rainfall and the second `var1.var` is the variability about the interpolated value.

Plot the interpolated storm-total rainfall field.

```{r}
tmap::tm_shape(r.int) +
  tmap::tm_dots("var1.pred",
                size = .1,
                palette = "Greens",
                title = "Rainfall (cm)") +
  tmap::tm_shape(FL.sf) +
  tmap::tm_borders() +
  tmap::tm_layout(legend.position = c("left", "bottom"),
                  title = "TC Fay (2008)",
                  title.position = c("left", "bottom"),
                  legend.outside = TRUE)
```

Note: a portion of the data locations are outside of the state but interest is only interpolated values within the state border as specified by the `newdata =` argument.

The spatial interpolation shows that parts of east central and north Florida were deluged by Fay with rainfall totals exceeding 30 cm (12 in).

The interpolation can also be done as an area average. For example what was the storm-total average rainfall for each county?

County level rainfall is relevant for water resource managers. _Block kriging_ produces an estimate of this area average, which will differ from a simple average over all sites within the county because of the spatial autocorrelation in rainfall observations.

You use the same function to interpolate but specify the spatial polygons rather than the spatial grid as the new data. Here the spatial polygons are the county borders.

```{r}
r.int2 <- krige(tpm ~ 1, 
                locations = FR.sf, 
                newdata = FL.sf, 
                model = vm)
```

Again plot the interpolations.

```{r}
tmap::tm_shape(r.int2) +
  tmap::tm_polygons(col = "var1.pred",
                    palette = "Greens",
                    title = "Rainfall (cm)") +
  tmap::tm_layout(legend.position = c("left", "bottom"),
                  title = "TC Fay (2008)",
                  title.position = c("left", "bottom"))
```

The overall pattern of rainfall from Fay featuring the largest amounts along the central east coast and over the Big Bend region are similar in both maps but these estimates answer questions like on average how much rain fell over Leon County during TC Fay? 

You compare the kriged average with the simple average at the county level with the `aggregate()` method. The argument `FUN = mean` says to compute the average of the values in `FR.sf` across the polygons in `FL.sf`.

```{r}
r.int3 <- aggregate(FR.sf, 
                    by = FL.sf, 
                    FUN = mean)
```

The result is a simple feature data frame of the average rainfall in each county.

The state-wide mean of the kriged estimates at the county level is

```{r}
r.int2$var1.pred |>
  mean() |>
  round(2)
```

This compares with a state-wide mean from the simple averages.

```{r}
r.int3$tpm |>
  mean() |>
  round(2)
```

The correlation between the two estimates across the 67 counties is 

```{r}
round(cor(r.int3$tpm, r.int2$var1.pred), 2)
```

The variogram model reduces the standard deviation of the kriged estimate relative to the standard deviation of the simple averages because of local smoothing.

```{r}
r.int2$var1.pred |>
  sd() |>
  round(2)

r.int3$tpm |>
  sd() |>
  round(2)
```

This can be seen with a scatter plot of simple averages versus kriged averages at the county level.

```{r}
compare.df <- data.frame(simpleAvg = r.int3$tpm,
                         krigeAvg = r.int2$var1.pred)
ggplot(compare.df, aes(x = simpleAvg,
                       y = krigeAvg)) +
  geom_point() +
  geom_abline(slope = 1) +
  geom_smooth(method = lm, se = FALSE)
```

The slope of the blue line is less than the black line (y = x line) indicating kriging over estimates at low amounts and under estimates at high amounts relative to simple averaging.

Kriging has the advantage of accompanying uncertainty estimates. The prediction variances are listed in a column in the spatial data frame saved from apply the `krige()` function. Variances are smaller in regions with more rainfall observations.  

Prediction variances are also smaller with block kriging as much of the variability within the county averages out. To compare the distribution characteristics of the prediction variances for point and block kriging of the rainfall observations, type

```{r}
r.int$var1.var |>
  summary() |>
  round(1)

r.int2$var1.var |>
  summary() |>
  round(1)
```

The median prediction variance (in cm$^2$) for the point kriging is close to the value of the nugget.

```{r}
r.int$var1.var |>
  median() |>
  round(1)
```
In contrast, the median prediction variance for our block kriging is a much smaller.

```{r} 
r.int2$var1.var |>
  median() |>
  round(1)
```

## Simulating spatial fields {-}

Simulations use the interpolated uncertainty to provide additional data for deterministic models. Suppose you have a hydrology model of rainfall runoff. Given a spatial field of rain amounts the model predicts a discharge rate at some location along a river. The uncertainty in the predicted runoff rate at the location is due to the uncertainty in where and how hard the rain fell (in the rainfall field) and not due to the deterministic hydrology model.

The uncertainty in the rainfall field is simulated conditional on the observations with the same `krige()` function by adding the argument `nsim =` that specifies the number of simulations.  

For a large number it may be necessary to limit the number neighbors in the kriging. This is done using the `nmax` argument. For a given location, the weights assigned to observations far away are very small, so it is efficient to limit how many are used in the simulation.

As an example, here you generate four realizations of the county-level average storm total rainfall for Fay and limit the neighborhood to 50 of the closest observation sites. This takes a few seconds.

```{r}
r.sim <- krige(tpm ~ 1, 
               locations = FR.sf, 
               newdata = FL.sf, 
               model = vm, 
               nsim = 4, 
               nmax = 50)
```

Given the variogram model, the simulations are conditional on the observed rainfall.

```{r}
tmap::tm_shape(r.sim) +
    tmap::tm_polygons(col = c("sim1", "sim2", "sim3", "sim4"),
                palette = "Greens",
                title = "Simulated Rainfall [cm]") +
    tmap::tm_facets(free.scales = FALSE) 
```

The overall pattern of rainfall remains the same, but there are differences especially in counties with fewer observations and in counties where the rainfall gradients are sharp.
