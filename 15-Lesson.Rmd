# Tuesday October 25, 2022 {.unnumbered}

**"To me programming is more than an important practical art. It is also a gigantic undertaking in the foundations of knowledge."** â€“ Grace Hopper

Today

- Estimating the relative risk of events
- Estimating second-order properties of spatial events

## Estimating the relative risk of events {-}

Separate spatial intensity maps across two marked types provides a way to estimate the risk of one event type conditional on the other event type. More generally, the relative risk of occurrence of some event is a conditional probability. In a non-spatial context, the risk of catching a disease if you are elderly relative to the risk if you are young.

Given a tornado somewhere in Texas what is the chance that it will cause at least EF3 damage? With the historical set of all tornadoes marked by the damage rating you can make a map of all tornadoes and a map of the EF3+ tornadoes and then take the ratio.

To see this start by importing the tornado data, mutating and selecting the damage rating as a factor called `EF` before turning the resulting simple feature data frame into a planar point pattern. 
```{r}
Torn.sf <- sf::st_read(dsn = here::here("data", "1950-2020-torn-initpoint")) |>
  sf::st_transform(crs = 3082) |>
  dplyr::filter(mag >= 0) |>
  dplyr::mutate(EF = as.factor(mag)) |>
  dplyr::select(EF)

library(spatstat)
T.ppp <- Torn.sf |>
  as.ppp()
```

Then subset by the boundary of Texas.

```{r}
TX.sf <- USAboundaries::us_states(states = "Texas") |>
  sf::st_transform(crs = sf::st_crs(Torn.sf))

W <- TX.sf |>
  as.owin()

T.ppp <- T.ppp[W]
summary(T.ppp)
```

Chance that a tornado anywhere in Texas will be at least EF3 or worse is the sum of the proportions for these types: .03616 + .00537 + .00067 = .042 (or 4.2%). 

As found previously there is a spatial intensity gradient across the state with fewer tornadoes in the southwest and more in the northeast. Also the more damaging tornadoes might be more common relative to all tornadoes in some parts of the state compared with other parts.

To create a map of the relative risk of the more damaging tornadoes you start by making two `ppp` objects, one being the set of all tornado events with damage ratings 0, 1, or 2 and the other the set of all tornado locations with damage ratings 3, 4, or 5. You do this by subset the object using brackets (`[]`) and the logical operator `|` (or) and then merge the two subsets assigning names `H` and `I` as marks with the `superimpose()` function.

```{r}
H.ppp <- unmark(T.ppp[T.ppp$marks == 2 | T.ppp$marks == 1 | T.ppp$marks == 0])
I.ppp <- unmark(T.ppp[T.ppp$marks == 3 | T.ppp$marks == 4 | T.ppp$marks == 5])
T2.ppp <- superimpose(H = H.ppp, 
                      I = I.ppp)
```

See https://en.wikipedia.org/wiki/Enhanced_Fujita_scale for definitions of EF tornado rating.

The chance that a tornado chosen at random is intense (EF3+) is 4.2%. Plot the event locations for the set of intense tornadoes.

```{r}
plot(I.ppp, 
     pch = 25, 
     cols = "red", 
     main = "")
plot(T.ppp, add = TRUE, lwd = .1)
```

To get the relative risk use the `relrisk()` function. If X is a multi-type point pattern with factor marks and two levels of the factor then the events of the first type (the first level of `marks(X)`) are treated as controls (conditionals) or non-events, and events of the second type are treated as cases.

The `relrisk()` function estimates the local chance of a case (i.e. the probability $p(u)$ that a point at $u$ will be a case) using a kernel density smoother. The bandwidth for the kernel is specified or can be found through an iterative cross-validation procedure (recall the bandwidth selection procedure used in geographic regression) using the `bw.relrisk()` function. 

The bandwidth has units of length (here meters). You specify a minimum and maximum bandwidth with the `hmin =` and `hmax =` arguments. This takes a few seconds.

```{r}
( bw <- bw.relrisk(T2.ppp,
                   hmin = 1000,
                   hmax = 200000) )
```

The optimal bandwidth (`sigma`) is 119770 meters or about 120 km. 

Now estimate the relative risk at points defined by a 256 by 256 grid and using the 120 km bandwidth for the kernel smoother.

```{r}
rr <- relrisk(T2.ppp, 
              sigma = bw,
              dimyx = c(256, 256))
```

The result is an object of class `im` (image) with values you interpret as the conditional probability of an 'intense' tornado.

You retrieve the range of probabilities with the `range()` function. Note that many of the values are `NA` corresponding pixels that are outside the window so you set the `na.rm` argument to `TRUE`.

```{r}
range(rr, na.rm = TRUE)
```

The probabilities range from a low of .5% to a high of 6%. This range compares with the statewide average probability of 4.2%.

Map the probabilities with the `plot()` method.
```{r}
plot(rr)
```

Make a better map by converting the image to a raster, setting the CRS, and then using functions from the {tmap} package.

```{r}
tr.r <- raster::raster(rr)
raster::crs(tr.r) <- sf::st_crs(Torn.sf)$proj4string

tmap::tm_shape(tr.r) +
  tmap::tm_raster()
```

The chance that a tornado is more damaging peaks in the northeast part of the state.

Since the relative risk is computed for any point it is of interest to extract the probabilities for cities and towns.

You get city locations with the `us_cities()` function from the {USAboundaries} package that extracts a simple feature data frame of cities. The CRS is 4326 and you filter to keep only cities with at least 100000 in 2010.

```{r}
Cities.sf <- USAboundaries::us_cities(state = "TX") |>
  sf::st_transform(crs = raster::crs(tr.r)) |>
  dplyr::filter(population > 100000)
```

Use the `extract()` function from the {raster} package to get a single value for each city. Put these values into the simple feature data frame.

```{r}
Cities.sf$tr <- raster::extract(tr.r, 
                                Cities.sf)

Cities.sf |>
  dplyr::arrange(desc(tr)) 
```

To illustrate the results create a graph using the `geom_lollipop()` function from the {ggalt} package. Use the package {scales} to allow for labels in percent.
```{r}
library(ggalt)
library(scales)

ggplot(Cities.sf, aes(x = reorder(city, tr), y = tr)) +
    geom_lollipop(point.colour = "steelblue", point.size = 3) +
    scale_y_continuous(labels = percent, limits = c(0, .0625)) +
    coord_flip() +
    labs(x = "", y = NULL, 
         title = "Historical chance that a tornado caused at least EF3 damage",
         subtitle = "Cities in Texas with a 2010 population > 100,000",
         caption = "Data from SPC (1950-2020)") +
  theme_minimal()
```

Another example: Florida wildfires

Given a wildfire in Florida what is the probability that it was started by lightning? 

Import wildfire data (available here: https://www.fs.usda.gov/rds/archive/catalog/RDS-2013-0009.4) as a simple feature data frame and transform the native CRS to a Florida GDL Albers (EPSG 3086).
```{r}
if(!"FL_Fires" %in% list.files(here::here("data"))){
  download.file("http://myweb.fsu.edu/jelsner/temp/data/FL_Fires.zip",
                destfile = here::here("data", "FL_Fires.zip"))
unzip(zipfile = here::here("data", "FL_Fires.zip"),
      exdir = here::here("data"))
}

FL_Fires.sf <- sf::st_read(dsn = here::here("data", "FL_Fires")) |>
  sf::st_transform(crs = 3086)
dim(FL_Fires.sf)
```

Each row is a unique fire and the data spans the period 1992-2015. There are over 90K rows and 38 variables. 

To make things run faster, here you analyze only a random sample of all the data. You do this with the `dplyr::sample_n()` function where the argument `size =` specifies the number of rows to choose at random. Save the sample of events to the object `FL_FiresS.sf`. First set the seed for the random number generator so that the set of rows chosen will be the same every time you run the code.

```{r}
set.seed(78732)

FL_FiresS.sf <- FL_Fires.sf |>
  dplyr::sample_n(size = 2000)

dim(FL_FiresS.sf)
```

The result is a simple feature data frame with exactly 2000 rows.

The character variable `STAT_CAU_1` indicates the cause of the wildfire.

```{r}
FL_FiresS.sf$STAT_CAU_1 |>
  table()
```

There are 13 causes (listed in alphabetical order) with various occurrence frequencies. Lightning is the most common.

To analyze these data as spatial events, you first convert the simple feature data to a `ppp` object over a window defined by the state boundaries. Use the cause of the fire as a factor mark.
```{r}
F.ppp <- FL_FiresS.sf["STAT_CAU_1"] |>
  as.ppp()

W <- USAboundaries::us_states(states = "Florida") |>
  sf::st_transform(crs = sf::st_crs(FL_Fires.sf)) |>
  as.owin()

F.ppp <- F.ppp[W]
marks(F.ppp) <- as.factor(marks(F.ppp)) # make the character marks factor marks

summary(F.ppp)
```

Output from the `summary()` method displays a table of frequency by type including the proportion and the average spatial intensity (per square meters). 

The probability that a wildfire is caused by lightning is about 25% (`proportion` column of the frequency versus type table). How does this probability vary over the state?

Note that the window contains four separate polygons to capture the main boundary (`polygon 4`) and the Florida Keys.
```{r}
plot(W)
```

First split the object `F.ppp` on whether or not the cause was lightning and then merge the two event types and assign names `NL` (human caused) and `L` (lightning caused) as marks.

```{r}
L.ppp <- F.ppp[F.ppp$marks == "Lightning"] |>
  unmark()
NL.ppp <- F.ppp[F.ppp$marks != "Lightning"] |>
  unmark()

LNL.ppp <- superimpose(NL = NL.ppp, 
                       L = L.ppp)

summary(LNL.ppp)
```

Now the two types are `NL` and `L` composing 75% and 25% of all wildfire events.

The function `relrisk()` computes the spatially-varying probability of a case (event type), (i.e. the probability $p(u)$ that a point at location $u$ will be a case).

Here you compute the relative risk on a 256 by 256 grid.
```{r}
wfr <- relrisk(LNL.ppp, 
               dimyx = c(256, 256))
```

Create a map from the raster by first converting the image object to a raster object and assigning the CRS with the `crs()` function from the {raster} package. Add the county borders for geographic reference.
```{r}
wfr.r <- raster::raster(wfr)
raster::crs(wfr.r) <- sf::st_crs(FL_Fires.sf)$proj4string

FL.sf <- USAboundaries::us_counties(state = "FL") |>
  sf::st_transform(crs = sf::st_crs(FL_Fires.sf))

tmap::tm_shape(wfr.r) +
  tmap::tm_raster(title = "Probability") +
tmap::tm_shape(FL.sf) +
  tmap::tm_borders(col = "gray70") +
tmap::tm_legend(position = c("left", "center") ) +
tmap::tm_layout(main.title = "Chance a wildfire was started by lightning (1992-2015)",
                main.title.size = 1) +
tmap::tm_compass(position = c("right", "top")) +
tmap::tm_credits(text = "Data source: Karen Short https://doi.org/10.2737/RDS-2013-0009.4",
                 position = c("left", "bottom")) 
```

## Estimating second-moment properties of spatial events {-}

Spatial intensity is a first-moment property of event locations (like the average of a set of numbers). It answers the question: where are events more and less frequent? 

Clustering is a second-moment property of event locations (like the variance of a a set of numbers). It answers the question: is the probability of an event in the proximity of another event higher than expected by chance? 

On example of cluster occurs with the location of trees in a forest. A tree's seed dispersal mechanism leads to a greater likelihood of another tree nearby.

Let $r$ be the distance between two event locations or the distance between an event and an arbitrary point within the domain, then functions to describe clustering include:

- The nearest neighbor distance function $G(r)$: The cumulative distribution of the distances from an event to the nearest other event (event-to-event function). It summarizes the distance between events (amount of clustering).

- The empty space function $F(r)$: The cumulative distribution of the distances from a point in the domain to the nearest event (point-to-event function). It summarizes the distance gaps between events (amount of gappiness or lacunarity).

- The reduced second-moment function (Ripley $K$) $K(r)$: Defined such that $\lambda \times K(r)$ is the expected number of additional events within a distance $r$ of an event, where $\lambda$ is the average intensity of the events. It is a measure of the spatial autocorrelation among the events.

To assess the degree of clustering and significance (in a statistical sense), we estimate values of the function using our data set and compare the resulting curve (empirical curve) to a theoretical curve assuming a homogeneous Poisson process. 

The theoretical curve is well defined for homogeneous point patterns (recall: CSR--complete spatial randomness). Deviations of an 'empirical' curve from a theoretical curve provides evidence against CSR.

The theoretical functions assuming a homogeneous Poisson process are:

- $F(r) = G(r) = 1 - \exp(-\lambda \pi r^2)$
- $K(r) = \pi r^2$

where $\lambda$ is the domain average spatial intensity and $\exp()$ is the exponential function.

Recall the Swedish pine saplings data that comes with the {spatstat} package.
```{r}
data(swedishpines)
class(swedishpines)
```

Assign the data to an object called `SP` to reduce the amount of typing. 
```{r}
( SP <- swedishpines )
```

The output indicates that there are 71 events within a rectangle window 96 by 100 units where one unit is .1 meters.

You obtain the values for the nearest neighbor function using the `Gest()` function from the {spatstat} package. Use the argument `correction = "none"` so no corrections are made for events near the window borders. Assign the output to a list object called `G`.

```{r}
( G <- Gest(SP,
            correction = "none") )
```

The output includes the distance `r`, the raw uncorrected estimate of $G(r)$ (empirical estimate) at various distances, and a theoretical estimate at those same distances based on a homogeneous Poisson process. Using the `plot()` method on the saved object `G` you compare the empirical estimates with the theoretical estimates. Here two horizontal lines are added to help with the interpretation.

```{r}
plot(G)
abline(h = c(.2, .5), 
       col = "black",
       lty = 2)
```

Values of G are on the vertical axis and values of distance (lag) are on the horizontal axis starting at 0. The black curve is the uncorrected estimate of $G_{raw}(r)$ from the event locations and the red curve is $G_{pois}(r)$ estimated from a homogeneous Poisson process with the same average intensity as the pine saplings.

The horizontal dashed line at G = .2 intersects the black line at a relative distance (r) of 5 units. This means that 20% of the events have another event _within_ 5 units. This means that 20% of the saplings have another sapling withing .5 meter. 

Imagine placing a disc of radius 5 units around all 71 events then counting the number of events that have another event under the disc. That number divided by 71 is G(r).

To check this compute all pairwise distances with the `pairdist()` function.
```{r}
PDmatrix <- pairdist(SP)
PDmatrix[1:6, 1:6]
```

This creates a 71 x 71 square matrix of distances. 

Sum the number of rows whose distances are within 5 units. The minus one means you don't count the row containing event over which you are summing (an event location is not a neighbor of itself).
```{r}
sum(rowSums(PDmatrix < 5) - 1) / nrow(PDmatrix) * 100
```

Returning to the plot, the horizontal dashed line at G = .5 intersects the black line at .8 meters indicating that 50% of the pine saplings have another pine sapling within .8 meter.

You see that for a given radius the $G_{raw}$ line is _below_ the $G_{pois}(r)$ line indicating that there are _fewer_ pine saplings with another pine sapling in the vicinity than expected by chance.

For example, if the saplings were arranged under a model of CSR, you would expect 20% of the pairwise distances to be within .3 meter and 50% of them to be within .55 meter.

You make a better plot by first converting the object `G` to a data frame and then using {ggplot2} functions. Here you do this and then remove estimates for distances greater than 1.1 meter and convert the distance units to meters.
```{r}
G.df <- as.data.frame(G) |>
  dplyr::filter(r < 11) |>
  dplyr::mutate(r = r * .1)

ggplot(data = G.df, 
       mapping = aes(x = r, y = raw)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  geom_hline(yintercept = c(.2, .5), lty = 'dashed') +
  xlab("Lag distance (m)") +  ylab("G(r): Cumulative % of events having another event within a distance r") +
  theme_minimal()
```

Values for the empty space function are obtained from the `Fest()` function. Here you apply the Kaplan-Meier correction for edge effects with `correction = "km"`. The function returns the percent of the domain within a distance from any event. 

Imagine again placing the disc, but this time on top of every point in the window and counting the number of points that have an event underneath.

Make a plot and add some lines to help with interpretation. 
```{r}
F.df <- SP |>
  Fest(correction = "km") |>
  as.data.frame() |>
  dplyr::filter(r < 11) |>
  dplyr::mutate(r = r * .1)

ggplot(data = F.df, 
       mapping = aes(x = r, y = km)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  geom_hline(yintercept = c(.7, .58), lty = 'dashed') +
  geom_vline(xintercept = .61, lty = 2) +
  xlab("Lag distance (m)") +  ylab("Percent of domain within a distance r of an event") +
  theme_minimal()
```

The horizontal dashed line at F = .7 intersects the black line at a distance of .61 meter. This means that 70% of the spatial domain is less than .61 meters from a sapling. The red line is the theoretical homogeneous Poisson process model. If the process was CSR slightly less than 58% (F = .58) of the domain would be less than .6 meter from a sapling. In words, the arrangement of saplings is less "gappy" (more regular) than expected by chance.

The J function is the ratio of the F function to the G function. For a CSR processes the value of J is one. Here we see a large and systematic departure of J from one for distances greater than about .5 meter, due to the regularity in the spacing of the saplings.

```{r}
J.df <- SP |>
    Jest() |>
    as.data.frame() |>
    dplyr::filter(r < 10) |>
    dplyr::mutate(r = r * .1)

ggplot(data = J.df, 
       mapping = aes(x = r, y = km)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  xlab("Lag distance (m)") + ylab("") +
  theme_minimal()
```

A commonly used distance function for assessing clustering in point pattern data is called Ripley's K function. It is estimated with the `Kest()` function. 

Mathematically it is defined as

$$
\hat K(r) = \frac{1}{\hat \lambda} \sum_{j \ne i} \frac{I(r_{ij} < r)}{n}
$$

where $r_{ij}$ is the Euclidean distance between event $i$ and event $j$, $r$ is the search radius, and $\hat \lambda$ is an estimate of the intensity $(\hat \lambda = n/|A|)$ where $|A|$ is the window area and $n$ is the number of events. $I(.)$ is an indicator function equal to 1 when the expression $r_{ij} < r$, and 0 otherwise. If the events are homogeneous, $\hat{K}(r)$ increases at a rate proportional to $\pi r^2$.