# Tuesday February 21, 2023 {-}

**"Feeling a little uncomfortable with your skills is a sign of learning, and continuous learning is what the tech industry thrives on!"** --- Vanessa Hurst

## Using local spatial autocorrelation to understand the tornado record {-}

It is well known that the tornado record, that extends back to 1950, is incomplete

Is the frequency of recent tornado reports correlated with the number of people in a region? Might this correlation extend to the number of people in neighboring regions?

To answer these questions you quantify the non-spatial correlation and the bi-variate spatial autocorrelation between tornado occurrences and population. To keep this manageable you focus on one state (Iowa)

Start by getting the U.S. Census data with functions from the {tidycensus} package. Downloading U.S. census data using functions from the {tidycensus} package requires you register with the Census Bureau 

You get an API key from http://api.census.gov/data/key_signup.html. Then use the `tidycensus::census_api_key()` function and put your key in quotes.
```{r, eval=FALSE}
tidycensus::census_api_key("YOUR API KEY GOES HERE")
```

The `get_decennial()` function gets you access to the 1990, 2000, and 2010 decennial Census data and the `get_acs()` function gets you access to the 5-year American Community Survey data. For example, here is how you get county-level population for the state of Iowa

```{r}
Counties.sf <- tidycensus::get_acs(geography = "county", 
                                   variables = "B02001_001E", 
                                   state = "IA",
                                   geometry = TRUE)
```

The code returns a simple feature data frame with county borders as multi-polygons. The variable `B02001_001E` is the 2016-2020 population estimate in each county within the state

Next get the tornado data and count the number of tracks by county. A single track can intersect more than one county

```{r}
Torn.sf <- sf::st_read(dsn = here::here("data", "1950-2021-torn-aspath"), 
                       layer = "1950-2021-torn-aspath") |>
  sf::st_transform(crs = sf::st_crs(Counties.sf)) |>
  dplyr::filter(yr >= 2016)

( TorCounts.df <- Torn.sf |>
  sf::st_intersection(Counties.sf) |>
  sf::st_drop_geometry() |>
  dplyr::group_by(GEOID) |>
  dplyr::summarize(nT = dplyr::n()) )
```

Next join the counts to the simple feature data frame by the common column name `GEOID`

```{r}
Counties.sf <- Counties.sf |>
  dplyr::left_join(TorCounts.df,
                   by = "GEOID") |>
  dplyr::mutate(nT = tidyr::replace_na(nT, 0)) |>
  dplyr::mutate(Area = sf::st_area(Counties.sf),
                rate = nT/Area/(2021 - 2016 + 1) * 10^10,
                lpop = log10(estimate))
```

Note that some counties had no tornadoes and the `dplyr::left_join()` returns a value of `NA` for those. You use `dplyr::mutate()` with `tidyr::replace_na()` to turn those counts to a value of 0.

Make a two-panel map displaying the log of the population and the tornado rates.

```{r}
map1 <- tmap::tm_shape(Counties.sf) +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_fill(col = "lpop",
                title = "Log Population",
                palette = "Blues") +
  tmap::tm_layout(legend.outside = "TRUE")

map2 <- tmap::tm_shape(Counties.sf) +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_fill(col = "rate",
                title = "Annual Rate\n[/100 sq. km]",
                palette = "Greens") +
  tmap::tm_layout(legend.outside = "TRUE")

tmap::tmap_arrange(map1, map2)
```

There appears some relationship. The non-spatial correlation between the two variables is obtained with the `cor.test()` function

```{r}
lpop <- Counties.sf$lpop
rate <- as.numeric(Counties.sf$rate)

cor.test(lpop, rate)
```

The bi-variate spatial autocorrelation is assessed using the Lee statistic. A formal non-parametric test under the null hypothesis of no bi-variate spatial autocorrelation is done using a Monte Carlo simulation

```{r}
nbs <- spdep::poly2nb(Counties.sf)
wts <- spdep::nb2listw(nbs)

Lee <- spdep::lee(lpop, rate, 
                  listw = wts, 
                  n = length(nbs))
Lee$L

spdep::lee.mc(lpop, rate, listw = wts, nsim = 9999)
```

Finally you map out the local variation in the bi-variate spatial autocorrelation

```{r}
Counties.sf$localL <- Lee$localL

tmap::tm_shape(Counties.sf) +
  tmap::tm_fill("localL",
                title = "Local Bivariate\nSpatial Autocorrelation") +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_layout(legend.outside = TRUE)
```

What might cause this? Cedar County (dark green) lies between the cities of Cedar Rapids (to the northwest), Iowa City (to the west) and the Quad Cities (to the east). Commuters from neighboring counties into cities like Cedar Rapids provide an ad hoc spotter network for all kinds of phenomenon including severe weather and tornadoes

Repeat this analysis for the state of Kansas and compare/contrast the results and interpretation

Also, compare local Lee with local Moran

```{r}
Ii_stats <- spdep::localmoran(rate, 
                              listw = wts)
Counties.sf$localI = Ii_stats[, 1]

tmap::tm_shape(Counties.sf) +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_fill(col = "localI",
                title = "Local Autocorrelation",
                palette = "Purples") +
  tmap::tm_layout(legend.outside = "TRUE")
```

The clustering of tornado occurrences coincides with the largest population corridor in the state

## Constraining group membership based on spatial autocorrelation {-}

As a spatial data analyst you likely will face the situation in which there are many variables and you need to group them in a way that minimizes inter-group variation but maximizes between-group variation. If you know the number of groups before hand then a common grouping (or clustering) method is called K-means

If your data is spatial you may want the additional constraint that the resulting groups be geographically linked. There are many situations that require separating geographies into discrete but contiguous regions ("regionalization") such as designing communities, planning areas, amenity zones, logistical units, or for setting up experiments with real world geographic constraints

Optimal grouping using only traditional cluster metrics is generally sub-optimal in practice for regionalization because the metrics do not consider geographic contiguity

Unconstrained grouping on data with spatial characteristics may result in contiguous regions because of autocorrelation, but if you want to _ensure_ that all groups are spatially-contiguous you need a method specifically designed for the task. The 'skater' algorithm available in the {spdep} package is well-implemented and well-documented

The 'skater' algorithm (spatial 'k'luster analysis by tree edge removal) builds a connectivity graph to represent spatial relationships between neighboring areas, where each area is represented by a node and edges represent connections between areas. Edge costs are calculated by evaluating the dissimilarity in attribute space between neighboring areas. The connectivity graph is reduced by pruning edges with higher dissimilarity

Consider again the crime data at the tract level in the city of Columbus, Ohio. The tract polygons are projected with arbitrary spatial coordinates

```{r}
( CC.sf <- sf::st_read(dsn = here::here("data", "columbus"),
                       layer = "columbus") )
```

First, create choropleth maps of housing value, income, and crime.
```{r}
tmap::tm_shape(CC.sf) +
  tmap::tm_fill(col = c("HOVAL", "INC", "CRIME"))
```

The maps show distinct regional patterns. Housing values and income are clustered toward the southeast and crime is clustered in the center. But although housing values are also high in the north you don't necessarily want to group that tract with those in the southeast because they are geographically distinct

To group these patterns under the constraint of spatial contiguity you first scale the attribute values and center them using the `scale()` function. Scaling and centering variables should be done before any type of clustering procedure

```{r}
( CCs.df <- CC.sf |> 
    dplyr::mutate(HOVALs = scale(HOVAL),
                  INCs = scale(INC),
                  CRIMEs = scale(CRIME)) |>
    dplyr::select(HOVALs, INCs, CRIMEs) |>
    sf::st_drop_geometry() )
```

Next create adjacency neighbors using queen contiguity. Note queen contiguity is the default but here you specify `TRUE` as a reminder that you can change this

```{r}
nbs <- spdep::poly2nb(CC.sf, 
                      queen = TRUE)

plot(CC.sf$geometry)
plot(nbs, 
     sf::st_centroid(sf::st_geometry(CC.sf)),
     add = TRUE)
```

Next combine the contiguity graph with your scaled attribute data to calculate edge costs based on distances between each node. The function `spdep::nbcosts()` provides distance methods for Euclidean, Manhattan, Canberra, binary, Minkowski, and Mahalanobis, and defaults to Euclidean if not specified like here. Save the object of class `nbdist` with name `edge_costs`

```{r}
edge_costs <- spdep::nbcosts(nbs, 
                             data = CCs.df)
```

Next transform the edge costs into spatial weights using the `spdep::nb2listw()` function before constructing the minimum spanning tree with the weights list

```{r}
wts <- spdep::nb2listw(nbs,
                       glist = edge_costs,
                       style = "B")
mst <- spdep::mstree(wts)

head(mst)
```

Edges with higher dissimilarity are removed leaving a set of nodes and edges that take the minimum sum of dissimilarities across all edges of the tree (a minimum spanning tree).

The edge connecting node (tract) 41 with node (tract) 32 has a dissimilarity of .58 units. The edge connecting tract 32 with tract 23 has a dissimilarity of .56 units

Finally, the `spdep::skater()` function partitions the graph by identifying which edges to remove based on dissimilarity while maximizing the between-group variation. The `ncuts =` argument specifies the number of partitions to make, resulting in `ncuts` + 1 groups

```{r}
clus5 <- spdep::skater(edges = mst[, 1:2], 
                       data = CCs.df, 
                       ncuts = 4)
```

Where are these groups located?

```{r}
CC.sf <- CC.sf |>
  dplyr::mutate(Group = clus5$groups)

library(ggplot2)

ggplot() +
  geom_sf(data = CC.sf,
          mapping = aes(fill = factor(Group)))
```

The map shows five distinct regions based on the three variables of income, housing value, and crime. Importantly the regions are contiguous

Region 2 encompasses most tracts in the urban core where housing values and income are low and crime rates are highest. Regions 4 and 1 in the east and west are where housing values and income are moderately high and crime rates are lower. Region 5 is where income and housing values are highest and crime is the lowest

As a comparison, here is the result of grouping the same three variables using hierarchical clustering using the method of minimum variance (Ward) and without regard to spatial contiguity

```{r}
dd <- dist(CCs.df)
hc <- hclust(dd, 
             method = "ward.D")
hcGroup <- cutree(hc, k = 5)

CC.sf <- CC.sf |>
  dplyr::mutate(hcGroup = hcGroup)

ggplot() +
  geom_sf(data = CC.sf,
          mapping = aes(fill = factor(hcGroup)))
```

Here the map shows five regions but the regions are not contiguous.

More information: <https://www.tandfonline.com/doi/abs/10.1080/13658810600665111>

Also the {motif} package has functions that implement and extend ideas of the pattern-based spatial analysis. They can be used to describe spatial patterns of categorical raster data for any defined regular and irregular areas

Patterns are represented quantitatively using built-in signatures based on co-occurrence matrices but the functions are flexible to allow for user-defined functions. Functions enable spatial analysis such as search, change detection, and clustering to be performed on spatial patterns <https://jakubnowosad.com/motif/>

## Estimating spatial autocorrelation in model residuals {-}

A spatial regression model should be entertained for your data whenever the residuals from an ordinary-least-squares (OLS) regression model exhibit significant spatial autocorrelation

So you first fit an OLS regression model regressing the response variable onto the explanatory variables and then check for autocorrelation in the residuals. If there is significant spatial autocorrelation in the residuals then you should consider some type of spatial regression model

Staying with the crime data at the tract level in the city of Columbus, Ohio

```{r}
CC.sf <- sf::st_read(dsn = here::here("data", "columbus"),
                     layer = "columbus")
```

`CRIME` is the response variable and `INC` and `HOVAL` as the explanatory variables. How well do these two explanatory variables statistically explain the amount of crime at the tract level?

An answer to this question is obtained by regressing crime onto income and housing values. Here you use the `lm()` function and save the results to the object `model.ols`

Set the formula, then use the formula as the first argument in the `lm()` function. Summarize the results with the `summary()` method

```{r}
f <- CRIME ~ INC + HOVAL

model.ols <- lm(f, 
                data = CC.sf)
summary(model.ols)
```

The model statistically explains 55% of the variation in crime as is seen by looking at the multiple R-squared value

Looking at the coefficients (values under the `Estimate` column), you see that _higher_ incomes are associated with _lower_ values of crime (negative coefficient) and _higher_ housing values are associated with _lower_ crime. For every one unit increase in income, crime values decrease by 1.6 units

Use the `residuals()` method to extract the vector of residuals from the model object

```{r}
( res <- residuals(model.ols) )
```

There are 49 residuals, one for each tract. The residuals are the difference between the observed crime rates and the predicted crime rates (observed - predicted). A residual that has a value greater than 0 indicates that the model _under_ predicts the observed crime rate in that tract and a residual that has a value less than 0 indicates that the model _over_ predicts the observed crime rate.

A normal distribution should be a good approximation to the distribution of the residuals. You check this with the `sm::sm.density()` function with the first argument the vector of residuals (`res`) and the argument `model =` set to "Normal"

```{r}
sm::sm.density(res, 
               model = "Normal")
```

The density curve of the residuals (black line) fits completely within the blue ribbon that defines a normal distribution

Next create a map of the model residuals. Do the residuals show any pattern of clustering? Since the values in the vector of residuals `res` are arranged in the same order as the rows in the simple feature data frame you create a new column in the data frame using the `$` syntax and calling the new column `res`

```{r}
CC.sf$res <- res

tmap::tm_shape(CC.sf) +
  tmap::tm_fill(col = "res") +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_layout(title = "Linear model residuals")
```

The map shows contiguous tracts with negative residuals across the southwestern and southern part of the city and a group of contiguous tracts with positive residuals toward the center

The map indicates some clustering but the clustering appears to be less than with the crime values themselves. That is, after accounting for regional factors related to crime, the autocorrelation is reduced

To determine the amount of autocorrelation in the residuals use the `spdep::lm.morantest()` function, passing the regression model object and the weights object to it. Note that you once again use the default neighborhood and weighting schemes to generate the weights matrix `wts`

```{r}
nbs <- CC.sf |>
  spdep::poly2nb()
wts <- nbs |>
  spdep::nb2listw()

model.ols |>
  spdep::lm.morantest(listw = wts)
```

Moran's I on the model residuals is .22. This compares with the value of .5 on the value of crime alone

```{r}
m <- CC.sf$CRIME |>
  length()
s <- wts |>
  spdep::Szero()

CC.sf$CRIME |>
  spdep::moran(listw = wts, 
               n = m, 
               S0 = s)
```

Part of the autocorrelation in the crime rates is statistically 'absorbed' by the explanatory factors

The $p$-value on I of .002, thus you reject the null hypothesis of no spatial autocorrelation in the model residuals and conclude that a spatial regression model would be an improvement over the non-spatial OLS model. The $z$-value (as the basis for the $p$-value) takes into account the fact that these are residuals from a model so the variance is adjusted accordingly

Given significant spatial autocorrelation in the model residuals, the next step is to choose the type of spatial regression model

## Choosing a spatial regression model {-}

Ordinary least-squares regression models fit to spatial data can lead to improper inference because observations are not independent. This might lead to poor policy decisions. If the residuals from an OLS model are strongly correlated the model is not specified properly

You can try to improve the model by adding variables. If that's not possible (no additional data, or no clue as to what variable to include), you can try a spatial regression model. Spatial regression models are widely used in econometrics and epidemiology

The equation for a regression model in vector notation is

$$
y = X \beta + \varepsilon
$$
where $y$ is a $n$ by 1 vector of response variable values, $X$ is a $n$ by $p+1$ matrix containing the explanatory variables and augmented by a column of ones for the intercept term, $\beta$ is a $p+1$ $\times$ 1 vector of model coefficients and $\varepsilon$ is a $n$ by 1 vector of residuals (iid: independent and identically distributed)

A couple options exist if the elements of the vector $\varepsilon$ are correlated. One is to include a spatial lag term so the model becomes

$$
y = \rho W y + X \beta + \varepsilon
$$

where $Wy$ is the weighted average of the neighborhood response values with $W$ the spatial weights matrix, and $\rho$ is the autoregression coefficient. This is called a spatial autoregressive (SAR) model

Note: $Wy$ is the spatial lag variable you compute with the `spdep::lag.listw()` function and $\rho$ is Moran's I. Thus the model is also called a spatial lag model (SLM)

Justification for the spatial lag model is domain specific but motivated by a 'diffusion' process. The response variable $y_i$ is influenced by the explanatory variables at location $i$ and by explanatory variables at locations $j$

$\rho Wy$ is called the _spatial signal_ term and $\beta X$ is called the _trend_ term

Another option is to include a spatial error term so the model becomes

$$
y = X\beta + \lambda W \epsilon + u
$$

where $\lambda$ is the autoregression coefficient, $W\epsilon$ is the spatial error term representing the weighted average of the neighborhood residuals, and $u$ are the overall residuals assumed to be iid. This is called a spatial error model (SEM)

Here the lag term is computed using the residuals rather the response variable

Application of the spatial error model is motivated by the omitted variable bias. Suppose the variable $y$ is statistically described by two variables $x$ and $z$ each centered on zero and independent. Then

$$
y = \beta x + \theta z
$$

If $z$ is not observed, then the vector $\theta z$ is nested in the error term $\epsilon$

$$
y = \beta x + \epsilon
$$

Examples of an unobserved latent variable $z$ include local culture, social capital, neighborhood readiness. Importantly you would expect the latent variable to be spatially correlated (e.g., culture will be similar across neighborhoods), so let

$$
z = \lambda W z + r\\
z = (I - \lambda W)^{-1} r
$$

where $r$ is a vector of random independent residuals (e.g., culture is similar but not identical), $W$ is the spatial weights matrix and $\lambda$ is a scalar spatial correlation parameter. Substituting into the equation above

$$
y = \beta x + \theta z \\
y = \beta x +   \theta (I - \lambda W)^{-1} r\\
y = \beta x + (I - \lambda W)^{-1} \varepsilon
$$

where 

$$
\varepsilon = \theta r
$$

Another motivation for considering a spatial error model is heterogeneity. Suppose you have multiple observations for each unit. If you want a model that incorporates individual effects you can include a $n \times 1$ vector $a$ of individual intercepts for each unit

$$
y = a + X\beta
$$

where now $X$ is a $n$ $\times$ $p$ matrix

In a cross-sectional setting with one observation per unit (typically the case in observational studies), this approach is not possible since you will have more parameters than observations

Instead you can treat $a$ as a vector of spatial random effects. You assume that the intercepts follows a spatially smoothed process

$$
a = \lambda W a + \epsilon \\
a = (I - \lambda W)^{-1} \epsilon
$$

which leads to the previous model

$$
y = X\beta + (I - \lambda W)^{-1} \epsilon
$$

In the absence of domain-specific knowledge of the process that might be responsible for the spatially autocorrelated residuals, you can run some statistical tests on the linear model

The tests are performed with the `spdep::lm.LMtests()` function. The `LM` stands for 'Lagrange multiplier' indicating that the technique simultaneously determines the coefficients on the explanatory variables AND the coefficient on the spatial lag variable

The test type is specified as a character string. The tests should be considered in a sequence starting with the standard versions and moving to the 'robust' versions if the choice remains ambiguous

To perform LM tests you specify the model object, the weights matrix, and the two model types using the `test =` argument. The model types are specified as character strings `"LMerr"` and `"LMlag"` for the spatial error and lag models, respectively

```{r}
model.ols |>
  spdep::lm.LMtests(listw = wts, 
                    test = c("LMerr", "LMlag"))
```

The output shows that both the spatial error and spatial lag models are significant ($p$-value < .15). Ideally one model is significant and the other is not, and you choose the model that is significant

Since both are significant, you test again. This time you use the robust forms of the statistics with character strings `"RLMerr"` and `"RLMlag"` in the `test =` argument

```{r}
model.ols |>
  spdep::lm.LMtests(listw = wts, 
                    test = c("RLMerr", "RLMlag"))
```

Here the error model has a large $p$-value and the lag model has a $p$-value that is less than .15 so you choose the lag model for your spatial regression

A decision tree (from Luc Anselin) shows the sequence of tests for making a choice about which type of spatial model to use [Decision Tree](http://myweb.fsu.edu/jelsner/temp/SpatialRegressionDecisionTree.png)

If both tests show significance models, then you should fit both models and check which one results in the lowest information criteria (AIC)

Another options is to include both a spatial lag term and a spatial error term into a single model

Ordinary least-squares regression models fit to spatial data can lead to improper inference because observations are not independent. This might lead to poor policy decisions. Thus it's necessary to check the residuals from an aspatial model for autocorrelation. If the residuals are strongly correlated the model is not specified properly