# Thursday October 27, 2022 {.unnumbered}

**"Good code is its own best documentation. As you're about to add a comment, ask yourself, 'How can I improve the code so that this comment isn't needed?' Improve the code and then document it to make it even clearer."** - Steve McConnell

Today

- Determining the statistical significance of event clustering
- Estimating event clustering in multi-type event locations
- Interpolating output from a distance function
- Inferring event interaction from a distance function
- Removing duplicate event locations and defining the domain

Thursday we will look at modeling point pattern data. Next Tuesday there will be a lab.

## Determining the statistical significance of event clustering {-}

The plots show a separation between the black solid line and the red line, but is this separation large relative to sampling variation? Is the above difference between the empirical and theoretical distance functions (e.g., $G$) large enough to conclude there is significant clustering?

There are two ways to approach statistical inference. 1) Compare the function computed with the observed data against the function computed data generated under the null hypothesis and ask: does the function fall outside the envelope of functions from the null cases? 2) Get estimates of uncertainty on the function and ask: does the uncertainty interval contain the null case? 

With the first approach you take a `ppp` object and then compute the function of interest (e.g., Ripley's K) for a specified number of samples under the null hypothesis of a homogeneous Poisson process.

To make things run faster you consider a subset of all the tornadoes (those that have an EF rating of 2 or higher). You create a new `ppp` object that contains only tornadoes rated at least EF2. Since the marks is a factor vector you can't use `>=`.

```{r}
ST.ppp <- unmark(T.ppp[T.ppp$marks == 2 | 
                       T.ppp$marks == 3 | 
                       T.ppp$marks == 4 |
                       T.ppp$marks == 5])
plot(ST.ppp)
```

The `envelope()` method from the {spatstat} package is used on this new `ST.ppp` object. You specify the function with the `fun = Kest` argument and the number of samples with the `nsim =` argument. You then convert the output to a data frame. It takes a few seconds to complete the computation of $K$ for all 99 samples.

```{r}
Kenv.df <- envelope(ST.ppp, 
                    fun = Kest, 
                    nsim = 99) |>
  as.data.frame()

head(Kenv.df)
```

The resulting data frame contains estimates of Ripley's $K$ as a function of lag distance (`r`) (column labeled `obs`). It also has the estimates of $K$ under the null hypothesis of CSR (`theo`) and the lowest (`lo`) and highest (`hi`) values of $K$ across the 99 samples.

You plot this information using the `geom_ribbon()` layer to include a gray ribbon around the model of CSR.

```{r}
ggplot(data = Kenv.df, 
       mapping = aes(x = r, y = obs * intensity(ST.ppp))) +
  geom_ribbon(mapping = aes(ymin = lo * intensity(ST.ppp), 
                            ymax = hi * intensity(ST.ppp)), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo * intensity(ST.ppp)), color = "red") +
  xlab("Lag distance (km)") + ylab("K(r)") +
  theme_minimal()
```

The $K$ function computed on the data is the black line and the $K$ function under CSR is the red line. The uncertainty ribbon (gray band) connects the point-wise minimum and maximum values of $K$ computed from the 99 generated point pattern samples.

Since the black line lies outside the gray band you can confidently conclude that the tornado reports are _more_ clustered than one would expect by chance.

If the specific intention is to test a null hypothesis of CSR, then a single statistic indicating the departure of $K$ computed on the observations from the theoretical $K$ is appropriate. 

One such statistic is the maximum absolute deviation (MAD) and is implemented with the `mad.test()` function from the {spatstat} package. The function performs a hypothesis test for goodness-of-fit of the observations to the theoretical model. The larger the value of the statistic, the less likely it is that the data are CSR.

```{r}
mad.test(ST.ppp, 
         fun = Kest, 
         nsim = 99)
```

The maximum absolute deviation is 7297 which is very large so the $p$-value is small and you reject the null hypothesis of CSR for these data. This is consistent with the graph. Note: Since there are 99 simulations the lowest $p$-value is .01.

Another test statistic is related to the sum of the squared deviations between the estimated and theoretical functions. It is implemented with the `dclf.test()` function.

```{r}
dclf.test(ST.ppp, 
          fun = Kest, 
          nsim = 99)
```

Again the $p$-value on the test statistic against the two-sided alternative is less than .01.

Compare these test results on tornado report clustering with test results on pine sapling clustering in the `swedishpines` data set.

```{r}
SP <- swedishpines
Kenv.df <- envelope(SP, 
                    fun = Kest, 
                    nsim = 99) |>
  as.data.frame()

ggplot(data = Kenv.df, 
       mapping = aes(x = r * .1, y = obs * intensity(SP))) +
  geom_ribbon(aes(ymin = lo * intensity(SP),
                  ymax = hi * intensity(SP)), 
              fill = "gray70") +
  geom_line() + geom_line(aes(y = theo * intensity(SP)), 
                          color = "red") +
  xlab("Lag distance (m)") + 
  ylab("K(r), Expected number of additional saplings\n within a distance r of a sapling") +
  theme_minimal()
```

At short distances (closer than about 1 m) the black line is below the red line and outside the gray ribbon which you interpret to mean that there are fewer pine saplings near other pine saplings than would be expected by chance at this scale. This 'regularity' might be the result of competition among the saplings.

At larger distances the black line is close to the red line and inside the gray ribbon which you interpret to mean that, at this larger spatial scale, the distribution of pine saplings is indistinguishable from CSR.

Based on the fact that much of the black line is within the gray envelope you might anticipate that a formal test against the null hypothesis of CSR will likely fail to reject.

```{r}
mad.test(SP, 
         fun = Kest, 
         nsim = 99)
dclf.test(SP, 
          fun = Kest, 
          nsim = 99)
```

Both return a $p$-value that is greater than .15 so you fail to reject the null hypothesis of CSR.

In the second approach to inference the procedure of re-sampling is used. Note the distinction: Re-sampling refers to generating samples from the data while sampling, as above, refers to generating samples from some theoretical model.

The bootstrap procedure is a re-sampling strategy whereby new samples are generated from the data by randomly choosing events within the domain. An event that is chosen for the 'bootstrap' sample gets the chance to be chosen again (called 'with replacement'). The number of events in each bootstrap sample must equal the number of events in the data.

Consider 15 numbers from 1 to 15. Then pick randomly from that set of numbers with replacement until the sample size is 15 to create a bootstrap sample.

```{r}
( x <- 1:15 )
sample(x, replace = TRUE)
```

Some numbers get picked more than once and some do not get picked at all.

The average of the original 15 `x` values is 8 but the average over the set of numbers in the bootstrap sample will not necessarily be 8. However, the distribution of the averages over many bootstrap samples will be centered close to this average.

```{r}
mx <- NULL
for(i in 1:99){
  mx[i] <- mean(sample(x, replace = TRUE))
}

mx.df <- as.data.frame(mx)
  ggplot(data = mx.df,
         mapping = aes(mx)) +
    geom_density() +
    geom_vline(xintercept = mean(x),
               color = "red")
```

The important thing is that the bootstrap distribution provides an estimate of the uncertainty on the computed mean through the range of possible average values.

In this way, the `lohboot()` function estimates the uncertainty on the computed spatial statistic using a bootstrap procedure. It works by computing a local version of the function (e.g., `localK()`) on the set of re-sampled events.

```{r}
Kboot.df <- ST.ppp |>
  lohboot(fun = Kest) |>
  as.data.frame()

ggplot(data = Kboot.df, 
       mapping = aes(x = r, y = iso * intensity(ST.ppp))) +
  geom_ribbon(aes(ymin = lo * intensity(ST.ppp), 
                  ymax = hi * intensity(ST.ppp)), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo * intensity(ST.ppp)), color = "red") +
  xlab("Lag distance (km)") + ylab("K(r)") +
  theme_minimal()
```

Now the uncertainty band is plotted about the black line ($K$ function computed on the observations) rather than about the null model (red line). The 95% uncertainty band does to include the CSR model so you confidently conclude that the tornadoes in Kansas are more clustered than chance.

Repeating for the Swedish pine saplings.

```{r}
Kboot.df <- SP |>
  lohboot(fun = Kest) |>
  as.data.frame()

ggplot(Kboot.df, aes(x = r * .1, y = iso * intensity(SP))) +
  geom_ribbon(aes(ymin = lo * intensity(SP), 
                  ymax = hi * intensity(SP)), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo * intensity(SP)), color = "blue", lty = 'dashed') +
  xlab("Lag distance (m)") + ylab("K(r)") +
  theme_minimal()
```

At short distances (closer than about 1.5 m) the gray ribbon is below the blue line which you interpret to mean that there are fewer pine saplings near other pine saplings than would be expected by chance at this scale indicating regularity.


## Estimating event clustering in multi-type event locations {-}

Often the interest focuses on whether the occurrence of one event type influences (or is influenced by) another event type. For example, does the occurrence of one species of tree influence the occurrence of another species?

Analogues to the $G$ and $K$ functions are available for 'multi-type' point patterns where the marks are factors.

A common statistic for examining 'cross correlation' of event type occurrences is the cross $K$ function $K_{ij}(r)$, which estimates the expected number of events of type $j$ within a distance $r$ of type $i$.

Consider the data called `lansing` from the {spatstat} package that contains the locations of 2,251 trees of various species in a wooded lot in Lansing, MI as a `ppp` object.

```{r}
data(lansing)
summary(lansing)
```

The data are a multi-type planar point pattern with the marks indicating tree species. There are 135 black oaks, 703 hickories, etc. The spatial unit is 924 feet.

Compute and plot the cross $K$ function for Maple and Hickory trees.

```{r}
Kc.df <- lansing |>
  Kcross(i = "maple",
         j = "hickory") |>
  as.data.frame()
 
ggplot(data = Kc.df, 
       mapping = aes(x = r, y = iso)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  geom_vline(xintercept = .2, lty = 'dashed') +
  geom_hline(yintercept = .093, lty = 'dashed') +
  geom_hline(yintercept = .125, lty = 'dashed') +
  xlab("Distance") + ylab("Kc(r)") +
  theme_minimal()
```

The vertical axis is the number of hickory trees within a radius r of a maple tree divided by the average intensity of the hickories. So at a distance of .2 (.2 x 924 ft = 180 ft) from a random maple there is an average of roughly 65 hickories (.093 x 703 hickories). If hickory and maple trees are CSR you would expect about 88 maples (.125 * 703) within that distance.

The presence of a hickory tree reduces the likelihood that a maple tree will be nearby.

Do the same for the EF1 and EF3 tornadoes in Kansas.

```{r}
plot(Kcross(T.ppp, 
            i = "1", 
            j = "3"))
abline(v = 70)
abline(h = 18700)
abline(h = 15500)

Kc.df <- T.ppp |>
  Kcross(i = "1", 
         j = "3") |>
  as.data.frame()
ggplot(data = Kc.df, 
       mapping = aes(x = r, y = iso)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  geom_vline(xintercept = 70, lty = 'dashed') +
  geom_hline(yintercept = 18700, lty = 'dashed') +
  geom_hline(yintercept = 15500, lty = 'dashed') +
  xlab("Distance") + ylab("Kc(r)") +
  theme_minimal()
```

The vertical axis is the number of EF3 tornadoes within a radius r of an EF1 tornado divided by the average intensity of the EF3 tornadoes. At a distance of 70 km from a random EF1 tornado there are on average 18500 x .000296 = 5.5 EF3 tornadoes. If EF1 and EF3 tornadoes are CSR then you would expect, on average, somewhat fewer EF3 tornadoes in the vicinity of EF1 tornadoes (15500 x .000296 = 4.6).

You can see this more clearly by using the `envelope()` function with the `fun = Kross`. You first use the `subset()` method with `drop = TRUE` to make a new `ppp` object with only those two groups.
```{r}
T.ppp13 <- subset(T.ppp,
                  marks == "1" |
                  marks == "3",
                  drop = TRUE)

Kcenv.df <- T.ppp13 |>
  envelope(fun = Kcross,
           nsim = 99) |>
  as.data.frame()

ggplot(data = Kcenv.df, 
       mapping = aes(x = r, y = obs)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo), color = "red", lty = 'dashed') +
  xlab("Lag distance (km)") + ylab("Kc(r)") +
  theme_minimal()
```

And you can formally test as before using the `mad.test()` function.
```{r}
mad.test(T.ppp13, fun = Kcross, nsim = 99)
dclf.test(T.ppp13, fun = Kcross, nsim = 99)
```

Both tests lead you to conclude that EF3 tornadoes are more likely near EF1 tornadoes than would be expected if they were independently CSR.

## Interpolating the output from a distance function {-}

Compute Ripley $K$ and look at the classes of the resulting object.

```{r}
K <- Kest(T.ppp)
class(K)
```

It has two classes `fv` and `data.frame`. It is a data frame but with additional attribute information. You focus on the data frame portion. 
```{r}
K.df <- as.data.frame(K)
head(K.df)
```

In particular you  want the values of `r` and `iso`. The value of `iso` times the average spatial intensity is the number of tornadoes within a distance `r`.

You add this information to the data frame.
```{r}
K.df <- K.df |>
  dplyr::mutate(nT = summary(T.ppp)$intensity * iso)
```

Suppose you are interested in the average number of tornadoes at a distance of exactly 50 km. Use the `approx()` function to interpolate the value of `nT` at a distance of 50 km.
```{r}
approx(x = K.df$r, 
       y = K.df$nT,
       xout = 50)$y
```

Finally, the variance stabilized Ripley $K$ function called the $L$ function is often used instead of $K$.  The sample version of the $L$ function is defined as
$$
\hat{L}(r) = \Big( \hat{K}(r)/\pi\Big)^{1/2}.
$$

For data that is CSR, the $L$ function has expected value $r$ and its variance is approximately constant in $r$. A common plot is a graph of $r - \hat{L}(r)$ against $r$, which approximately follows the horizontal zero-axis with constant dispersion if the data follow a homogeneous Poisson process.

## Inferring event interaction from a distance function {-}

The distance functions ($G$, $K$, etc) that are used to quantify clustering are defined and estimated under the assumption that the process that produced the events is stationary (homogeneous). If this is true then you can treat any sub-region of the domain as an independent and identically distributed (iid) sample from the entire set of data.

If the spatial distribution of the event locations is influenced by event interaction then the functions will deviate from the theoretical model of CSR. But a deviation from CSR does not imply event interaction. 

Moreover, the functions characterize the spatial arrangement of event locations 'on average' so variability in an interaction as a function of scale may not be detected.

As an example of the latter case, here you generate event locations at random with clustering on a small scale but with regularity on a larger scale. On average the event locations are CSR as indicated by the $K$ function.

```{r}
suppressMessages(library(spatstat))

set.seed(0112)
X <- rcell(nx = 15)
plot(X, main = "")
```

There are two 'local' clusters one in the north and one in the south. But overall the events appear to be more regular (inhibition) than CSR. 

Interpretation of the process that created the event locations based on Ripley's $K$ would be that the arrangement of events is CSR.

```{r}
library(ggplot2)

K.df <- X |>
  Kest() |>
  as.data.frame()

ggplot(K.df, aes(x = r, y = iso)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  xlab("Lag distance (km)") + ylab("K(r)") +
  theme_minimal()
```

The empirical curve (black line) coincides with the theoretical CSR line (red line) indicating CSR.

And the maximum absolute deviation test under the null hypothesis of CSR returns a large $p$-value so you fail to reject it.

```{r}
mad.test(X, fun = Kest, nsim = 99)
```

As an example of the former case, here you generate event locations that have no inter-event interaction but there is a trend in the spatial intensity.

```{r}
X <- rpoispp(function(x, y){ 300 * exp(-3 * x) })
plot(X, main = "") 
```

By design there is a clear trend toward fewer events moving toward the east.

You compute and plot the $K$ function on these event locations.

```{r}
K.df <- X |>
  Kest() |>
  as.data.frame()

ggplot(K.df, aes(x = r, y = iso)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  xlab("Lag distance (km)") + ylab("K(r)") +
  theme_minimal()
```

The $K$ function indicates clustering but this is an artifact of the trend in the intensity.

In the case of a known trend in the spatial intensity, you need to use the `Kinhom()` function. For example, compare the uncertainty envelopes from a homogeneous and inhomogeneous Poisson process. 

Start by plotting the output from the `envelope()` function with `fun = Kest`. The `global = TRUE` argument indicates that the envelopes are simultaneous rather than point-wise (`global = FALSE` which is the default). Point-wise envelopes assume the estimates are independent (usually not a good assumption) across the range of distances so the standard errors will be smaller resulting in narrower bands.

```{r}
Kenv <- envelope(X, 
                 fun = Kest, 
                 nsim = 39, 
                 rank = 1, 
                 global = TRUE)
Kenv.df <- as.data.frame(Kenv)

ggplot(Kenv.df, aes(x = r, y = obs)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo), color = "red", lty = 'dashed') +
  xlab("Lag distance (km)") + ylab("K(r)") +
  theme_minimal()
```

After a distance of about .15 units the empirical curve (black line) is outside the uncertainty band indicating the events are more clustered than CSR.

However when you use the `fun = Kinhom` the empirical curve is completely inside the uncertainty band.

```{r}
Kenv <- envelope(X, 
                 fun = Kinhom, 
                 nsim = 99, 
                 rank = 1, 
                 global = TRUE)
Kenv.df <- as.data.frame(Kenv)

ggplot(Kenv.df, aes(x = r, y = obs)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo), color = "red", lty = 'dashed') +
  xlab("Lag distance (km)") + ylab("K(r), Expected number of additional events\n within a distance r of an event") +
  theme_minimal()
```

You conclude that the point pattern data are consistent with an inhomogeneous Poisson process without event interaction.

Let's return to the Kansas tornadoes (EF1+). You import the data and create a point pattern object windowed by the state borders.

```{r}
Torn.sf <- sf::st_read(dsn = here::here("data", "1950-2020-torn-initpoint")) |>
  sf::st_transform(crs = 3082) |>
  dplyr::filter(mag >= 1, yr >= 1994) |>
  dplyr::mutate(EF = as.factor(mag)) |>
  dplyr::select(EF)

ST.ppp <- Torn.sf["EF"] |>
  as.ppp()

KS.sf <- USAboundaries::us_states(states = "Kansas") |>
  sf::st_transform(crs = sf::st_crs(Torn.sf)$proj4string)

W <- KS.sf |>
  as.owin()

ST.ppp <- ST.ppp[W] |>
  spatstat.geom::rescale(s = 1000, 
                         unitname = "km")
plot(ST.ppp)
```

There are more tornado reports in the west than in the east, especially across the southern part of the state indicating the process producing the events is not homogeneous. This means there are other factors contributing to local event intensity.

Evidence for clustering must account for this inhomogeneity. Here you do this by computing the envelope around the inhomogeneous Ripley K function using the argument `fun = Kinhom`.
```{r}
Kenv <- envelope(ST.ppp,
                 fun = Kinhom,
                 nsim = 39,
                 rank = 1,
                 global = TRUE)

Kenv.df <- as.data.frame(Kenv)

ggplot(Kenv.df, aes(x = r, y = obs)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo), color = "red", lty = 'dashed') +
  xlab("Lag distance (km)") + ylab("K(r)") +
  theme_minimal()
```

The output reveals no evidence of clustering at distances less than about 70 km. At greater distances there is evidence of regularity indicated by the black line significantly below the red line. This is due to the fact that tornado reports are more common near cities and towns and cities and towns tend to be spread out more regular than CSR.

## Removing duplicate event locations and defining the domain {-}

The functions in the {spatstat} package require the event locations (as a `ppp` object) and a domain over which the spatial statistics are computed (as an `owin` object).

If no `owin` object is specified, the statistics are computed over a rectangle (bounding box) defined by the northern most, southern most, eastern most, and western most event locations.

To see this, consider the Florida wildfire data as a simple feature data frame. Extract only fires occurring in Baker County (west of Duval County--Jacksonville). Include only wildfires started by lightning and select the fire size variable.
```{r}
FL_Fires.sf <- sf::st_read(dsn = here::here("data", "FL_Fires")) |>
  sf::st_transform(crs = 3086)

Baker.sf <- USAboundaries::us_counties(states = "FL") |>
  dplyr::select(name) |>
  dplyr::filter(name == "Baker") |>
  sf::st_transform(crs = 3086)

BakerFires.sf <- FL_Fires.sf |>
  sf::st_intersection(Baker.sf) |>
  dplyr::filter(STAT_CAU_1 == "Lightning") |>
  dplyr::select(FIRE_SIZE_)
```

Create a `ppp` object and an unmarked `ppp` object. Summarize the unmarked object and make a plot.
```{r}
BF.ppp <- BakerFires.sf |>
  as.ppp() 

BFU.ppp <- unmark(BF.ppp)

summary(BFU.ppp)
plot(BFU.ppp)
```

The average intensity is 18 wildfires per 10 square km. But the intensity is based on a square domain. The lack of events in the northeast part of the domain is due to the fact that you removed wildfires outside the county border.

Further, two event locations are identical if their x,y coordinates are the same, and their marks are the same (if they carry marks).

Remove duplicate events with the `unique()` function, set the domain to be the county border, and set the name for the unit of length to meters.

```{r}
BFU.ppp <- unique(BFU.ppp)

W <- Baker.sf |>
  as.owin()

BFU.ppp <- BFU.ppp[W]

unitname(BFU.ppp) <- "meters"

summary(BFU.ppp)
plot(BFU.ppp)
```

Now the average intensity is 21 wildfires per 10 sq. km.

Apply Ripley's $K$ function and graph the results.
```{r}
K.df <- BFU.ppp |>
  Kest() |>
  as.data.frame()

ggplot(K.df, aes(x = r, y = iso * intensity(BFU.ppp))) +
  geom_line() +
  geom_line(aes(y = theo * intensity(BFU.ppp)), color = "red") +
  xlab("Lag distance (m)") + ylab("K(r), Expected number of additional wildfires\n within a distance r of any wildfire") +
  theme_minimal()
```

We see a difference indicating a cluster of event locations, but is the difference significant against a null hypothesis of a homogeneous Poisson?
```{r}
Kenv.df <- envelope(BFU.ppp, 
                    fun = Kest, 
                    nsim = 39, 
                    rank = 1, 
                    global = TRUE) |>
  as.data.frame()

ggplot(Kenv.df, aes(x = r, y = obs)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo), color = "red", lty = 'dashed') +
  xlab("Lag distance (m)") + ylab("K(r)") +
  theme_minimal()
```

Yes it is.
