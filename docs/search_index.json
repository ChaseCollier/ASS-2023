[["tuesday-november-15-2022.html", "Tuesday November 15, 2022 Fitting a log-Gaussian Cox model Assessing how well the model fits An example of a point process model in the wild", " Tuesday November 15, 2022 “Sometimes it pays to stay in bed on Monday, rather than spending the rest of the week debugging Monday’s code.” - Christopher Thompson Today Fitting a log-Gaussian Cox model Assessing how well the model fits An example of a point process model in the wild Fitting a log-Gaussian Cox model Last time you saw how to fit a cluster model to point pattern data using the kppm() function from the {spatstat} family of packages. You fit a Thomas model to the maple trees in the Lansing Woods data. The Thomas cluster model is a type of Cox process where the logarithm of the spatial intensity is a sample from a non-negative random variable. If the dispersion of events around other events is uniform rather than Gaussian than you fit a Matérn cluster model. A limitation of the Thomas and Matérn cluster models is that the samples are assumed to be spatially independent. Nearby locations might have different spatial intensities. If there is a systematic trend or covariate influence on the spatial intensity then you can include this in the model. A related but more flexible process is the log-Gaussian Cox process (LGCP). A LGCP has a hierarchical structure, where at the first level the events are assumed to be drawn from a Poisson distribution conditional on the intensity function, and at the second level the log of the intensity function is assumed to be drawn from a Gaussian process. That is, the log spatial intensity values are spatially correlated. The flexibility of the model arises from the Gaussian process prior specified over the log-intensity function. Given this hierarchical structure with a Gaussian process at the second level, fitting this model to observed spatial point pattern data is a computational challenge. One way is through the method of stochastic partial differential equations (SPDE), which involves a probability (Bayesian) framework to approximate posterior distributions. To see how this works and to get a glimpse of the Bayesian framework, here you consider a 1D space and you fit the model using functions from the {inlabru} and {INLA} packages. Example modified from https://inlabru-org.github.io/inlabru/articles/web/1d_lgcp.html Install and make the packages available to this session. library(inlabru) ## Loading required package: sp library(INLA) ## Loading required package: Matrix ## Loading required package: foreach ## Loading required package: parallel ## This is INLA_22.05.07 built 2022-05-07 09:58:26 UTC. ## - See www.r-inla.org/contact-us for how to get help. ## - To enable PARDISO sparse library; see inla.pardiso() library(mgcv) ## Loading required package: nlme ## This is mgcv 1.8-40. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. library(ggplot2) Get the data to model from the {inlabru} package using the data(Poisson2_1D) function. The data are in the object pts2. data(Poisson2_1D) pts2 |&gt; dplyr::glimpse() ## Rows: 130 ## Columns: 1 ## $ x &lt;dbl&gt; 25.1064528, 9.1504482, 17.8802463, 28.0073385, 1.8994489, 8.3805420,… The object pts2 is a one column data frame with column name x. pts2 |&gt; range() ## [1] 0.3293196 51.0375325 The values of x are strictly positive between .33 and 51. Plot the data as points along a horizontal line together with a histogram estimating the 1D spatial intensity. Here you choose about 20 bins across the range of values from 0 to 55. ggplot(data = pts2) + geom_histogram(mapping = aes(x = x), binwidth = 55 / 20, boundary = 0, fill = NA, color = &quot;black&quot;) + geom_point(mapping = aes(x = x), y = 0, pch = &quot;|&quot;, cex = 4) The histogram is a discrete version of the spatial intensity. It shows that events along the horizontal axis tend to be most common near the value of 20. Your goal is a smoothed estimate of the 1-D spatial intensity taking into account event clustering. First create a 1D mesh of 50 points (length.out =) across the range of values (from 0 to 55). The end points of the mesh are unconstrained by setting boundary = \"free\". x &lt;- seq(from = 0, to = 55, length.out = 50) mesh1D &lt;- inla.mesh.1d(loc = x, boundary = &quot;free&quot;) Then specify the spatial correlation with a Matérn cluster model. The first argument is the mesh onto which the model will be built. The argument prior.range = accepts a vector of length two with the first element the lag distance (range) of the spatial correlation and the second element the probability that the range will be less than that value. If the second value is NA, the value of the first element is used as a fixed range. The argument prior.sigma = accepts a vector of length two with the first element the marginal standard deviation of the spatial intensity and the second element the probability that the standard deviation will be greater than that value. Here you are non-committal on the range of spatial correlation so you specify a large distance (150) with a 75% chance that it will be less than that. That is you give a broad range to the prior. Matern &lt;- inla.spde2.pcmatern(mesh = mesh1D, prior.range = c(150, .75), prior.sigma = c(.1, .75)) Next specify the full model and assign it to the object f. f &lt;- x ~ spde1D(x, model = Matern) + Intercept(1) Next fit the model to the actual event locations in pts2. You use the log Gaussian Cox process lgcp() function from the {inlabru} package. The domain = argument specifies the 1D mesh as a list object. model.lgcp &lt;- lgcp(components = f, data = pts2, domain = list(x = mesh1D)) You look at the output posterior distributions of the model parameters using the spde.posterior() function. The function returns x and y values for a plot of the posterior probability density function (PDF) as a data frame, which you plot with the plot.bru() function (plot() method). Start with the probability density function for the range parameter by specifying what = \"range\". spde.posterior(result = model.lgcp, name = &quot;spde1D&quot;, what = &quot;range&quot;) |&gt; plot() The prior range value was specified broadly but the posterior range is focused on values between 2.5 and 5. The output is better viewed on a logarithmic scale by specifying what = \"log.range\". spde.posterior(model.lgcp, name = &quot;spde1D&quot;, what = &quot;log.range&quot;) |&gt; plot() Next you plot the probability density function for the Matérn correlation component of the model. spde.posterior(model.lgcp, name = &quot;spde1D&quot;, what = &quot;matern.correlation&quot;) |&gt; plot() The black line is the posterior median correlation as a function of lag distance. The maximum correlation of 1 at zero lag distance decays to .5 correlation out at a distance of about 20 units. You can get a feel for sensitivity to priors by specifying different priors and looking at these posterior plots. Always a good idea when fitting models using Bayesian methods. For example, change the prior range from 150 to 30 and refit, then compare the probability density function of the Matérn correlation. You predict on the ‘response’ scale [i.e. the intensity function \\(\\lambda\\)(s)] as follows. First set up a data frame of explanatory values at which to predict (here grid.df). Then use the predict() method with data = grid.df and formula = ~ exp(spde1D + Intercept. It takes a few seconds to make predictions at each grid point location. grid.df &lt;- data.frame(x = seq(from = 0, to = 55, by = 1)) pred.df &lt;- predict(model.lgcp, data = grid.df, formula = ~ exp(spde1D + Intercept)) The output is a data frame containing the locations on the grid (x) and the corresponding summary statistics (mean, median, standard deviation, and quantiles) on the posterior predictions at those location. pred.df |&gt; dplyr::glimpse() ## Rows: 56 ## Columns: 9 ## $ x &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, … ## $ mean &lt;dbl&gt; 1.900890, 1.967191, 2.056187, 2.162741, 2.281678, 2.41… ## $ sd &lt;dbl&gt; 0.7837649, 0.6085022, 0.5085471, 0.4657688, 0.4515758,… ## $ q0.025 &lt;dbl&gt; 0.8236974, 1.0336339, 1.2361107, 1.3969261, 1.5240599,… ## $ q0.5 &lt;dbl&gt; 1.745684, 1.885463, 2.030922, 2.147504, 2.242965, 2.38… ## $ q0.975 &lt;dbl&gt; 3.704594, 3.337070, 3.211471, 3.093691, 3.153309, 3.33… ## $ median &lt;dbl&gt; 1.745684, 1.885463, 2.030922, 2.147504, 2.242965, 2.38… ## $ mean.mc_std_err &lt;dbl&gt; 0.07837649, 0.06085022, 0.05085471, 0.04657688, 0.0451… ## $ sd.mc_std_err &lt;dbl&gt; 0.08380199, 0.04453279, 0.03366782, 0.03234552, 0.0341… You pass this data frame to the plot() method to produce the following prediction plot using the grammar of graphics. plot(pred.df, color = &quot;red&quot;) + geom_point(data = pts2, mapping = aes(x = x), y = 0, pch = &quot;|&quot;, cex = 2) + xlab(&quot;x&quot;) + ylab(&quot;Spatial intensity\\n number of events per unit interval&quot;) + theme_minimal() The LGCP model provides a smoothed spatial intensity of the events and a 95% credible interval about the intensity at each grid location. The intensity values are the number of events per unit interval. How does this compare with the intensity function that generated the data? The function lambda2_1D( )in the data object Poission2_1D calculates the true intensity that was used in simulating the data. To plot this function you make a data frame with x- and y-coordinates giving the true intensity function, \\(\\lambda(s)\\). Here you use 150 x-values to get a smooth plot. xs &lt;- seq(from = 0, to = 55, length = 150) true.lambda &lt;- data.frame(x = xs, y = lambda2_1D(xs)) Now plot the LGCP model predicted values together with the true intensity function. plot(pred.df, color = &quot;red&quot;) + geom_point(data = pts2, mapping = aes(x = x), y = 0, pch = &quot;|&quot;, cex = 2) + geom_line(data = true.lambda, mapping = aes(x, y)) + xlab(&quot;x&quot;) + ylab(&quot;Spatial intensity&quot;) + theme_minimal() The match is pretty good. Keep in mind that the data represents just one sample generated from the You can look at the goodness-of-fit of the model using the function bincount( ), which plots the 95% credible intervals in a set of bins along the x-axis together with the observed count in each bin. bc &lt;- bincount( result = model.lgcp, observations = pts2, breaks = seq(from = 0, to = max(pts2), length = 12), predictor = x ~ exp(spde1D + Intercept) ) attributes(bc)$ggp The credible intervals are shown as red rectangles, the mean fitted value as a short horizontal blue line, and the observed data as black points. Abundance is the integral of the intensity over the entire space. Here space is 1D and you estimate the abundance by integrating the predicted intensity over the range of x. Integration is done as a weighted sum of the intensities. The locations along the x axis and their weights are constructed using the ipoints() function. Here you create 50 equally-space integration points cover the 1D range. The weights are all equal to 55/100. ips &lt;- ipoints(c(0, 55), 100, name = &quot;x&quot;) head(ips) ## x weight group ## 1 0.275 0.55 1 ## 2 0.825 0.55 1 ## 3 1.375 0.55 1 ## 4 1.925 0.55 1 ## 5 2.475 0.55 1 ## 6 3.025 0.55 1 Then compute the abundance over the entire domain with the predict() method. ( Lambda &lt;- predict(model.lgcp, ips, ~ sum(weight * exp(spde1D + Intercept))) ) ## mean sd q0.025 q0.5 q0.975 median mean.mc_std_err ## 1 131.7543 10.36502 115.1424 131.7101 152.352 131.7101 1.036502 ## sd.mc_std_err ## 1 0.6281854 mean is the posterior mean abundance sd is the estimated standard error of the posterior of the abundance q0.025 and q0.975 are the 95% credible interval bounds q0.5 is the posterior median abundance The mean number of events is just over 130 with a standard deviation of 11.5 events. Recall that the LGCP has a hierarchical structure, where at the first level the process is assumed Poisson conditional on the intensity function, and at the second level the log of the intensity function is assumed to be drawn from a Gaussian process. The above posterior values for the abundance takes into account only the variance due to the parameters of the intensity function (2nd level). It neglects the variance in the number of events, given the intensity function (first level). To include both variances you need to modify the input to the predict( ) method. You include a data frame that samples from a Poisson density for each value of the abundance (here N = 50:250). Nest &lt;- predict(model.lgcp, ips, ~ data.frame(N = 50:250, dpois = dpois(50:250, lambda = sum(weight * exp(spde1D + Intercept))))) The result shows the same set of statistics as were calculated for Lambda, but here for every abundance value between 50 and 250, rather than for the posterior mean abundance alone. Nest |&gt; head() ## N mean sd q0.025 q0.5 q0.975 ## 1 50 3.434692e-11 2.686956e-10 1.665395e-22 1.128620e-15 1.652912e-10 ## 2 51 7.022969e-11 5.430209e-10 4.982635e-22 2.852213e-15 3.506754e-10 ## 3 52 1.409225e-10 1.076393e-09 1.462134e-21 7.069409e-15 7.296902e-10 ## 4 53 2.776127e-10 2.093570e-09 4.209807e-21 1.719143e-14 1.489738e-09 ## 5 54 5.371114e-10 3.996914e-09 1.189706e-20 4.103202e-14 2.985204e-09 ## 6 55 1.020979e-09 7.492651e-09 3.301177e-20 9.615348e-14 5.873272e-09 ## median mean.mc_std_err sd.mc_std_err ## 1 1.128620e-15 2.686956e-11 1.313671e-10 ## 2 2.852213e-15 5.430209e-11 2.650385e-10 ## 3 7.069409e-15 1.076393e-10 5.243941e-10 ## 4 1.719143e-14 2.093570e-10 1.017864e-09 ## 5 4.103202e-14 3.996914e-10 1.938901e-09 ## 6 9.615348e-14 7.492651e-10 3.625764e-09 You compute the 95% prediction interval and the median with the inla.qmarginal() function. inla.qmarginal(c(.025, .5, .975), marginal = list(x = Nest$N, y = Nest$mean)) ## [1] 99.44964 128.80636 163.22009 Now compare Lambda to Nest using a plot. First calculate the posterior conditional on the mean of Lambda. Nest$plugin_estimate &lt;- dpois(Nest$N, lambda = Lambda$mean) Then plot it and the unconditional posterior. ggplot(data = Nest) + geom_line(aes(x = N, y = mean, color = &quot;Posterior&quot;)) + geom_line(aes(x = N, y = plugin_estimate, color = &quot;Plugin&quot;)) Can you explain the difference? Spatial distribution of gorilla nests using SPDE https://inlabru-org.github.io/inlabru/index.html Assessing how well the model fits A model should be capable of generating fake data that are statistically indistinguishable from the real data. If your model is a point pattern model you produce samples of event locations with the simulate() function. Let’s return to the Swedish pine sapling data and the inhibition model that you fit last time. You assume a (stationary) Strauss process with interaction radius r. The parameters \\(\\beta\\) and \\(\\gamma\\) define the pairwise interaction in which each event contributes a factor \\(\\beta\\) to the intenstiy of the point pattern, and each pair of events closer than r units apart contributes a factor \\(\\gamma\\) to the intensity where \\(\\gamma\\) is less than one. You use the ppm() function and include the point pattern data as the first argument. You set the trend term to a constant (implying a stationary process) with the argument trend ~ 1 and the interaction radius to 10 units with the argument interaction = Strauss(r = 10) and a border correction out to a distance of 10 units from the window with the rbord = argument. library(spatstat) ## Loading required package: spatstat.data ## Loading required package: spatstat.geom ## spatstat.geom 2.4-0 ## ## Attaching package: &#39;spatstat.geom&#39; ## The following object is masked from &#39;package:inlabru&#39;: ## ## vertices ## Loading required package: spatstat.random ## spatstat.random 2.2-0 ## Loading required package: spatstat.core ## Loading required package: rpart ## spatstat.core 2.4-4 ## Loading required package: spatstat.linnet ## spatstat.linnet 2.3-2 ## ## spatstat 2.3-4 (nickname: &#39;Watch this space&#39;) ## For an introduction to spatstat, type &#39;beginner&#39; SP &lt;- swedishpines model.in &lt;- SP |&gt; ppm(trend = ~ 1, interaction = Strauss(r = 10), rbord = 10) Here you generate three samples of the Swedish pine sapling data and for comparison plot them alongside the actual data. X &lt;- model.in |&gt; simulate(nsim = 3) ## Generating 3 simulated patterns ...1, 2, 3. plot(SP) plot(X[[1]]) plot(X[[2]]) plot(X[[3]]) The samples of point pattern data look similar to the actual data providing evidence that the inhibition model is adequate. To quantitatively assess the similarity you can use the envelope() function to compute the \\(K\\) function on 99 samples and on the actual data. The \\(K\\) function values are averaged over all samples and a mean line represents the mean model curve. Uncertainty is assessed with a band that ranges from the minimum to the maximum K at each distance. Do this with the inhibition model for the pine saplings. This takes a few seconds to complete. model.in |&gt; envelope(fun = Kest, nsim = 99, correction = &#39;border&#39;) |&gt; plot(legend = FALSE) ## Generating 99 simulated realisations of fitted Gibbs model ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, ## 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, ## 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99. ## ## Done. The black line is the empirical (data) curve and the red line is the average over the 99 samples. The two lines are close and the black line falls nearly completely within the gray uncertainty band indicating the model fits the data well. The kink in the red curve is the result of specifying 10 units for the interaction distance. From this plot you confidently conclude that a homogeneous inhibition model is adequate for describing the pine sapling data. What about the cluster model for the maple trees? You used a Thomas cluster process which means that centered on each event the chance of a nearby event decays as a two-dimensional Gaussian distribution. The latent rate of a nearby event is a two-dimensional kernel. This differs from a Matérn cluster process which means that centered on each event there is an equal chance of a nearby event out to some distance r. Use use the kppm() function and include the point pattern data as the first argument. You assume stationarity so trend = 1 and the argument clusters = is set to \"Thomas\". MT &lt;- lansing |&gt; subset(marks == &quot;maple&quot;) |&gt; unmark() ( model.cl &lt;- MT |&gt; kppm(trend = ~ 1, clusters = &quot;Thomas&quot;) ) ## Stationary cluster point process model ## Fitted to point pattern dataset &#39;MT&#39; ## Fitted by minimum contrast ## Summary statistic: K-function ## ## Uniform intensity: 514 ## ## Cluster model: Thomas process ## Fitted cluster parameters: ## kappa scale ## 21.74344366 0.06752959 ## Mean cluster size: 23.63931 points Now plot the \\(K\\) function on the data and on 99 model simulations. plot(envelope(model.cl, fun = Kest, nsim = 99, correction = &#39;border&#39;), legend = FALSE) ## Generating 99 simulated realisations of fitted cluster model ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, ## 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, ## 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99. ## ## Done. In the case of the maple trees, a cluster model is adequate. However, it might not be satisfying since you know about the potential for inhibition caused by the presence of hickory trees. Also there were more trees in the south than in the north so the stationary assumption is suspect. You fit a second cluster model where the intensity is a linear function of distance in the north-south direction. model.cl2 &lt;- MT |&gt; kppm(trend = ~ y, clusters = &quot;Thomas&quot;) model.cl2 ## Inhomogeneous cluster point process model ## Fitted to point pattern dataset &#39;MT&#39; ## Fitted by minimum contrast ## Summary statistic: inhomogeneous K-function ## ## Log intensity: ~y ## ## Fitted trend coefficients: ## (Intercept) y ## 6.894933 -1.486252 ## ## Cluster model: Thomas process ## Fitted cluster parameters: ## kappa scale ## 26.955877 0.053585 ## Mean cluster size: [pixel image] This is an inhomogeneous cluster point process model. The logarithm of the intensity depends on y (Log intensity: ~y). The fitted trend coefficient is negative as expected, since there are fewer trees as you move north (increasing y direction). There is one spatial unit in the north-south direction so you interpret this coefficient to mean there are 77% fewer trees in the north than in the south. The 77% comes from the formula 1 - exp(-1.486) = .77. The average number of clusters (kappa) is higher at about 27 (it was 22 with the stationary model). The cluster scale parameter (sigma), indicating the characteristic size of the cluster (in distance units) is lower at .0536. That makes sense since some of the event-to-event distances are accounted for by the trend term. Simulate data using the new model and compare the inhomogenous \\(K\\) function between the simulations and the observed data. model.cl2 |&gt; envelope(fun = Kinhom, nsim = 99, correction = &#39;border&#39;) |&gt; plot(legend = FALSE) ## Generating 99 simulated realisations of fitted cluster model ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, ## 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, ## 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99. ## ## Done. The black line falls within the gray band and the gray band is narrower than the simulations using the homogeneous cluster model. If the intensity of events depends on spatial location as it does with the maple trees you can include a trend and covariate term in the model. For a trend term, the formula ~ x corresponds to a spatial trend of the form \\(\\lambda(x) = \\exp(a + bx)\\), while ~ x + y corresponds to \\(\\lambda(x, y) = \\exp(a + bx + cy)\\) where x, y are the spatial coordinates. For a covariates, the formula is ~ covariate1 + covariate2. Consider the bei data from the {spatstat} package containing the locations of 3605 trees in a tropical rain forest. bei |&gt; plot() Accompanied by covariate data giving the elevation (altitude) and slope of elevation in the study region. The data bei.extra is a list containing two pixel images, elev (elevation in meters) and grad (norm of elevation gradient). These pixel images are objects of class im, see im.object. bei.extra |&gt; image() Compute and plot the \\(L\\) function on the ppp object bei. bei |&gt; envelope(fun = Lest, nsim = 39, global = TRUE, correction = &quot;border&quot;) |&gt; plot(legend = FALSE) ## Generating 39 simulations of CSR ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39. ## ## Done. There is significant clustering indicated by the black line sitting far above the CSR line. There are more trees in the vicinity of other trees than expected by chance. But how much of the clustering is due to variations in terrain? You start by fitting a model that includes elevation and gradient as covariates without clustering. This is done with the trend = argument naming the image variables and including the argument covariates = indicating a data frame or, in this case, a list whose entries are image functions. model.ppm1 &lt;- bei |&gt; ppm(trend = ~ elev + grad, covariates = bei.extra) Check to see if elevation and gradient as explanatory variables are significant in the model. model.ppm1 |&gt; summary() ## Point process model ## Fitting method: maximum likelihood (Berman-Turner approximation) ## Model was fitted using glm() ## Algorithm converged ## Call: ## ppm.ppp(Q = bei, trend = ~elev + grad, covariates = bei.extra) ## Edge correction: &quot;border&quot; ## [border correction distance r = 0 ] ## -------------------------------------------------------------------------------- ## Quadrature scheme (Berman-Turner) = data + dummy + weights ## ## Data pattern: ## Planar point pattern: 3604 points ## Average intensity 0.00721 points per square metre ## Window: rectangle = [0, 1000] x [0, 500] metres ## Window area = 5e+05 square metres ## Unit of length: 1 metre ## ## Dummy quadrature points: ## 130 x 130 grid of dummy points, plus 4 corner points ## dummy spacing: 7.692308 x 3.846154 metres ## ## Original dummy parameters: = ## Planar point pattern: 16904 points ## Average intensity 0.0338 points per square metre ## Window: rectangle = [0, 1000] x [0, 500] metres ## Window area = 5e+05 square metres ## Unit of length: 1 metre ## Quadrature weights: ## (counting weights based on 130 x 130 array of rectangular tiles) ## All weights: ## range: [1.64, 29.6] total: 5e+05 ## Weights on data points: ## range: [1.64, 14.8] total: 41000 ## Weights on dummy points: ## range: [1.64, 29.6] total: 459000 ## -------------------------------------------------------------------------------- ## FITTED MODEL: ## ## Nonstationary Poisson process ## ## ---- Intensity: ---- ## ## Log intensity: ~elev + grad ## Model depends on external covariates &#39;elev&#39; and &#39;grad&#39; ## Covariates provided: ## elev: im ## grad: im ## ## Fitted trend coefficients: ## (Intercept) elev grad ## -8.56355220 0.02143995 5.84646680 ## ## Estimate S.E. CI95.lo CI95.hi Ztest Zval ## (Intercept) -8.56355220 0.341113849 -9.23212306 -7.89498134 *** -25.104675 ## elev 0.02143995 0.002287866 0.01695581 0.02592408 *** 9.371155 ## grad 5.84646680 0.255781018 5.34514522 6.34778838 *** 22.857313 ## ## ----------- gory details ----- ## ## Fitted regular parameters (theta): ## (Intercept) elev grad ## -8.56355220 0.02143995 5.84646680 ## ## Fitted exp(theta): ## (Intercept) elev grad ## 1.909398e-04 1.021671e+00 3.460097e+02 The output shows that both elevation and elevation gradient are significant in explaining the spatial varying intensity of the trees. Since the conditional intensity is on a log scale you interpret the elevation coefficient as follows: For a one meter increase in elevation the local spatial intensity increases by a amount equal to exp(.021) or 2%. Check how well the model fits the data. Again this is done with the envelope() function using the model object as the first argument. E &lt;- model.ppm1 |&gt; envelope(fun = Lest, nsim = 39, correction = &quot;border&quot;, global = TRUE) ## Generating 78 simulated realisations of fitted Poisson model (39 to estimate ## the mean and 39 to calculate envelopes) ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, ## 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78. ## ## Done. E |&gt; plot(main = &quot;Inhomogeneous Poisson Model&quot;, legend = FALSE) You conclude that although elevation and elevation slope are significant in explaining the spatial distribution of trees, they do not explain all the clustering. An improvement is made by adding a cluster process to the model. This is done with the function kppm(). model.ppm2 &lt;- bei |&gt; kppm(trend = ~ elev + grad, covariates = bei.extra, clusters = &quot;Thomas&quot;) E &lt;- model.ppm2 |&gt; envelope(Lest, nsim = 39, global = TRUE, correction = &quot;border&quot;) ## Generating 78 simulated realisations of fitted cluster model (39 to estimate ## the mean and 39 to calculate envelopes) ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, ## 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78. ## ## Done. E |&gt; plot(main = &quot;Clustered Inhomogeneous Model&quot;, legend = FALSE) The uncertainty band is much wider. The empirical curve fits inside the band so you conclude that an inhomogeneous cluster process appears to be an adequate description of the point pattern data. An example of a point process model in the wild The vast majority of tornadoes have winds of less than 60 m/s (120 mph). Violent tornadoes, with winds exceeding 90 m/s, are rare. Most of these potentially destructive and deadly tornadoes occur from rotating thunderstorms called supercells, with formation contingent on local (storm-scale) meteorological conditions. The long-term risk of a tornado at a given location is assessed using historical records, however, the rarity of the most violent tornadoes make these rate estimates unstable. Here you use the more stable rate estimates from the larger set of less violent tornadoes to create more reliable estimates of violent tornado frequency. For this exercise attention is restricted to tornadoes occurring in Kansas over the period 1954–2020. Torn.sf &lt;- sf::st_read(dsn = here::here(&quot;data&quot;, &quot;1950-2020-torn-initpoint&quot;)) |&gt; sf::st_transform(crs = 3082) |&gt; dplyr::filter(mag &gt;= 0, yr &gt;= 1954) |&gt; dplyr::mutate(EF = mag, EFf = as.factor(EF)) |&gt; dplyr::select(yr, EF, EFf) ## Reading layer `1950-2020-torn-initpoint&#39; from data source ## `/Users/jelsner/Desktop/ClassNotes/ASS-2022/data/1950-2020-torn-initpoint&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 66244 features and 22 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -163.53 ymin: 17.7212 xmax: -64.7151 ymax: 61.02 ## Geodetic CRS: WGS 84 W.sfc &lt;- USAboundaries::us_states(states = &quot;Kansas&quot;) |&gt; sf::st_transform(crs = sf::st_crs(Torn.sf)) |&gt; sf::st_geometry() Torn.sf &lt;- Torn.sf[W.sfc, ] Create a owin and ppp objects. Note that although you already subset by Kansas tornadoes above you need to subset on the ppp object to assign the KS boundary as the analysis window. KS.win &lt;- W.sfc |&gt; as.owin() T.ppp &lt;- Torn.sf[&quot;EF&quot;] |&gt; as.ppp() T.ppp &lt;- T.ppp[KS.win] summary(T.ppp) ## Marked planar point pattern: 4139 points ## Average intensity 1.918005e-08 points per square unit ## ## *Pattern contains duplicated points* ## ## Coordinates are given to 1 decimal place ## i.e. rounded to the nearest multiple of 0.1 units ## ## marks are numeric, of type &#39;double&#39; ## Summary: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.0000 0.0000 0.6139 1.0000 5.0000 ## ## Window: polygonal boundary ## single connected closed polygon with 169 vertices ## enclosing rectangle: [1317675.9, 1980294.8] x [7114969, 7458570] units ## (662600 x 343600 units) ## Window area = 2.15797e+11 square units ## Fraction of frame area: 0.948 There are 4139 tornadoes over the period with an average intensity of 192 per 100 square kilometer (multiply the average intensity in square meters by 10^10). Separate the point pattern data into non-violent tornadoes and violent tornadoes. The non-violent tornadoes include those with an EF rating of 0, 1, 2 or 3. The violent tornadoes include those with an EF rating of 4 or 5. NV.ppp &lt;- T.ppp |&gt; subset(marks &lt;= 3 &amp; marks &gt;= 0) |&gt; unmark() summary(NV.ppp) ## Planar point pattern: 4098 points ## Average intensity 1.899006e-08 points per square unit ## ## *Pattern contains duplicated points* ## ## Coordinates are given to 1 decimal place ## i.e. rounded to the nearest multiple of 0.1 units ## ## Window: polygonal boundary ## single connected closed polygon with 169 vertices ## enclosing rectangle: [1317675.9, 1980294.8] x [7114969, 7458570] units ## (662600 x 343600 units) ## Window area = 2.15797e+11 square units ## Fraction of frame area: 0.948 V.ppp &lt;- T.ppp |&gt; subset(marks &gt;= 4) |&gt; unmark() V.ppp |&gt; summary() ## Planar point pattern: 41 points ## Average intensity 1.899933e-10 points per square unit ## ## Coordinates are given to 1 decimal place ## i.e. rounded to the nearest multiple of 0.1 units ## ## Window: polygonal boundary ## single connected closed polygon with 169 vertices ## enclosing rectangle: [1317675.9, 1980294.8] x [7114969, 7458570] units ## (662600 x 343600 units) ## Window area = 2.15797e+11 square units ## Fraction of frame area: 0.948 The spatial intensity of the non-violent tornadoes is 190 per 100 sq km. The spatial intensity of the violent tornadoes is 1.9 per 100 square kilometer. Plot the locations of the violent tornado events. V.ppp |&gt; plot() Early we found that the spatial intensity of tornado reports was a function of distance to nearest city with fewer reports in rural areas. So here you include this as an explanatory variable. Import the data, set the CRS, and transform the CRS to match that of the tornadoes. Exclude cities with fewer than 1000 people. C.sf &lt;- USAboundaries::us_cities() |&gt; dplyr::filter(population &gt;= 1000) |&gt; sf::st_transform(crs = sf::st_crs(Torn.sf)) ## City populations for contemporary data come from the 2010 census. Then convert the simple feature data frame to a ppp object. Then subset the events by the analysis window (Kansas border). C.ppp &lt;- C.sf |&gt; as.ppp() ## Warning in as.ppp.sf(C.sf): only first attribute column is used for marks C.ppp &lt;- C.ppp[KS.win] |&gt; unmark() C.ppp |&gt; plot() Next create a distance map of the city locations using the distmap() function. Zc &lt;- C.ppp |&gt; distmap() Zc |&gt; plot() The pixel values of the im object are distances is meters. Blue indicates locations that are less than 20 km from a city. Interest lies with the distance to nearest non-violent tornado. You check to see if this might be a useful variable in a model so you make a distance map for the non-violent events and then use the rhohat() function. Znv &lt;- NV.ppp |&gt; distmap() rhat &lt;- rhohat(V.ppp, Znv, adjust = 1.5, smoother = &quot;kernel&quot;, method = &quot;transform&quot;) dist &lt;- rhat$Znv rho &lt;- rhat$rho hi &lt;- rhat$hi lo &lt;- rhat$lo Rho.df &lt;- data.frame(dist = dist, rho = rho, hi = hi, lo = lo) ggplot(data = Rho.df) + geom_ribbon(mapping = aes(x = dist, ymin = lo, ymax = hi), alpha = .3) + geom_line(aes(x = dist, y = rho), col = &quot;black&quot;) + ylab(&quot;Spatial intensity of violent tornadoes&quot;) + xlab(&quot;Distance from nearest non-violent tornado (m)&quot;) + theme_minimal() This shows that regions that get non-violent tornadoes also see higher rates of violent tornadoes. So the model should include two covariates (trend terms), distance to nearest city and distance to nearest non-violent tornado. model.ppm1 &lt;- V.ppp |&gt; ppm(trend = ~ Zc + Znv, covariates = list(Zc = Zc, Znv = Znv)) model.ppm1 |&gt; summary() |&gt; coef() ## Estimate S.E. CI95.lo CI95.hi Ztest ## (Intercept) -2.079665e+01 3.689920e-01 -2.151986e+01 -2.007344e+01 *** ## Zc -3.213231e-05 1.118327e-05 -5.405111e-05 -1.021350e-05 ** ## Znv -2.235788e-04 8.585891e-05 -3.918592e-04 -5.529845e-05 ** ## Zval ## (Intercept) -56.360705 ## Zc -2.873248 ## Znv -2.604026 As expected the model shows fewer violent tornadoes with increasing distance from the nearest city (negative coefficient on Zc) and fewer violent tornadoes with increasing distance from a non-violent tornado (negative coefficient on Znv). Since the spatial unit is meters the coefficient of -3.06e-05 is interpreted as a [1 - exp(-.0306)] * 100% or 3% decrease in violent tornado reports per kilometer of distance from a city. Similarly the coefficient on distance from nearest non-violent tornado is interpreted as a 23% decrease in violent tornado reports per kilometer of distance from nearest non-violent tornado. Check if there is any residual nearest neighbor correlation. E &lt;- model.ppm1 |&gt; envelope(fun = Kest, nsim = 39, global = TRUE) ## Generating 78 simulated realisations of fitted Poisson model (39 to estimate ## the mean and 39 to calculate envelopes) ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, ## 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78. ## ## Done. E |&gt; plot(main = &quot;Inhomogeneous Poisson Model&quot;, legend = FALSE) There appears to be a bit of regularity at smaller scales. The empirical curve (black line) falls slightly below the model (dashed red line). There are fewer nearby violent tornadoes than one would expect. To see if this is statistically significant, you add an inhibition process to the model. model.ppm2 &lt;- V.ppp |&gt; ppm(trend = ~ Zc + Znv, covariates = list(Zc = Zc, Znv = Znv), interaction = Strauss(r = 40000)) model.ppm2 |&gt; summary() |&gt; coef() ## Estimate S.E. CI95.lo CI95.hi Ztest ## (Intercept) -1.999626e+01 0.6389281922 -2.124853e+01 -1.874398e+01 *** ## Zc -4.125674e-05 0.0000129859 -6.670864e-05 -1.580484e-05 ** ## Znv -2.325491e-04 0.0001163074 -4.605075e-04 -4.590757e-06 * ## Interaction -6.232454e-01 0.3926001130 -1.392727e+00 1.462367e-01 ## Zval ## (Intercept) -31.296564 ## Zc -3.177041 ## Znv -1.999435 ## Interaction -1.587481 The interaction coefficient has a negative sign as expected from the above plot, but the standard error is relatively large so it is not statistically significant. Remove the inhibition process and add a trend term in the east-west direction. model.ppm3 &lt;- V.ppp |&gt; ppm(trend = ~ Zc + Znv + x, covariates = list(Zc = Zc, Znv = Znv)) model.ppm3 |&gt; summary() |&gt; coef() ## Estimate S.E. CI95.lo CI95.hi Ztest ## (Intercept) -2.381531e+01 1.891801e+00 -2.752317e+01 -2.010745e+01 *** ## Zc -2.274246e-05 1.255697e-05 -4.735366e-05 1.868739e-06 ## Znv -2.379710e-04 8.651254e-05 -4.075324e-04 -6.840952e-05 ** ## x 1.681064e-06 1.020308e-06 -3.187026e-07 3.680830e-06 ## Zval ## (Intercept) -12.588694 ## Zc -1.811143 ## Znv -2.750711 ## x 1.647605 There is a significant eastward trend but it appears to confound the distance to city term because the Zc term is no longer significant. Why is this? Settle on the first model and generate simulated data from it. model.ppm1 |&gt; simulate(nsim = 6) |&gt; plot() ## Generating 6 simulated patterns ...1, 2, 3, 4, 5, 6. plot(V.ppp) The model appears to due a good job simulating data that looks like the actual data. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
