[["thursday-october-13-2022.html", "Thursday October 13, 2022 Estimating the relative risk of events Estimating second-moment properties of spatial events", " Thursday October 13, 2022 “To me programming is more than an important practical art. It is also a gigantic undertaking in the foundations of knowledge.” – Grace Hopper Today Estimating the relative risk of events Estimating second-order properties of spatial events Estimating the relative risk of events Separate spatial intensity maps across two marked types provides a way to estimate the risk of one event type conditional on the other event type. More generally, the relative risk of occurrence of some event is a conditional probability. In a non-spatial context, the risk of catching a disease if you are elderly relative to the risk if you are young. Given a tornado somewhere in Texas what is the chance that it will cause at least EF3 damage? With the historical set of all tornadoes marked by the damage rating you can make a map of all tornadoes and a map of the EF3+ tornadoes and then take the ratio. To see this start by importing the tornado data, mutating and selecting the damage rating as a factor called EF before turning the resulting simple feature data frame into a planar point pattern. Torn.sf &lt;- sf::st_read(dsn = here::here(&quot;data&quot;, &quot;1950-2020-torn-initpoint&quot;)) |&gt; sf::st_transform(crs = 3082) |&gt; dplyr::filter(mag &gt;= 0) |&gt; dplyr::mutate(EF = as.factor(mag)) |&gt; dplyr::select(EF) ## Reading layer `1950-2020-torn-initpoint&#39; from data source ## `/Users/jameselsner/Desktop/ClassNotes/ASS-2022/data/1950-2020-torn-initpoint&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 66244 features and 22 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -163.53 ymin: 17.7212 xmax: -64.7151 ymax: 61.02 ## Geodetic CRS: WGS 84 library(spatstat) ## Loading required package: spatstat.data ## Loading required package: spatstat.geom ## spatstat.geom 2.4-0 ## Loading required package: spatstat.random ## spatstat.random 2.2-0 ## Loading required package: spatstat.core ## Loading required package: nlme ## Loading required package: rpart ## spatstat.core 2.4-4 ## Loading required package: spatstat.linnet ## spatstat.linnet 2.3-2 ## ## spatstat 2.3-4 (nickname: &#39;Watch this space&#39;) ## For an introduction to spatstat, type &#39;beginner&#39; T.ppp &lt;- Torn.sf |&gt; as.ppp() Then subset by the boundary of Texas. TX.sf &lt;- USAboundaries::us_states(states = &quot;Texas&quot;) |&gt; sf::st_transform(crs = sf::st_crs(Torn.sf)) W &lt;- TX.sf |&gt; as.owin() T.ppp &lt;- T.ppp[W] summary(T.ppp) ## Marked planar point pattern: 8932 points ## Average intensity 1.293119e-08 points per square unit ## ## *Pattern contains duplicated points* ## ## Coordinates are given to 1 decimal place ## i.e. rounded to the nearest multiple of 0.1 units ## ## Multitype: ## frequency proportion intensity ## 0 4773 0.5343708000 6.910052e-09 ## 1 2557 0.2862741000 3.701865e-09 ## 2 1225 0.1371473000 1.773479e-09 ## 3 323 0.0361621100 4.676192e-10 ## 4 48 0.0053739360 6.949141e-11 ## 5 6 0.0006717421 8.686426e-12 ## ## Window: polygonal boundary ## single connected closed polygon with 550 vertices ## enclosing rectangle: [873763.8, 2116649.8] x [5881245, 7063086] units ## (1243000 x 1182000 units) ## Window area = 6.90733e+11 square units ## Fraction of frame area: 0.47 Chance that a tornado anywhere in Texas will be at least EF3 or worse is the sum of the proportions for these types: .03616 + .00537 + .00067 = .042 (or 4.2%). As found previously there is a spatial intensity gradient across the state with fewer tornadoes in the southwest and more in the northeast. Also the more damaging tornadoes might be more common relative to all tornadoes in some parts of the state compared with other parts. To create a map of the relative risk of the more damaging tornadoes you start by making two ppp objects, one being the set of all tornado events with damage ratings 0, 1, or 2 and the other the set of all tornado locations with damage ratings 3, 4, or 5. You do this by subset the object using brackets ([]) and the logical operator | (or) and then merge the two subsets assigning names H and I as marks with the superimpose() function. H.ppp &lt;- unmark(T.ppp[T.ppp$marks == 2 | T.ppp$marks == 1 | T.ppp$marks == 0]) I.ppp &lt;- unmark(T.ppp[T.ppp$marks == 3 | T.ppp$marks == 4 | T.ppp$marks == 5]) T2.ppp &lt;- superimpose(H = H.ppp, I = I.ppp) ## Warning: data contain duplicated points See https://en.wikipedia.org/wiki/Enhanced_Fujita_scale for definitions of EF tornado rating. The chance that a tornado chosen at random is intense (EF3+) is 4.2%. Plot the event locations for the set of intense tornadoes. plot(I.ppp, pch = 25, cols = &quot;red&quot;, main = &quot;&quot;) plot(T.ppp, add = TRUE, lwd = .1) To get the relative risk use the relrisk() function. If X is a multi-type point pattern with factor marks and two levels of the factor then the events of the first type (the first level of marks(X)) are treated as controls (conditionals) or non-events, and events of the second type are treated as cases. The relrisk() function estimates the local chance of a case (i.e. the probability \\(p(u)\\) that a point at \\(u\\) will be a case) using a kernel density smoother. The bandwidth for the kernel is specified or can be found through an iterative cross-validation procedure (recall the bandwidth selection procedure used in geographic regression) using the bw.relrisk() function. The bandwidth has units of length (here meters). You specify a minimum and maximum bandwidth with the hmin = and hmax = arguments. This takes a few seconds. ( bw &lt;- bw.relrisk(T2.ppp, hmin = 1000, hmax = 200000) ) ## sigma ## 119770.4 The optimal bandwidth (sigma) is 119770 meters or about 120 km. Now estimate the relative risk at points defined by a 256 by 256 grid and using the 120 km bandwidth for the kernel smoother. rr &lt;- relrisk(T2.ppp, sigma = bw, dimyx = c(256, 256)) The result is an object of class im (image) with values you interpret as the conditional probability of an ‘intense’ tornado. You retrieve the range of probabilities with the range() function. Note that many of the values are NA corresponding pixels that are outside the window so you set the na.rm argument to TRUE. range(rr, na.rm = TRUE) ## [1] 0.005003694 0.060170214 The probabilities range from a low of .5% to a high of 6%. This range compares with the statewide average probability of 4.2%. Map the probabilities with the plot() method. plot(rr) Make a better map by converting the image to a raster, setting the CRS, and then using functions from the {tmap} package. tr.r &lt;- raster::raster(rr) raster::crs(tr.r) &lt;- sf::st_crs(Torn.sf)$proj4string tmap::tm_shape(tr.r) + tmap::tm_raster() The chance that a tornado is more damaging peaks in the northeast part of the state. Since the relative risk is computed for any point it is of interest to extract the probabilities for cities and towns. You get city locations with the us_cities() function from the {USAboundaries} package that extracts a simple feature data frame of cities. The CRS is 4326 and you filter to keep only cities with at least 100000 in 2010. Cities.sf &lt;- USAboundaries::us_cities(state = &quot;TX&quot;) |&gt; sf::st_transform(crs = raster::crs(tr.r)) |&gt; dplyr::filter(population &gt; 100000) ## City populations for contemporary data come from the 2010 census. Use the extract() function from the {raster} package to get a single value for each city. Put these values into the simple feature data frame. Cities.sf$tr &lt;- raster::extract(tr.r, Cities.sf) Cities.sf |&gt; dplyr::arrange(desc(tr)) ## Simple feature collection with 29 features and 13 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 893306.2 ymin: 5900924 xmax: 2063028 ymax: 6916010 ## CRS: PROJCRS[&quot;unknown&quot;, ## BASEGEOGCRS[&quot;unknown&quot;, ## DATUM[&quot;North American Datum 1983&quot;, ## ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ID[&quot;EPSG&quot;,6269]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8901]]], ## CONVERSION[&quot;unknown&quot;, ## METHOD[&quot;Lambert Conic Conformal (2SP)&quot;, ## ID[&quot;EPSG&quot;,9802]], ## PARAMETER[&quot;Latitude of false origin&quot;,18, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8821]], ## PARAMETER[&quot;Longitude of false origin&quot;,-100, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8822]], ## PARAMETER[&quot;Latitude of 1st standard parallel&quot;,27.5, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8823]], ## PARAMETER[&quot;Latitude of 2nd standard parallel&quot;,35, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8824]], ## PARAMETER[&quot;Easting at false origin&quot;,1500000, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8826]], ## PARAMETER[&quot;Northing at false origin&quot;,5000000, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8827]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]]] ## # A tibble: 29 × 14 ## city state_name state_abbr county county_name stplfips_2010 name_2010 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Mckinney Texas TX COLLIN Collin 4845744 McKinney… ## 2 Mesquite Texas TX Dallas Dallas 4847892 Mesquite… ## 3 Garland Texas TX Dallas Dallas 4829000 Garland … ## 4 Plano Texas TX Collin Collin 4858016 Plano ci… ## 5 Frisco Texas TX Collin Collin 4827684 Frisco c… ## 6 Dallas Texas TX DALLAS Dallas 4819000 Dallas c… ## 7 Carrollton Texas TX Dallas Dallas 4813024 Carrollt… ## 8 Irving Texas TX Dallas Dallas 4837000 Irving c… ## 9 Denton Texas TX DENTON Denton 4819972 Denton c… ## 10 Grand Prair… Texas TX Dallas Dallas 4830464 Grand Pr… ## # … with 19 more rows, and 7 more variables: city_source &lt;chr&gt;, ## # population_source &lt;chr&gt;, place_type &lt;chr&gt;, year &lt;int&gt;, population &lt;int&gt;, ## # geometry &lt;POINT [m]&gt;, tr &lt;dbl&gt; To illustrate the results create a graph using the geom_lollipop() function from the {ggalt} package. Use the package {scales} to allow for labels in percent. library(ggalt) ## Loading required package: ggplot2 ## Registered S3 methods overwritten by &#39;ggalt&#39;: ## method from ## grid.draw.absoluteGrob ggplot2 ## grobHeight.absoluteGrob ggplot2 ## grobWidth.absoluteGrob ggplot2 ## grobX.absoluteGrob ggplot2 ## grobY.absoluteGrob ggplot2 library(scales) ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:spatstat.geom&#39;: ## ## rescale ggplot(Cities.sf, aes(x = reorder(city, tr), y = tr)) + geom_lollipop(point.colour = &quot;steelblue&quot;, point.size = 3) + scale_y_continuous(labels = percent, limits = c(0, .0625)) + coord_flip() + labs(x = &quot;&quot;, y = NULL, title = &quot;Historical chance that a tornado caused at least EF3 damage&quot;, subtitle = &quot;Cities in Texas with a 2010 population &gt; 100,000&quot;, caption = &quot;Data from SPC (1950-2020)&quot;) + theme_minimal() Another example: Florida wildfires Given a wildfire in Florida what is the probability that it was started by lightning? Import wildfire data (available here: https://www.fs.usda.gov/rds/archive/catalog/RDS-2013-0009.4) as a simple feature data frame and transform the native CRS to a Florida GDL Albers (EPSG 3086). if(!&quot;FL_Fires&quot; %in% list.files(here::here(&quot;data&quot;))){ download.file(&quot;http://myweb.fsu.edu/jelsner/temp/data/FL_Fires.zip&quot;, destfile = here::here(&quot;data&quot;, &quot;FL_Fires.zip&quot;)) unzip(zipfile = here::here(&quot;data&quot;, &quot;FL_Fires.zip&quot;), exdir = here::here(&quot;data&quot;)) } FL_Fires.sf &lt;- sf::st_read(dsn = here::here(&quot;data&quot;, &quot;FL_Fires&quot;)) |&gt; sf::st_transform(crs = 3086) ## Reading layer `FL_Fires&#39; from data source ## `/Users/jameselsner/Desktop/ClassNotes/ASS-2022/data/FL_Fires&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 90261 features and 37 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -9750382 ymin: 2824449 xmax: -8908899 ymax: 3632749 ## Projected CRS: Mercator_2SP dim(FL_Fires.sf) ## [1] 90261 38 Each row is a unique fire and the data spans the period 1992-2015. There are over 90K rows and 38 variables. To make things run faster, here you analyze only a random sample of all the data. You do this with the dplyr::sample_n() function where the argument size = specifies the number of rows to choose at random. Save the sample of events to the object FL_FiresS.sf. First set the seed for the random number generator so that the set of rows chosen will be the same every time you run the code. set.seed(78732) FL_FiresS.sf &lt;- FL_Fires.sf |&gt; dplyr::sample_n(size = 2000) dim(FL_FiresS.sf) ## [1] 2000 38 The result is a simple feature data frame with exactly 2000 rows. The character variable STAT_CAU_1 indicates the cause of the wildfire. FL_FiresS.sf$STAT_CAU_1 |&gt; table() ## ## Arson Campfire Children Debris Burning ## 147 32 93 239 ## Equipment Use Fireworks Lightning Miscellaneous ## 308 4 495 199 ## Missing/Undefined Powerline Railroad Smoking ## 74 9 365 31 ## Structure ## 4 There are 13 causes (listed in alphabetical order) with various occurrence frequencies. Lightning is the most common. To analyze these data as spatial events, you first convert the simple feature data to a ppp object over a window defined by the state boundaries. Use the cause of the fire as a factor mark. F.ppp &lt;- FL_FiresS.sf[&quot;STAT_CAU_1&quot;] |&gt; as.ppp() W &lt;- USAboundaries::us_states(states = &quot;Florida&quot;) |&gt; sf::st_transform(crs = sf::st_crs(FL_Fires.sf)) |&gt; as.owin() F.ppp &lt;- F.ppp[W] marks(F.ppp) &lt;- as.factor(marks(F.ppp)) # make the character marks factor marks summary(F.ppp) ## Marked planar point pattern: 2000 points ## Average intensity 1.297232e-08 points per square unit ## ## *Pattern contains duplicated points* ## ## Coordinates are given to 2 decimal places ## i.e. rounded to the nearest multiple of 0.01 units ## ## Multitype: ## frequency proportion intensity ## Arson 147 0.0735 9.534653e-10 ## Campfire 32 0.0160 2.075571e-10 ## Children 93 0.0465 6.032128e-10 ## Debris Burning 239 0.1195 1.550192e-09 ## Equipment Use 308 0.1540 1.997737e-09 ## Fireworks 4 0.0020 2.594463e-11 ## Lightning 495 0.2475 3.210649e-09 ## Miscellaneous 199 0.0995 1.290746e-09 ## Missing/Undefined 74 0.0370 4.799757e-10 ## Powerline 9 0.0045 5.837543e-11 ## Railroad 365 0.1825 2.367448e-09 ## Smoking 31 0.0155 2.010709e-10 ## Structure 4 0.0020 2.594463e-11 ## ## Window: polygonal boundary ## 4 separate polygons (no holes) ## vertices area relative.area ## polygon 1 356 1.53185e+11 0.994000 ## polygon 2 15 8.05114e+08 0.005220 ## polygon 3 5 7.46249e+07 0.000484 ## polygon 4 5 1.09937e+08 0.000713 ## enclosing rectangle: [52649.1, 794026.5] x [56850.4, 781579.4] units ## (741400 x 724700 units) ## Window area = 1.54174e+11 square units ## Fraction of frame area: 0.287 Output from the summary() method displays a table of frequency by type including the proportion and the average spatial intensity (per square meters). The probability that a wildfire is caused by lightning is about 25% (proportion column of the frequency versus type table). How does this probability vary over the state? Note that the window contains four separate polygons to capture the main boundary (polygon 4) and the Florida Keys. plot(W) First split the object F.ppp on whether or not the cause was lightning and then merge the two event types and assign names NL (human caused) and L (lightning caused) as marks. L.ppp &lt;- F.ppp[F.ppp$marks == &quot;Lightning&quot;] |&gt; unmark() NL.ppp &lt;- F.ppp[F.ppp$marks != &quot;Lightning&quot;] |&gt; unmark() LNL.ppp &lt;- superimpose(NL = NL.ppp, L = L.ppp) ## Warning: data contain duplicated points summary(LNL.ppp) ## Marked planar point pattern: 2000 points ## Average intensity 1.297232e-08 points per square unit ## ## *Pattern contains duplicated points* ## ## Coordinates are given to 2 decimal places ## i.e. rounded to the nearest multiple of 0.01 units ## ## Multitype: ## frequency proportion intensity ## NL 1505 0.7525 9.761669e-09 ## L 495 0.2475 3.210649e-09 ## ## Window: polygonal boundary ## 4 separate polygons (no holes) ## vertices area relative.area ## polygon 1 356 1.53185e+11 0.994000 ## polygon 2 15 8.05114e+08 0.005220 ## polygon 3 5 7.46249e+07 0.000484 ## polygon 4 5 1.09937e+08 0.000713 ## enclosing rectangle: [52649.1, 794026.5] x [56850.4, 781579.4] units ## (741400 x 724700 units) ## Window area = 1.54174e+11 square units ## Fraction of frame area: 0.287 Now the two types are NL and L composing 75% and 25% of all wildfire events. The function relrisk() computes the spatially-varying probability of a case (event type), (i.e. the probability \\(p(u)\\) that a point at location \\(u\\) will be a case). Here you compute the relative risk on a 256 by 256 grid. wfr &lt;- relrisk(LNL.ppp, dimyx = c(256, 256)) Create a map from the raster by first converting the image object to a raster object and assigning the CRS with the crs() function from the {raster} package. Add the county borders for geographic reference. wfr.r &lt;- raster::raster(wfr) raster::crs(wfr.r) &lt;- sf::st_crs(FL_Fires.sf)$proj4string FL.sf &lt;- USAboundaries::us_counties(state = &quot;FL&quot;) |&gt; sf::st_transform(crs = sf::st_crs(FL_Fires.sf)) tmap::tm_shape(wfr.r) + tmap::tm_raster(title = &quot;Probability&quot;) + tmap::tm_shape(FL.sf) + tmap::tm_borders(col = &quot;gray70&quot;) + tmap::tm_legend(position = c(&quot;left&quot;, &quot;center&quot;) ) + tmap::tm_layout(main.title = &quot;Chance a wildfire was started by lightning (1992-2015)&quot;, main.title.size = 1) + tmap::tm_compass(position = c(&quot;right&quot;, &quot;top&quot;)) + tmap::tm_credits(text = &quot;Data source: Karen Short https://doi.org/10.2737/RDS-2013-0009.4&quot;, position = c(&quot;left&quot;, &quot;bottom&quot;)) Estimating second-moment properties of spatial events Spatial intensity is a first-moment property of event locations (like the average of a set of numbers). It answers the question: where are events more and less frequent? Clustering is a second-moment property of event locations (like the variance of a a set of numbers). It answers the question: is the probability of an event in the proximity of another event higher than expected by chance? On example of cluster occurs with the location of trees in a forest. A tree’s seed dispersal mechanism leads to a greater likelihood of another tree nearby. Let \\(r\\) be the distance between two event locations or the distance between an event and an arbitrary point within the domain, then functions to describe clustering include: The nearest neighbor distance function \\(G(r)\\): The cumulative distribution of the distances from an event to the nearest other event (event-to-event function). It summarizes the distance between events (amount of clustering). The empty space function \\(F(r)\\): The cumulative distribution of the distances from a point in the domain to the nearest event (point-to-event function). It summarizes the distance gaps between events (amount of gappiness or lacunarity). The reduced second-moment function (Ripley \\(K\\)) \\(K(r)\\): Defined such that \\(\\lambda \\times K(r)\\) is the expected number of additional events within a distance \\(r\\) of an event, where \\(\\lambda\\) is the average intensity of the events. It is a measure of the spatial autocorrelation among the events. To assess the degree of clustering and significance (in a statistical sense), we estimate values of the function using our data set and compare the resulting curve (empirical curve) to a theoretical curve assuming a homogeneous Poisson process. The theoretical curve is well defined for homogeneous point patterns (recall: CSR–complete spatial randomness). Deviations of an ‘empirical’ curve from a theoretical curve provides evidence against CSR. The theoretical functions assuming a homogeneous Poisson process are: \\(F(r) = G(r) = 1 - \\exp(-\\lambda \\pi r^2)\\) \\(K(r) = \\pi r^2\\) where \\(\\lambda\\) is the domain average spatial intensity and \\(\\exp()\\) is the exponential function. Recall the Swedish pine saplings data that comes with the {spatstat} package. data(swedishpines) class(swedishpines) ## [1] &quot;ppp&quot; Assign the data to an object called SP to reduce the amount of typing. ( SP &lt;- swedishpines ) ## Planar point pattern: 71 points ## window: rectangle = [0, 96] x [0, 100] units (one unit = 0.1 metres) The output indicates that there are 71 events within a rectangle window 96 by 100 units where one unit is .1 meters. You obtain the values for the nearest neighbor function using the Gest() function from the {spatstat} package. Use the argument correction = \"none\" so no corrections are made for events near the window borders. Assign the output to a list object called G. ( G &lt;- Gest(SP, correction = &quot;none&quot;) ) ## Function value object (class &#39;fv&#39;) ## for the function r -&gt; G(r) ## ................................................ ## Math.label Description ## r r distance argument r ## theo G[pois](r) theoretical Poisson G(r) ## raw hat(G)[raw](r) uncorrected estimate of G(r) ## ................................................ ## Default plot formula: .~r ## where &quot;.&quot; stands for &#39;raw&#39;, &#39;theo&#39; ## Recommended range of argument r: [0, 22.26] ## Available range of argument r: [0, 22.26] ## Unit of length: 0.1 metres The output includes the distance r, the raw uncorrected estimate of \\(G(r)\\) (empirical estimate) at various distances, and a theoretical estimate at those same distances based on a homogeneous Poisson process. Using the plot() method on the saved object G you compare the empirical estimates with the theoretical estimates. Here two horizontal lines are added to help with the interpretation. plot(G) abline(h = c(.2, .5), col = &quot;black&quot;, lty = 2) Values of G are on the vertical axis and values of distance (relative) are on the horizontal axis starting at 0. The black curve is the uncorrected estimate of \\(G_{raw}(r)\\) from the event locations and the red curve is \\(G_{pois}(r)\\) estimated from a homogeneous Poisson process with the same average intensity as the pine saplings. The horizontal dashed line at G = .2 intersects the black line at a relative distance (r) of 5 units. This means that 20% of the events have another event within 5 units. This means that 20% of the saplings have another sapling withing .5 meter. Imagine placing a disc of radius 5 units around all 71 events then counting the number of events that have another event under the disc. That number divided by 71 is G(r). To check this compute all pairwise distances with the pairdist() function. PDmatrix &lt;- pairdist(SP) PDmatrix[1:6, 1:6] ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.00000 27.000000 37.01351 15.03330 54.33231 25.298221 ## [2,] 27.00000 0.000000 10.04988 12.04159 27.65863 8.544004 ## [3,] 37.01351 10.049876 0.00000 22.00000 17.72005 14.764823 ## [4,] 15.03330 12.041595 22.00000 0.00000 39.31921 11.401754 ## [5,] 54.33231 27.658633 17.72005 39.31921 0.00000 30.066593 ## [6,] 25.29822 8.544004 14.76482 11.40175 30.06659 0.000000 This creates a 71 x 71 square matrix of distances. Sum the number of rows whose distances are within 5 units. The minus one means you don’t count the row containing event over which you are summing (an event location is not a neighbor of itself). sum(rowSums(PDmatrix &lt; 5) - 1) / nrow(PDmatrix) * 100 ## [1] 19.71831 Returning to the plot, the horizontal dashed line at G = .5 intersects the black line at .8 meters indicating that 50% of the pine saplings have another pine sapling within .8 meter. You see that for a given radius the \\(G_{raw}\\) line is below the \\(G_{pois}(r)\\) line indicating that there are fewer pine saplings with another pine sapling in the vicinity than expected by chance. For example, if the saplings were arranged under a model of CSR, you would expect 20% of the pairwise distances to be within .3 meter and 50% of them to be within .55 meter. You make a better plot by first converting the object G to a data frame and then using {ggplot2} functions. Here you do this and then remove estimates for distances greater than 1.1 meter and convert the distance units to meters. G.df &lt;- as.data.frame(G) |&gt; dplyr::filter(r &lt; 11) |&gt; dplyr::mutate(r = r * .1) ggplot(data = G.df, mapping = aes(x = r, y = raw)) + geom_line() + geom_line(aes(y = theo), color = &quot;red&quot;) + geom_hline(yintercept = c(.2, .5), lty = &#39;dashed&#39;) + xlab(&quot;Distance (m)&quot;) + ylab(&quot;G(r): Cumulative % of events having another event within a distance r&quot;) + theme_minimal() Values for the empty space function are obtained from the Fest() function. Here you apply the Kaplan-Meier correction for edge effects with correction = \"km\". The function returns the percent of the domain within a distance from any event. Imagine again placing the disc, but this time on top of every point in the window and counting the number of points that have an event underneath. Make a plot and add some lines to help with interpretation. F.df &lt;- SP |&gt; Fest(correction = &quot;km&quot;) |&gt; as.data.frame() |&gt; dplyr::filter(r &lt; 11) |&gt; dplyr::mutate(r = r * .1) ggplot(data = F.df, mapping = aes(x = r, y = km)) + geom_line() + geom_line(aes(y = theo), color = &quot;red&quot;) + geom_hline(yintercept = c(.7, .58), lty = &#39;dashed&#39;) + geom_vline(xintercept = .61, lty = 2) + xlab(&quot;Distance (m)&quot;) + ylab(&quot;Percent of domain within a distance r of an event&quot;) + theme_minimal() The horizontal dashed line at F = .7 intersects the black line at a distance of .61 meter. This means that 70% of the spatial domain is less than .61 meters from a sapling. The red line is the theoretical homogeneous Poisson process model. If the process was CSR slightly less than 58% (F = .58) of the domain would be less than .6 meter from a sapling. In words, the arrangement of saplings is less “gappy” (more regular) than expected by chance. The J function is the ratio of the F function to the G function. For a CSR processes the value of J is one. Here we see a large and systematic departure of J from one for distances greater than about .5 meter, due to the regularity in the spacing of the saplings. J.df &lt;- SP |&gt; Jest() |&gt; as.data.frame() |&gt; dplyr::filter(r &lt; 10) |&gt; dplyr::mutate(r = r * .1) ggplot(data = J.df, mapping = aes(x = r, y = km)) + geom_line() + geom_line(aes(y = theo), color = &quot;red&quot;) + xlab(&quot;Distance (m)&quot;) + ylab(&quot;&quot;) + theme_minimal() A commonly used distance function for assessing clustering in point pattern data is called Ripley’s K function. It is estimated with the Kest() function. Mathematically it is defined as \\[ \\hat K(r) = \\frac{1}{\\hat \\lambda} \\sum_{j \\ne i} \\frac{I(r_{ij} &lt; r)}{n} \\] where \\(r_{ij}\\) is the Euclidean distance between event \\(i\\) and event \\(j\\), \\(r\\) is the search radius, and \\(\\hat \\lambda\\) is an estimate of the intensity \\((\\hat \\lambda = n/|A|)\\) where \\(|A|\\) is the window area and \\(n\\) is the number of events. \\(I(.)\\) is an indicator function equal to 1 when the expression \\(r_{ij} &lt; r\\), and 0 otherwise. If the events are homogeneous, \\(\\hat{K}(r)\\) increases at a rate proportional to \\(\\pi r^2\\). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
