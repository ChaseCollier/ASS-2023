[["tuesday-february-21-2023.html", "Tuesday February 21, 2023 0.1 Using local spatial autocorrelation to understand the tornado record Constraining group membership based on spatial autocorrelation Estimating spatial autocorrelation in model residuals Choosing a spatial regression model Fitting and interpreting spatial regression models", " Tuesday February 21, 2023 “Feeling a little uncomfortable with your skills is a sign of learning, and continuous learning is what the tech industry thrives on!” — Vanessa Hurst 0.1 Using local spatial autocorrelation to understand the tornado record Is the frequency of tornado reports correlated with the number of people in a region? Might this correlation extend to the number of people in neighboring regions? To answer these questions you quantify the non-spatial correlation and the bi-variate spatial autocorrelation between tornado occurrences and population. To keep this manageable you focus on one state (Iowa) Start by getting the U.S. Census data with functions from the {tidycensus} package. Downloading U.S. census data using functions from the {tidycensus} package requires you register with the Census Bureau You can get an API key from http://api.census.gov/data/key_signup.html. Then use the tidycensus::census_api_key() function and put your key in quotes. tidycensus::census_api_key(&quot;YOUR API KEY GOES HERE&quot;) The get_decennial() function grants access to the 1990, 2000, and 2010 decennial US Census data and the get_acs() function grants access to the 5-year American Community Survey data. For example, here is how you get county-level population for Iowa Counties.sf &lt;- tidycensus::get_acs(geography = &quot;county&quot;, variables = &quot;B02001_001E&quot;, state = &quot;IA&quot;, geometry = TRUE) ## Getting data from the 2017-2021 5-year ACS ## Downloading feature geometry from the Census website. To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`. ## | | | 0% | | | 1% | |= | 1% | |= | 2% | |== | 2% | |== | 3% | |== | 4% | |=== | 4% | |=== | 5% | |==== | 5% | |==== | 6% | |===== | 6% | |===== | 7% | |===== | 8% | |====== | 8% | |====== | 9% | |======= | 9% | |======= | 10% | |======= | 11% | |======== | 11% | |======== | 12% | |========= | 12% | |========= | 13% | |========= | 14% | |========== | 14% | |========== | 15% | |=========== | 15% | |=========== | 16% | |============ | 16% | |============ | 17% | |============ | 18% | |============= | 18% | |============= | 19% | |============== | 19% | |============== | 20% | |============== | 21% | |=============== | 21% | |=============== | 22% | |================ | 22% | |================ | 23% | |================= | 24% | |================= | 25% | |================== | 25% | |================== | 26% | |=================== | 26% | |=================== | 27% | |=================== | 28% | |==================== | 28% | |==================== | 29% | |===================== | 29% | |===================== | 30% | |===================== | 31% | |====================== | 31% | |====================== | 32% | |======================= | 32% | |======================= | 33% | |======================== | 34% | |======================== | 35% | |========================= | 35% | |========================= | 36% | |========================== | 36% | |========================== | 37% | |=========================== | 38% | |=========================== | 39% | |============================ | 39% | |============================ | 40% | |============================ | 41% | |============================= | 41% | |============================= | 42% | |============================== | 42% | |============================== | 43% | |============================== | 44% | |=============================== | 44% | |=============================== | 45% | |================================ | 45% | |================================ | 46% | |================================= | 47% | |================================= | 48% | |================================== | 48% | |================================== | 49% | |=================================== | 49% | |=================================== | 50% | |=================================== | 51% | |==================================== | 51% | |==================================== | 52% | |===================================== | 52% | |===================================== | 53% | |===================================== | 54% | |====================================== | 54% | |====================================== | 55% | |======================================= | 55% | |======================================= | 56% | |======================================== | 56% | |======================================== | 57% | |======================================== | 58% | |========================================= | 58% | |========================================= | 59% | |========================================== | 59% | |========================================== | 60% | |========================================== | 61% | |=========================================== | 61% | |=========================================== | 62% | |============================================ | 62% | |============================================ | 63% | |============================================ | 64% | |============================================= | 64% | |============================================= | 65% | |============================================== | 65% | |============================================== | 66% | |=============================================== | 66% | |=============================================== | 67% | |=============================================== | 68% | |================================================ | 68% | |================================================ | 69% | |================================================= | 69% | |================================================= | 70% | |================================================= | 71% | |================================================== | 71% | |================================================== | 72% | |=================================================== | 72% | |=================================================== | 73% | |=================================================== | 74% | |==================================================== | 74% | |==================================================== | 75% | |===================================================== | 75% | |===================================================== | 76% | |====================================================== | 76% | |====================================================== | 77% | |====================================================== | 78% | |======================================================= | 78% | |======================================================= | 79% | |======================================================== | 79% | |======================================================== | 80% | |======================================================== | 81% | |========================================================= | 81% | |========================================================= | 82% | |========================================================== | 82% | |========================================================== | 83% | |========================================================== | 84% | |=========================================================== | 84% | |=========================================================== | 85% | |============================================================ | 85% | |============================================================ | 86% | |============================================================= | 86% | |============================================================= | 87% | |============================================================= | 88% | |============================================================== | 88% | |============================================================== | 89% | |=============================================================== | 89% | |=============================================================== | 90% | |=============================================================== | 91% | |================================================================ | 91% | |================================================================ | 92% | |================================================================= | 92% | |================================================================= | 93% | |================================================================= | 94% | |================================================================== | 94% | |================================================================== | 95% | |=================================================================== | 95% | |=================================================================== | 96% | |==================================================================== | 96% | |==================================================================== | 97% | |==================================================================== | 98% | |===================================================================== | 98% | |===================================================================== | 99% | |======================================================================| 99% | |======================================================================| 100% The code returns a simple feature data frame with county borders as multi-polygons. The variable B02001_001E is the 2016-2020 population estimate in each county within the state Next get the tornado data and count the number of tracks by county. A single track can intersect more than one county Torn.sf &lt;- sf::st_read(dsn = here::here(&quot;data&quot;, &quot;1950-2021-torn-aspath&quot;), layer = &quot;1950-2021-torn-aspath&quot;) |&gt; sf::st_transform(crs = sf::st_crs(Counties.sf)) |&gt; dplyr::filter(yr &gt;= 2016) ## Reading layer `1950-2021-torn-aspath&#39; from data source ## `/Users/jelsner/Desktop/ClassNotes/ASS-2023/data/1950-2021-torn-aspath&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 67558 features and 22 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -163.53 ymin: 17.7212 xmax: -64.7151 ymax: 61.02 ## Geodetic CRS: WGS 84 ( TorCounts.df &lt;- Torn.sf |&gt; sf::st_intersection(Counties.sf) |&gt; sf::st_drop_geometry() |&gt; dplyr::group_by(GEOID) |&gt; dplyr::summarize(nT = dplyr::n()) ) ## Warning: attribute variables are assumed to be spatially constant throughout ## all geometries ## # A tibble: 92 × 2 ## GEOID nT ## &lt;chr&gt; &lt;int&gt; ## 1 19001 7 ## 2 19003 3 ## 3 19007 5 ## 4 19009 2 ## 5 19011 11 ## 6 19013 4 ## 7 19015 7 ## 8 19017 3 ## 9 19019 6 ## 10 19021 6 ## # … with 82 more rows Next join the counts to the simple feature data frame by the common column name GEOID Counties.sf &lt;- Counties.sf |&gt; dplyr::left_join(TorCounts.df, by = &quot;GEOID&quot;) |&gt; dplyr::mutate(nT = tidyr::replace_na(nT, 0)) |&gt; dplyr::mutate(Area = sf::st_area(Counties.sf), rate = nT/Area/(2020 - 2016 + 1) * 10^10, lpop = log10(estimate)) Note that some counties had no tornadoes and the dplyr::left_join() returns a value of NA for those. You use dplyr::mutate() with tidyr::replace_na() to turn those counts to a value of 0. Make a two-panel map displaying the log of the population and the tornado rates. map1 &lt;- tmap::tm_shape(Counties.sf) + tmap::tm_borders(col = &quot;gray70&quot;) + tmap::tm_fill(col = &quot;lpop&quot;, title = &quot;Log Population&quot;, palette = &quot;Blues&quot;) + tmap::tm_layout(legend.outside = &quot;TRUE&quot;) map2 &lt;- tmap::tm_shape(Counties.sf) + tmap::tm_borders(col = &quot;gray70&quot;) + tmap::tm_fill(col = &quot;rate&quot;, title = &quot;Annual Rate\\n[/10,000 sq. km]&quot;, palette = &quot;Greens&quot;) + tmap::tm_layout(legend.outside = &quot;TRUE&quot;) tmap::tmap_arrange(map1, map2) There appears some relationship. The non-spatial correlation between the two variables is obtained with the cor.test() function lpop &lt;- Counties.sf$lpop rate &lt;- as.numeric(Counties.sf$rate) cor.test(lpop, rate) ## ## Pearson&#39;s product-moment correlation ## ## data: lpop and rate ## t = 2.7884, df = 97, p-value = 0.006376 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.07925894 0.44584430 ## sample estimates: ## cor ## 0.2724085 The bi-variate spatial autocorrelation is assessed using the Lee statistic. A formal non-parametric test under the null hypothesis of no bi-variate spatial autocorrelation is done using a Monte Carlo simulation nbs &lt;- spdep::poly2nb(Counties.sf) wts &lt;- spdep::nb2listw(nbs) Lee &lt;- spdep::lee(lpop, rate, listw = wts, n = length(nbs)) Lee$L ## [1] 0.1755534 spdep::lee.mc(lpop, rate, listw = wts, nsim = 9999) ## ## Monte-Carlo simulation of Lee&#39;s L ## ## data: lpop , rate ## weights: wts ## number of simulations + 1: 10000 ## ## statistic = 0.17555, observed rank = 9999, p-value = 1e-04 ## alternative hypothesis: greater Finally you map out the local variation in the bi-variate spatial autocorrelation Counties.sf$localL &lt;- Lee$localL tmap::tm_shape(Counties.sf) + tmap::tm_fill(&quot;localL&quot;, title = &quot;Local Bivariate\\nSpatial Autocorrelation&quot;) + tmap::tm_borders(col = &quot;gray70&quot;) + tmap::tm_layout(legend.outside = TRUE) ## Variable(s) &quot;localL&quot; contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette. What might cause this? Cedar County (dark green) lies between the cities of Cedar Rapids (to the northwest), Iowa City (to the west) and the Quad Cities (to the east). Commuters from neighboring counties into cities like Cedar Rapids provide an ad hoc spotter network for all kinds of phenomenon including severe weather and tornadoes Repeat this analysis for the state of Kansas and compare/contrast the results and interpretation Also, compare local Lee with local Moran Ii_stats &lt;- spdep::localmoran(rate, listw = wts) Counties.sf$localI = Ii_stats[, 1] tmap::tm_shape(Counties.sf) + tmap::tm_borders(col = &quot;gray70&quot;) + tmap::tm_fill(col = &quot;localI&quot;, title = &quot;Local Autocorrelation&quot;, palette = &quot;Purples&quot;) + tmap::tm_layout(legend.outside = &quot;TRUE&quot;) The clustering of tornado occurrences coincides with the largest population corridor in the state Constraining group membership based on spatial autocorrelation As a spatial data analyst you likely will face the situation in which there are many variables and you need to group them in a way that minimizes inter-group variation but maximizes between-group variation. If you know the number of groups before hand then a common grouping (or clustering) method is called K-means If your data is spatial you may want the additional constraint that the resulting groups be geographically linked. There are many situations that require separating geographies into discrete but contiguous regions (“regionalization”) such as designing communities, planning areas, amenity zones, logistical units, or for setting up experiments with real world geographic constraints Optimal grouping using only traditional cluster metrics is generally sub-optimal in practice for regionalization because the metrics do not consider geographic contiguity Unconstrained grouping on data with spatial characteristics may result in contiguous regions because of autocorrelation, but if you want to ensure that all groups are spatially-contiguous you need a method specifically designed for the task. The ‘skater’ algorithm available in the {spdep} package is well-implemented and well-documented The ‘skater’ algorithm (spatial ’k’luster analysis by tree edge removal) builds a connectivity graph to represent spatial relationships between neighboring areas, where each area is represented by a node and edges represent connections between areas. Edge costs are calculated by evaluating the dissimilarity in attribute space between neighboring areas. The connectivity graph is reduced by pruning edges with higher dissimilarity Consider again the crime data at the tract level in the city of Columbus, Ohio. The tract polygons are projected with arbitrary spatial coordinates ( CC.sf &lt;- sf::st_read(dsn = here::here(&quot;data&quot;, &quot;columbus&quot;), layer = &quot;columbus&quot;) ) ## Reading layer `columbus&#39; from data source ## `/Users/jelsner/Desktop/ClassNotes/ASS-2023/data/columbus&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 49 features and 20 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 5.874907 ymin: 10.78863 xmax: 11.28742 ymax: 14.74245 ## CRS: NA ## Simple feature collection with 49 features and 20 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 5.874907 ymin: 10.78863 xmax: 11.28742 ymax: 14.74245 ## CRS: NA ## First 10 features: ## AREA PERIMETER COLUMBUS_ COLUMBUS_I POLYID NEIG HOVAL INC CRIME ## 1 0.309441 2.440629 2 5 1 5 80.467 19.531 15.725980 ## 2 0.259329 2.236939 3 1 2 1 44.567 21.232 18.801754 ## 3 0.192468 2.187547 4 6 3 6 26.350 15.956 30.626781 ## 4 0.083841 1.427635 5 2 4 2 33.200 4.477 32.387760 ## 5 0.488888 2.997133 6 7 5 7 23.225 11.252 50.731510 ## 6 0.283079 2.335634 7 8 6 8 28.750 16.029 26.066658 ## 7 0.257084 2.554577 8 4 7 4 75.000 8.438 0.178269 ## 8 0.204954 2.139524 9 3 8 3 37.125 11.337 38.425858 ## 9 0.500755 3.169707 10 18 9 18 52.600 17.586 30.515917 ## 10 0.246689 2.087235 11 10 10 10 96.400 13.598 34.000835 ## OPEN PLUMB DISCBD X Y NSA NSB EW CP THOUS NEIGNO ## 1 2.850747 0.217155 5.03 38.80 44.07 1 1 1 0 1000 1005 ## 2 5.296720 0.320581 4.27 35.62 42.38 1 1 0 0 1000 1001 ## 3 4.534649 0.374404 3.89 39.82 41.18 1 1 1 0 1000 1006 ## 4 0.394427 1.186944 3.70 36.50 40.52 1 1 0 0 1000 1002 ## 5 0.405664 0.624596 2.83 40.01 38.00 1 1 1 0 1000 1007 ## 6 0.563075 0.254130 3.78 43.75 39.28 1 1 1 0 1000 1008 ## 7 0.000000 2.402402 2.74 33.36 38.41 1 1 0 0 1000 1004 ## 8 3.483478 2.739726 2.89 36.71 38.71 1 1 0 0 1000 1003 ## 9 0.527488 0.890736 3.17 43.44 35.92 1 1 1 0 1000 1018 ## 10 1.548348 0.557724 4.33 47.61 36.42 1 1 1 0 1000 1010 ## geometry ## 1 POLYGON ((8.624129 14.23698... ## 2 POLYGON ((8.25279 14.23694,... ## 3 POLYGON ((8.653305 14.00809... ## 4 POLYGON ((8.459499 13.82035... ## 5 POLYGON ((8.685274 13.63952... ## 6 POLYGON ((9.401384 13.5504,... ## 7 POLYGON ((8.037741 13.60752... ## 8 POLYGON ((8.247527 13.58651... ## 9 POLYGON ((9.333297 13.27242... ## 10 POLYGON ((10.08251 13.03377... First, create choropleth maps of housing value, income, and crime. tmap::tm_shape(CC.sf) + tmap::tm_fill(col = c(&quot;HOVAL&quot;, &quot;INC&quot;, &quot;CRIME&quot;)) ## Warning: Currect projection of shape CC.sf unknown. Long-lat (WGS84) is ## assumed. The maps show distinct regional patterns. Housing values and income are clustered toward the southeast and crime is clustered in the center. But although housing values are also high in the north you don’t necessarily want to group that tract with those in the southeast because they are geographically distinct To group these patterns under the constraint of spatial contiguity you first scale the attribute values and center them using the scale() function. Scaling and centering variables should be done before any type of clustering procedure ( CCs.df &lt;- CC.sf |&gt; dplyr::mutate(HOVALs = scale(HOVAL), INCs = scale(INC), CRIMEs = scale(CRIME)) |&gt; dplyr::select(HOVALs, INCs, CRIMEs) |&gt; sf::st_drop_geometry() ) ## HOVALs INCs CRIMEs ## 1 2.27610855 0.90403637 -1.15961852 ## 2 0.33200225 1.20228067 -0.97579369 ## 3 -0.65450986 0.27721488 -0.26906635 ## 4 -0.28355918 -1.73545197 -0.16382075 ## 5 -0.82373916 -0.54755948 0.93250061 ## 6 -0.52454175 0.29001413 -0.54160387 ## 7 1.98005188 -1.04095129 -2.08883352 ## 8 -0.07100723 -0.53265603 0.19704853 ## 9 0.76701615 0.56301041 -0.27569218 ## 10 3.13893423 -0.13622431 -0.06741470 ## 11 -1.01462975 -1.21120127 1.62242856 ## 12 -1.00379913 -0.75866244 1.28954855 ## 13 0.17674452 -0.84615445 0.69251980 ## 14 0.24172862 -0.77356589 1.31109176 ## 15 -1.10669054 -0.78934601 0.80424271 ## 16 -1.06336790 -1.18349839 1.17796908 ## 17 0.17945213 -0.80249612 0.10398880 ## 18 1.16775124 -0.20863754 0.52794726 ## 19 -0.42435801 -0.48338699 1.15903863 ## 20 2.31943098 2.92722331 -2.08611253 ## 21 -0.99973763 -0.65223429 0.29555480 ## 22 -0.43248096 -0.46743153 -0.08509252 ## 23 0.50345189 1.18878008 -0.90128119 ## 24 0.79950834 -0.02436078 0.18939933 ## 25 -1.11210588 -1.03691859 1.56408123 ## 26 -0.98213783 -1.10284443 0.34908475 ## 27 -0.23482130 -0.62295340 1.05579183 ## 28 -0.84404667 -1.14299607 1.30234528 ## 29 -0.32146659 -0.99834496 1.53128622 ## 30 -0.86300035 -0.08222123 2.01787200 ## 31 -0.35937401 0.44974438 -1.04300226 ## 32 0.10092968 0.80076407 -0.95524408 ## 33 -0.80343164 -0.78145595 0.40875576 ## 34 -0.54078771 0.10047751 -0.66667072 ## 35 -0.61931016 -0.27368671 0.24182446 ## 36 -0.11568382 0.76517130 -1.24451072 ## 37 0.26338981 0.46324498 0.43725866 ## 38 -0.85216962 -0.57298301 1.11056729 ## 39 0.06302227 0.71923344 -0.95791733 ## 40 1.27335038 2.71033430 -1.12882028 ## 41 0.19840570 1.37323217 -0.96961443 ## 42 0.31933030 2.01600877 -1.11384361 ## 43 -0.68970950 -0.17444727 0.09172721 ## 44 -0.26731322 0.45342623 -0.54784308 ## 45 -0.57961574 -0.04206959 -0.36458895 ## 46 2.03962048 0.69240723 -1.11153410 ## 47 0.22006716 0.80216710 -0.43664372 ## 48 -0.63014089 -0.44919672 -0.50702314 ## 49 -0.14276051 0.77516538 -0.75228685 Next create adjacency neighbors using queen contiguity. Note queen contiguity is the default but here you specify TRUE as a reminder that you can change this nbs &lt;- spdep::poly2nb(CC.sf, queen = TRUE) plot(CC.sf$geometry) plot(nbs, sf::st_centroid(sf::st_geometry(CC.sf)), add = TRUE) Next combine the contiguity graph with your scaled attribute data to calculate edge costs based on distances between each node. The function spdep::nbcosts() provides distance methods for Euclidean, Manhattan, Canberra, binary, Minkowski, and Mahalanobis, and defaults to Euclidean if not specified like here. Save the object of class nbdist with name edge_costs edge_costs &lt;- spdep::nbcosts(nbs, data = CCs.df) Next transform the edge costs into spatial weights using the spdep::nb2listw() function before constructing the minimum spanning tree with the weights list. wts &lt;- spdep::nb2listw(nbs, glist = edge_costs, style = &quot;B&quot;) mst &lt;- spdep::mstree(wts) head(mst) ## [,1] [,2] [,3] ## [1,] 13 14 0.6261975 ## [2,] 13 8 0.6365171 ## [3,] 14 19 0.7422906 ## [4,] 8 5 1.0524815 ## [5,] 5 15 0.3936652 ## [6,] 15 16 0.5448893 Edges with higher dissimilarity are removed leaving a set of nodes and edges that take the minimum sum of dissimilarities across all edges of the tree (a minimum spanning tree). The edge connecting node (tract) 12 with node (tract) 16 has a dissimilarity of .44 units. The edge connecting tract 16 with tract 25 has a dissimilarity of .42 units Finally, the spdep::skater() function partitions the graph by identifying which edges to remove based on dissimilarity while maximizing the between-group variation. The ncuts = argument specifies the number of partitions to make, resulting in ncuts + 1 groups clus5 &lt;- spdep::skater(edges = mst[, 1:2], data = CCs.df, ncuts = 4) Where are these groups located? CC.sf &lt;- CC.sf |&gt; dplyr::mutate(Group = clus5$groups) library(ggplot2) ggplot() + geom_sf(data = CC.sf, mapping = aes(fill = factor(Group))) The map shows five distinct regions based on the three variables of income, housing value, and crime. Importantly the regions are contiguous Region 1 encompasses most tracts in the urban core where housing values and income are low and crime rates are highest. Regions 2 and 3 in the east and west are where housing values and income are moderately high and crime rates are lower. Region 4 is where income and housing values are highest and crime is the lowest As a comparison, here is the result of grouping the same three variables using hierarchical clustering using the method of minimum variance (Ward) and without regard to spatial contiguity dd &lt;- dist(CCs.df) hc &lt;- hclust(dd, method = &quot;ward.D&quot;) hcGroup &lt;- cutree(hc, k = 5) CC.sf &lt;- CC.sf |&gt; dplyr::mutate(hcGroup = hcGroup) ggplot() + geom_sf(data = CC.sf, mapping = aes(fill = factor(hcGroup))) Here the map shows five regions but the regions are not contiguous. More information: https://www.tandfonline.com/doi/abs/10.1080/13658810600665111 Also the {motif} package has functions that implement and extend ideas of the pattern-based spatial analysis. They can be used to describe spatial patterns of categorical raster data for any defined regular and irregular areas Patterns are represented quantitatively using built-in signatures based on co-occurrence matrices but the functions are flexible to allow for user-defined functions. Functions enable spatial analysis such as search, change detection, and clustering to be performed on spatial patterns https://jakubnowosad.com/motif/ Estimating spatial autocorrelation in model residuals A spatial regression model should be entertained for your data whenever the residuals from an ordinary-least-squares (OLS) regression model exhibit significant spatial autocorrelation So you first fit an OLS regression model regressing the response variable onto the explanatory variables and then check for autocorrelation in the residuals. If there is significant spatial autocorrelation in the residuals then you should consider some type of spatial regression model Staying with the crime data at the tract level in the city of Columbus, Ohio CC.sf &lt;- sf::st_read(dsn = here::here(&quot;data&quot;, &quot;columbus&quot;), layer = &quot;columbus&quot;) ## Reading layer `columbus&#39; from data source ## `/Users/jelsner/Desktop/ClassNotes/ASS-2023/data/columbus&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 49 features and 20 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 5.874907 ymin: 10.78863 xmax: 11.28742 ymax: 14.74245 ## CRS: NA CRIME is the response variable and INC and HOVAL as the explanatory variables. How well do these two explanatory variables statistically explain the amount of crime at the tract level? An answer to this question is obtained by regressing crime onto income and housing values. Here you use the lm() function and save the results to the object model.ols Set the formula, then use the formula as the first argument in the lm() function. Summarize the results with the summary() method f &lt;- CRIME ~ INC + HOVAL model.ols &lt;- lm(f, data = CC.sf) summary(model.ols) ## ## Call: ## lm(formula = f, data = CC.sf) ## ## Residuals: ## Min 1Q Median 3Q Max ## -34.418 -6.388 -1.580 9.052 28.649 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 68.6190 4.7355 14.490 &lt; 2e-16 *** ## INC -1.5973 0.3341 -4.780 1.83e-05 *** ## HOVAL -0.2739 0.1032 -2.654 0.0109 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.43 on 46 degrees of freedom ## Multiple R-squared: 0.5524, Adjusted R-squared: 0.5329 ## F-statistic: 28.39 on 2 and 46 DF, p-value: 9.341e-09 The model statistically explains 55% of the variation in crime as is seen by looking at the multiple R-squared value Looking at the coefficients (values under the Estimate column), you see that higher incomes are associated with lower values of crime (negative coefficient) and higher housing values are associated with lower crime. For every one unit increase in income, crime values decrease by 1.6 units Use the residuals() method to extract the vector of residuals from the model object ( res &lt;- residuals(model.ols) ) ## 1 2 3 4 5 6 ## 0.3465419 -3.6947990 -5.2873940 -19.9855151 6.4475490 -9.0734793 ## 7 8 9 10 11 12 ## -34.4177224 -1.9146840 4.3960594 13.5091017 10.9800573 9.5877236 ## 13 14 15 16 17 18 ## 4.7728320 16.1128397 0.6675424 3.5491565 -4.6630963 12.8399569 ## 19 20 21 22 23 24 ## 12.8428644 3.4948724 -6.0537589 -7.8697868 -1.7037730 6.9913819 ## 25 26 27 28 29 30 ## 11.0984343 -9.1741523 10.8026296 7.1086321 14.9005133 28.6487456 ## 31 32 33 34 35 36 ## -15.1722792 -8.1776706 -4.3438864 -12.9749799 -1.5798172 -14.4376850 ## 37 38 39 40 41 42 ## 12.8687861 9.0515532 -9.1569014 12.2449674 -2.7098171 1.3443547 ## 43 44 45 46 47 48 ## -3.5432909 -6.3880045 -9.4155428 -1.9731210 1.1150296 -15.7632989 ## 49 ## -6.2476690 There are 49 residuals, one for each tract. The residuals are the difference between the observed crime rates and the predicted crime rates (observed - predicted). A residual that has a value greater than 0 indicates that the model under predicts the observed crime rate in that tract and a residual that has a value less than 0 indicates that the model over predicts the observed crime rate. A normal distribution should be a good approximation to the distribution of the residuals. You check this with the sm::sm.density() function with the first argument the vector of residuals (res) and the argument model = set to “Normal” sm::sm.density(res, model = &quot;Normal&quot;) The density curve of the residuals (black line) fits completely within the blue ribbon that defines a normal distribution Next create a map of the model residuals. Do the residuals show any pattern of clustering? Since the values in the vector of residuals res are arranged in the same order as the rows in the simple feature data frame you create a new column in the data frame using the $ syntax and calling the new column res CC.sf$res &lt;- res tmap::tm_shape(CC.sf) + tmap::tm_fill(col = &quot;res&quot;) + tmap::tm_borders(col = &quot;gray70&quot;) + tmap::tm_layout(title = &quot;Linear model residuals&quot;) ## Warning: Currect projection of shape CC.sf unknown. Long-lat (WGS84) is ## assumed. ## Variable(s) &quot;res&quot; contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette. The map shows contiguous tracts with negative residuals across the southwestern and southern part of the city and a group of contiguous tracts with positive residuals toward the center The map indicates some clustering but the clustering appears to be less than with the crime values themselves. That is, after accounting for regional factors related to crime, the autocorrelation is reduced To determine the amount of autocorrelation in the residuals use the spdep::lm.morantest() function, passing the regression model object and the weights object to it. Note that you once again use the default neighborhood and weighting schemes to generate the weights matrix wts nbs &lt;- CC.sf |&gt; spdep::poly2nb() wts &lt;- nbs |&gt; spdep::nb2listw() model.ols |&gt; spdep::lm.morantest(listw = wts) ## ## Global Moran I for regression residuals ## ## data: ## model: lm(formula = f, data = CC.sf) ## weights: wts ## ## Moran I statistic standard deviate = 2.8393, p-value = 0.00226 ## alternative hypothesis: greater ## sample estimates: ## Observed Moran I Expectation Variance ## 0.222109407 -0.033418335 0.008099305 Moran’s I on the model residuals is .22. This compares with the value of .5 on the value of crime alone m &lt;- CC.sf$CRIME |&gt; length() s &lt;- wts |&gt; spdep::Szero() CC.sf$CRIME |&gt; spdep::moran(listw = wts, n = m, S0 = s) ## $I ## [1] 0.5001886 ## ## $K ## [1] 2.225946 Part of the autocorrelation in the crime rates is statistically ‘absorbed’ by the explanatory factors The \\(p\\)-value on I of .002, thus you reject the null hypothesis of no spatial autocorrelation in the model residuals and conclude that a spatial regression model would be an improvement over the non-spatial OLS model. The \\(z\\)-value (as the basis for the \\(p\\)-value) takes into account the fact that these are residuals from a model so the variance is adjusted accordingly Given significant spatial autocorrelation in the model residuals, the next step is to choose the type of spatial regression model Choosing a spatial regression model Ordinary least-squares regression models fit to spatial data can lead to improper inference because observations are not independent. This might lead to poor policy decisions. If the residuals from an OLS model are strongly correlated the model is not specified properly You can try to improve the model by adding variables. If that’s not possible (no additional data, or no clue as to what variable to include), you can try a spatial regression model. Spatial regression models are widely used in econometrics and epidemiology The equation for a regression model in vector notation is \\[ y = X \\beta + \\varepsilon \\] where \\(y\\) is a \\(n\\) by 1 vector of response variable values, \\(X\\) is a \\(n\\) by \\(p+1\\) matrix containing the explanatory variables and augmented by a column of ones for the intercept term, \\(\\beta\\) is a \\(p+1\\) \\(\\times\\) 1 vector of model coefficients and \\(\\varepsilon\\) is a \\(n\\) by 1 vector of residuals (iid: independent and identically distributed) A couple options exist if the elements of the vector \\(\\varepsilon\\) are correlated. One is to include a spatial lag term so the model becomes \\[ y = \\rho W y + X \\beta + \\varepsilon \\] where \\(Wy\\) is the weighted average of the neighborhood response values with \\(W\\) the spatial weights matrix, and \\(\\rho\\) is the autoregression coefficient. This is called a spatial autoregressive (SAR) model Note: \\(Wy\\) is the spatial lag variable you compute with the spdep::lag.listw() function and \\(\\rho\\) is Moran’s I. Thus the model is also called a spatial lag model (SLM) Justification for the spatial lag model is domain specific but motivated by a ‘diffusion’ process. The response variable \\(y_i\\) is influenced by the explanatory variables at location \\(i\\) and by explanatory variables at locations \\(j\\) \\(\\rho Wy\\) is called the spatial signal term and \\(\\beta X\\) is called the trend term Another option is to include a spatial error term so the model becomes \\[ y = X\\beta + \\lambda W \\epsilon + u \\] where \\(\\lambda\\) is the autoregression coefficient, \\(W\\epsilon\\) is the spatial error term representing the weighted average of the neighborhood residuals, and \\(u\\) are the overall residuals assumed to be iid. This is called a spatial error model (SEM) Here the lag term is computed using the residuals rather the response variable Application of the spatial error model is motivated by the omitted variable bias. Suppose the variable \\(y\\) is statistically described by two variables \\(x\\) and \\(z\\) each centered on zero and independent. Then \\[ y = \\beta x + \\theta z \\] If \\(z\\) is not observed, then the vector \\(\\theta z\\) is nested in the error term \\(\\epsilon\\) \\[ y = \\beta x + \\epsilon \\] Examples of an unobserved latent variable \\(z\\) include local culture, social capital, neighborhood readiness. Importantly you would expect the latent variable to be spatially correlated (e.g., culture will be similar across neighborhoods), so let \\[ z = \\lambda W z + r\\\\ z = (I - \\lambda W)^{-1} r \\] where \\(r\\) is a vector of random independent residuals (e.g., culture is similar but not identical), \\(W\\) is the spatial weights matrix and \\(\\lambda\\) is a scalar spatial correlation parameter. Substituting into the equation above \\[ y = \\beta x + \\theta z \\\\ y = \\beta x + \\theta (I - \\lambda W)^{-1} r\\\\ y = \\beta x + (I - \\lambda W)^{-1} \\varepsilon \\] where \\[ \\varepsilon = \\theta r \\] Another motivation for considering a spatial error model is heterogeneity. Suppose you have multiple observations for each unit. If you want a model that incorporates individual effects you can include a \\(n \\times 1\\) vector \\(a\\) of individual intercepts for each unit \\[ y = a + X\\beta \\] where now \\(X\\) is a \\(n\\) \\(\\times\\) \\(p\\) matrix In a cross-sectional setting with one observation per unit (typically the case in observational studies), this approach is not possible since you will have more parameters than observations Instead you can treat \\(a\\) as a vector of spatial random effects. You assume that the intercepts follows a spatially smoothed process \\[ a = \\lambda W a + \\epsilon \\\\ a = (I - \\lambda W)^{-1} \\epsilon \\] which leads to the previous model \\[ y = X\\beta + (I - \\lambda W)^{-1} \\epsilon \\] In the absence of domain-specific knowledge of the process that might be responsible for the spatially autocorrelated residuals, you can run some statistical tests on the linear model The tests are performed with the spdep::lm.LMtests() function. The LM stands for ‘Lagrange multiplier’ indicating that the technique simultaneously determines the coefficients on the explanatory variables AND the coefficient on the spatial lag variable The test type is specified as a character string. The tests should be considered in a sequence starting with the standard versions and moving to the ‘robust’ versions if the choice remains ambiguous To perform LM tests you specify the model object, the weights matrix, and the two model types using the test = argument. The model types are specified as character strings \"LMerr\" and \"LMlag\" for the spatial error and lag models, respectively model.ols |&gt; spdep::lm.LMtests(listw = wts, test = c(&quot;LMerr&quot;, &quot;LMlag&quot;)) ## ## Lagrange multiplier diagnostics for spatial dependence ## ## data: ## model: lm(formula = f, data = CC.sf) ## weights: wts ## ## LMerr = 5.2062, df = 1, p-value = 0.02251 ## ## ## Lagrange multiplier diagnostics for spatial dependence ## ## data: ## model: lm(formula = f, data = CC.sf) ## weights: wts ## ## LMlag = 8.898, df = 1, p-value = 0.002855 The output shows that both the spatial error and spatial lag models are significant (\\(p\\)-value &lt; .15). Ideally one model is significant and the other is not, and you choose the model that is significant Since both are significant, you test again. This time you use the robust forms of the statistics with character strings \"RLMerr\" and \"RLMlag\" in the test = argument model.ols |&gt; spdep::lm.LMtests(listw = wts, test = c(&quot;RLMerr&quot;, &quot;RLMlag&quot;)) ## ## Lagrange multiplier diagnostics for spatial dependence ## ## data: ## model: lm(formula = f, data = CC.sf) ## weights: wts ## ## RLMerr = 0.043906, df = 1, p-value = 0.834 ## ## ## Lagrange multiplier diagnostics for spatial dependence ## ## data: ## model: lm(formula = f, data = CC.sf) ## weights: wts ## ## RLMlag = 3.7357, df = 1, p-value = 0.05326 Here the error model has a large \\(p\\)-value and the lag model has a \\(p\\)-value that is less than .15 so you choose the lag model for your spatial regression A decision tree (from Luc Anselin) shows the sequence of tests for making a choice about which type of spatial model to use Decision Tree If both tests show significance models, then you should fit both models and check which one results in the lowest information criteria (AIC) Another options is to include both a spatial lag term and a spatial error term into a single model Ordinary least-squares regression models fit to spatial data can lead to improper inference because observations are not independent. This might lead to poor policy decisions. Thus it’s necessary to check the residuals from an aspatial model for autocorrelation. If the residuals are strongly correlated the model is not specified properly Fitting and interpreting spatial regression models Recall that the marginal effect of income on crime is -1.6 and the marginal effect of housing value on crime is -.27 model.ols ## ## Call: ## lm(formula = f, data = CC.sf) ## ## Coefficients: ## (Intercept) INC HOVAL ## 68.6190 -1.5973 -0.2739 A nice way to visualize the relative significance of the explanatory variables is to make a plot. Here you use the broom::tidy() method and then ggplot() as follows if(!require(broom)) install.packages(pkgs = &quot;broom&quot;, repos = &quot;http://cran.us.r-project.org&quot;) ## Loading required package: broom library(broom) ( d &lt;- broom::tidy(model.ols, conf.int = TRUE) ) ## # A tibble: 3 × 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 68.6 4.74 14.5 9.21e-19 59.1 78.2 ## 2 INC -1.60 0.334 -4.78 1.83e- 5 -2.27 -0.925 ## 3 HOVAL -0.274 0.103 -2.65 1.09e- 2 -0.482 -0.0662 library(ggplot2) ggplot(d[-1,], aes(x = estimate, # we do not plot the intercept term y = term, xmin = conf.low, xmax = conf.high, height = 0)) + geom_point(size = 2) + geom_vline(xintercept = 0, lty = 4) + geom_errorbarh() The maximum likelihood estimate is shown as a point and the confidence interval around the estimate is shown as a horizontal error bar. The default confidence level is 95% (conf.level = .95). The effects are statistically significant as the confidence intervals do not intersect the zero line (dashed-dotted) But you’ve shown above that the model residuals have significant spatial autocorrelation so reporting the marginal effects with an OLS regression model is incorrect Instead, you fit a spatially-lagged Y model using the lagsarlm() function from the {spatialreg} package. The model is \\[ y = \\rho W y + X \\beta + \\varepsilon \\] where \\(Wy\\) is the weighted average of the neighborhood response values (spatial lag variable) with \\(W\\) the spatial weights matrix, and \\(\\rho\\) is the autoregression coefficient The spatialreg::lagsarlm() function first determines a value for \\(\\rho\\) ( with the internal optimize() function) and then the \\(\\beta\\)’s are obtained using generalized least squares (GLS). The model formula f is the same as what you used to fit the OLS regression above. You save the model object as model.slym if(!require(spatialreg)) install.packages(pkgs = &quot;spatialreg&quot;, repos = &quot;http://cran.us.r-project.org&quot;) ## Loading required package: spatialreg ## Loading required package: spData ## Loading required package: Matrix ## Loading required package: sf ## Linking to GEOS 3.10.2, GDAL 3.4.2, PROJ 8.2.1; sf_use_s2() is TRUE model.slym &lt;- spatialreg::lagsarlm(formula = f, data = CC.sf, listw = wts) summary(model.slym) ## ## Call:spatialreg::lagsarlm(formula = f, data = CC.sf, listw = wts) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.652017 -5.334611 0.071473 6.107196 23.302618 ## ## Type: lag ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 45.603250 7.257404 6.2837 3.306e-10 ## INC -1.048728 0.307406 -3.4115 0.000646 ## HOVAL -0.266335 0.089096 -2.9893 0.002796 ## ## Rho: 0.42333, LR test value: 9.4065, p-value: 0.0021621 ## Asymptotic standard error: 0.11951 ## z-value: 3.5422, p-value: 0.00039686 ## Wald statistic: 12.547, p-value: 0.00039686 ## ## Log likelihood: -182.674 for lag model ## ML residual variance (sigma squared): 96.857, (sigma: 9.8416) ## Number of observations: 49 ## Number of parameters estimated: 5 ## AIC: 375.35, (AIC for lm: 382.75) ## LM test for residual autocorrelation ## test value: 0.24703, p-value: 0.61917 The first batch of output concerns the model residuals and the coefficients on the explanatory variables. The model residuals are the observed crime rates minus the predicted crime rates The coefficients on income and housing have the same sign (negative) and they remain statistically significant (-1.05 for income and -.27 for housing value). But you can’t interpret these coefficients as the marginal effects The next set of output is about the coefficient of spatial autocorrelation (\\(\\rho\\)). The value is .423 and a likelihood ratio test gives a value of 9.41 which translates to a \\(p\\)-value of .002. The null hypothesis is the autocorrelation is zero, so you confidently reject it. This is consistent with the significant Moran’s I value that you found in the linear model residuals Two other tests are performed on the value of \\(\\rho\\) including a z-test (t-test) using the asymptotic standard error and a Wald test. Both tests confirm that the lag term should be included in the model from crime involving income and housing values In spatial models that contain a lagged response term, the coefficients are not marginal effects. The spatial lag model allows for ‘spillover’. That is a change in an explanatory variable anywhere in the study domain will affect the value of the response variable everywhere. Spillover occurs even when the neighborhood weights matrix represents local contiguity. The spillover makes interpreting the coefficients more complicated With a spatially-lagged Y model a change in the value of an explanatory variable results in both direct and indirect effects on the response variable For example, the direct effect gives the impact a change in income has on crime averaged over all tracts. It takes into account the effects that occur from a change in the \\(i\\)th tract’s income on crime across neighboring tracts The indirect effect gives the impact of a change in income has on crime averaged over all other tracts. The indirect effect represent spillovers. The influences on the dependent variable \\(y\\) in a region rendered by change in \\(x\\) in some other region. For example, if all tracts \\(i \\ne j\\) (i not equal to j) increase their income, what will be the impact on crime in region \\(i\\)? The total effect (TE) is the sum of the direct and indirect effects. It measures the total cumulative impact on crime arising from one tract \\(j\\) increasing its income over all other tracts (on average). It is given by \\[ \\hbox{TE} = \\left(\\frac{\\beta_k}{1-\\rho^2}\\right)\\left(1 + \\rho\\right) \\] where \\(\\beta_k\\) is the effect of variable \\(k\\) and \\(\\rho\\) is the spatial autocorrelation coefficient. With \\(\\rho = 0\\) TE is \\(\\beta_k\\) Here \\(\\beta_{INC}\\) is -1.0487 and \\(\\rho\\) is .4233, so the total effect is ( TE_INC &lt;- -1.0487 / (1 - .4233^2) * (1 + .4233) ) ## [1] -1.81845 The direct, indirect, and total effects are shown using the spatialreg::impacts() function model.slym |&gt; spatialreg::impacts(listw = wts) ## Impact measures (lag, exact): ## Direct Indirect Total ## INC -1.1008955 -0.7176833 -1.8185788 ## HOVAL -0.2795832 -0.1822627 -0.4618459 The direct effects are the changes in the response variable of a particular region arising from a one unit increase in an explanatory variable in that region The indirect effects are the changes in the response variable of a particular region arising from a one unit increase in an explanatory variable in another region. For example, due to spatial autocorrelation, a one-unit change in the income variable in region 1 affects the crime rate in regions 2 and 3 The next set of output concerns the overall model fit. It includes the log likelihood value and the AIC (Akaike Information Criterion). The AIC value for the linear model is included. Here it is clear that the spatial lag model is an improvement (smaller AIC) over the aspatial model The larger the likelihood, the better the model and two times the difference in log likelihoods from two competing models divided by the number of observations gives a scale for how much improvement x &lt;- 2 * (logLik(model.slym) - logLik(model.ols))/49 x[1] ## [1] 0.1919701 Improvement table Likelihood difference Qualitative improvement 1 huge .1 large .01 good .001 okay The final bit of output is a Lagrange multiplier test for remaining autocorrelation. The null hypothesis is there is no remaining autocorrelation since we have a lag term in the model. The result is a high \\(p\\)-value so you are satisfied that the lag term takes care of the autocorrelation Compare the spatial lag model to a spatial error model. Here you use the spatialreg::errorsarlm() function. model.sem &lt;- spatialreg::errorsarlm(formula = f, data = CC.sf, listw = wts) summary(model.sem) ## ## Call:spatialreg::errorsarlm(formula = f, data = CC.sf, listw = wts) ## ## Residuals: ## Min 1Q Median 3Q Max ## -34.65998 -6.16943 -0.70623 7.75392 23.43878 ## ## Type: error ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 60.279469 5.365594 11.2344 &lt; 2.2e-16 ## INC -0.957305 0.334231 -2.8642 0.0041806 ## HOVAL -0.304559 0.092047 -3.3087 0.0009372 ## ## Lambda: 0.54675, LR test value: 7.2556, p-value: 0.0070679 ## Asymptotic standard error: 0.13805 ## z-value: 3.9605, p-value: 7.4786e-05 ## Wald statistic: 15.686, p-value: 7.4786e-05 ## ## Log likelihood: -183.7494 for error model ## ML residual variance (sigma squared): 97.674, (sigma: 9.883) ## Number of observations: 49 ## Number of parameters estimated: 5 ## AIC: 377.5, (AIC for lm: 382.75) You find the coefficient of spatial autocorrelation (\\(\\lambda\\)) is significant, but the log likelihood value from the model is smaller (-183.7) and the AIC value is larger (377.5) compared with corresponding values from the lag model. This is consistent with the Lagrange multiplier (LM) tests indicating the spatial lag model is more appropriate Also you can compare the log likelihoods from the two spatial regression models that you fit x &lt;- 2 * (logLik(model.slym) - logLik(model.sem))/49 x[1] ## [1] 0.04389617 With a value of .04 you conclude that there is good improvement of the lag model over the error model. Again, this is consistent with your decision above to use the lag model With the spatial error model the coefficients can be interpreted as marginal effects like with the OLS model If there are large differences (e.g., different signs) between the coefficient estimate from SEM and OLS, this suggests that neither model is yielding parameters estimates matching the underlying parameters of the data generating process You test whether there is a significant difference in coefficient estimates with the Hausman test under the hypothesis of no difference. spatialreg::Hausman.test(model.sem) ## ## Spatial Hausman test (asymptotic) ## ## data: NULL ## Hausman test = 5.6132, df = 3, p-value = 0.132 The \\(p\\)-value gives inconclusive evidence that the coefficients are different and that maybe the SEM is not the right way to proceed with these data The predict() method implements the predict.sarlm() function to calculate predictions from the spatial regression model. The prediction on a spatial lag Y model is decomposed into a “trend” term (explanatory variable effect) and a “signal” term (spatial smoother). The predicted fit is the sum of the trend and the signal terms when using the spatial lag model. You make predictions with the predict() method under the assumption that the mean response is known. You examine the structure of the corresponding predict object. ( predictedValues &lt;- predict(model.slym) ) ## This method assumes the response is known - see manual page ## fit trend signal ## 1 14.151553 3.689376 10.462177 ## 2 22.577864 11.466910 11.110954 ## 3 34.302562 21.851821 12.450741 ## 4 46.732511 32.065778 14.666733 ## 5 44.747335 27.617335 17.130001 ## 6 38.333111 21.136061 17.197049 ## 7 37.830286 16.778971 21.051314 ## 8 41.393775 23.826139 17.567636 ## 9 28.792040 13.151106 15.640934 ## 10 16.390116 5.667968 10.722148 ## 11 53.631524 32.525601 21.105923 ## 12 48.074429 29.765567 18.308862 ## 13 40.608933 24.482783 16.126150 ## 14 41.856029 23.729007 18.127021 ## 15 51.665885 30.455130 21.210754 ## 16 54.767238 32.599604 22.167634 ## 17 31.866732 24.208333 7.658399 ## 18 37.461969 15.795681 21.666289 ## 19 44.929428 25.269281 19.660147 ## 20 5.110404 -8.624965 13.735369 ## 21 47.617356 29.109014 18.508343 ## 22 40.412907 25.213797 15.199111 ## 23 18.640125 10.704444 7.935681 ## 24 39.747460 16.504544 23.242917 ## 25 53.116667 31.962568 21.154099 ## 26 52.303708 31.717686 20.586022 ## 27 39.228078 25.171897 14.056180 ## 28 51.354572 31.278691 20.075881 ## 29 49.767662 27.843360 21.924302 ## 30 45.589426 25.027103 20.562323 ## 31 27.465214 19.368347 8.096867 ## 32 20.869990 15.004949 5.865041 ## 33 44.697299 28.916463 15.780837 ## 34 31.720868 22.349636 9.371232 ## 35 38.985264 24.973807 14.011457 ## 36 24.222607 16.283179 7.939428 ## 37 37.811893 16.224746 21.587148 ## 38 46.388525 27.909226 18.479299 ## 39 22.524680 15.679043 6.845638 ## 40 6.730001 -2.182900 8.912900 ## 41 20.020878 11.101447 8.919431 ## 42 14.764446 6.662086 8.102360 ## 43 40.034408 24.726462 15.307946 ## 44 34.026283 18.893555 15.132728 ## 45 36.970894 23.393214 13.577680 ## 46 13.189170 6.118277 7.070893 ## 47 21.849812 14.410621 7.439191 ## 48 38.162353 26.076852 12.085501 ## 49 27.876102 16.356569 11.519533 The predicted values are in the column labeled fit. The predicted values are a sum of the trend term (\\(X\\beta\\)) and the signal term (\\(\\rho W y\\)). The signal term is called the spatial smoother As a first-order check if things are what you think they are, compare the first five predicted values with the corresponding observed values predictedValues[1:5] ## 1 2 3 4 5 ## 14.15155 22.57786 34.30256 46.73251 44.74734 CC.sf$CRIME[1:5] ## [1] 15.72598 18.80175 30.62678 32.38776 50.73151 Some predicted values are lower than the corresponding observed values and some are higher The predicted values along with the values for the trend and signal are added to the simple features data frame CC.sf$fit &lt;- as.numeric(predictedValues) CC.sf$trend &lt;- attr(predictedValues, &quot;trend&quot;) CC.sf$signal &lt;- attr(predictedValues, &quot;signal&quot;) You plot the observed versus the predicted as a scatter plot with a y = x line and a best-fit regression line ggplot(data = CC.sf, mapping = aes(x = CRIME, y = fit)) + geom_point() + geom_smooth(method = lm, se = FALSE, color = &quot;red&quot;) + geom_abline() + scale_x_continuous(limits = c(0, 70)) + scale_y_continuous(limits = c(0, 70)) + xlab(&quot;Observed Crime&quot;) + ylab(&quot;Predicted Crime&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; The components of the predictions are mapped and placed on the same plot ( g1 &lt;- ggplot() + geom_sf(data = CC.sf, aes(fill = fit)) + scale_fill_viridis_c() + ggtitle(&quot;Predicted Crime&quot;) ) ( g2 &lt;- ggplot() + geom_sf(data = CC.sf, aes(fill = trend)) + scale_fill_viridis_c() + ggtitle(&quot;Trend (Explanatory Variables)&quot;) ) ( g3 &lt;- ggplot() + geom_sf(data = CC.sf, aes(fill = signal)) + scale_fill_viridis_c() + ggtitle(&quot;Signal&quot;) ) library(patchwork) g1 + g2 + g3 The trend term and the spatial smoother have similar ranges indicating nearly equal contributions to the predictions. The largest difference between the two terms occurs in the city’s east side A map of the difference makes this clear CC.sf &lt;- CC.sf |&gt; dplyr::mutate(CovMinusSmooth = trend - signal) tmap::tm_shape(CC.sf) + tmap::tm_fill(col = &quot;CovMinusSmooth&quot;) ## Warning: Currect projection of shape CC.sf unknown. Long-lat (WGS84) is ## assumed. ## Variable(s) &quot;CovMinusSmooth&quot; contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette. How many tracts have a smaller residual with the lag model versus the OLS model? CC.sf |&gt; dplyr::mutate(residualsL = CRIME - fit, lagWins = abs(residuals(model.ols)) &gt; abs(residualsL), CovMinusSmooth = trend - signal) |&gt; sf::st_drop_geometry() |&gt; dplyr::summarize(N = sum(lagWins)) ## N ## 1 32 In 32 out of the 49 tracts the residuals from the spatial model are smaller than the residuals from the OLS model Another spatial regression option is to modify the linear model to include spatially-lagged explanatory variables. This is called the spatially-lagged X model \\[ y = X \\beta + XW \\theta + \\varepsilon \\] In this case the weights matrix is (post) multiplied by the matrix of X variables where \\(W\\) is again the weights matrix and \\(\\theta\\) is a vector of coefficients for each lagged explanatory variable Here you fit the spatially-lagged X model using the spatialreg::lmSLX() function and save the model object as model.slxm ( model.slxm &lt;- spatialreg::lmSLX(formula = f, data = CC.sf, listw = wts) ) ## ## Call: ## lm(formula = formula(paste(&quot;y ~ &quot;, paste(colnames(x)[-1], collapse = &quot;+&quot;))), ## data = as.data.frame(x), weights = weights) ## ## Coefficients: ## (Intercept) INC HOVAL lag.INC lag.HOVAL ## 74.5534 -1.0974 -0.2944 -1.3987 0.2148 With this model, beside the direct marginal effects of income and housing value on crime, you also have the spatially-lagged indirect effects The total effect of income on crime is the sum of the direct effect and indirect effect. And again, using the spatialreg::impacts() function you see this model.slxm |&gt; spatialreg::impacts(listw = wts) ## Impact measures (SlX, estimable): ## Direct Indirect Total ## INC -1.0973898 -1.398746 -2.49613551 ## HOVAL -0.2943898 0.214841 -0.07954881 You get the impact measures and their standard errors, z-values and \\(p\\)-values with the summary() method applied to the output of the impacts() function summary(spatialreg::impacts(model.slxm, listw = wts)) ## Impact measures (SlX, estimable, n-k): ## Direct Indirect Total ## INC -1.0973898 -1.398746 -2.49613551 ## HOVAL -0.2943898 0.214841 -0.07954881 ## ======================================================== ## Standard errors: ## Direct Indirect Total ## INC 0.3738313 0.5601247 0.4929713 ## HOVAL 0.1016586 0.2079212 0.2074767 ## ======================================================== ## Z-values: ## Direct Indirect Total ## INC -2.935522 -2.497204 -5.0634496 ## HOVAL -2.895867 1.033281 -0.3834108 ## ## p-values: ## Direct Indirect Total ## INC 0.0033299 0.012518 4.1174e-07 ## HOVAL 0.0037811 0.301473 0.70142 Results show that income has a significant direct and indirect effect on crime rates, but housing values only show a significant direct effect and not a significant indirect effect Again you visualize the relative significance of the effects model.slxm |&gt; broom::tidy(conf.int = TRUE) |&gt; dplyr::slice(-1) |&gt; ggplot(aes(x = estimate, y = term, xmin = conf.low, xmax = conf.high, height = 0)) + geom_point(size = 2) + geom_vline(xintercept = 0, lty = 4) + geom_errorbarh() ## Warning: The `tidy()` method for objects of class `SlX` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output. ## ## This warning is displayed once per session. Compare R squared values between the OLS model and the spatially-lagged X model summary(model.ols)$r.squared ## [1] 0.552404 summary(model.slxm)$r.squared ## [1] 0.6105076 The spatially lagged model has an R squared value that is higher than the R squared value from the linear regression Another way to find the correct spatial model is to consider both the spatial Durbin error model and the spatial Durbin model The spatial Durban error model (SDEM) is a spatial error model with a spatially-lagged X term added To fit a SDEM use the spatialreg::errorsarlm() function but include the argument etype = \"emixed\" to ensure that the spatially lagged X variables are added and the lagged intercept term is dropped when the weights style is row standardized (\"W\") ( model.sdem &lt;- spatialreg::errorsarlm(formula = f, data = CC.sf, listw = wts, etype = &quot;emixed&quot;) ) ## ## Call: ## spatialreg::errorsarlm(formula = f, data = CC.sf, listw = wts, ## etype = &quot;emixed&quot;) ## Type: error ## ## Coefficients: ## lambda (Intercept) INC HOVAL lag.INC lag.HOVAL ## 0.4035821 73.6450826 -1.0522585 -0.2781741 -1.2048761 0.1312451 ## ## Log likelihood: -181.779 The spatial Durban model (SDM) is a spatially-lagged Y model with a spatially-lagged X term added to it To fit a SDM use the lagsarlm() function but include the argument type = \"mixed\" to ensure that the spatially lagged X variables are added and the lagged intercept term is dropped when the weights style is row standardized (\"W\") ( model.sdm &lt;- spatialreg::lagsarlm(formula = f, data = CC.sf, listw = wts, type = &quot;mixed&quot;) ) ## ## Call: ## spatialreg::lagsarlm(formula = f, data = CC.sf, listw = wts, ## type = &quot;mixed&quot;) ## Type: mixed ## ## Coefficients: ## rho (Intercept) INC HOVAL lag.INC lag.HOVAL ## 0.4034626 44.3200052 -0.9199061 -0.2971294 -0.5839133 0.2576843 ## ## Log likelihood: -181.6393 How to do you choose between these two models? Is the relationship between crime and income and housing values a global or local effect? Is there any reason to think that if something happens in one tract it will spillover across the entire city? If crime happens in one tract does it influence crime across the entire city? If so, then it is a global relationship. Or should it be a more local effect? If there is more crime in one tract then maybe that influences crime in the neighboring tract but not tracts farther away. If so, then it is a local relationship If you think it is a local relationship, start with the spatial Durbin error model and look at the \\(p\\)-values on the direct and indirect effects summary(spatialreg::impacts(model.sdem, listw = wts, R = 500), zstats = TRUE) ## Impact measures (SDEM, estimable, n): ## Direct Indirect Total ## INC -1.0522585 -1.2048761 -2.257135 ## HOVAL -0.2781741 0.1312451 -0.146929 ## ======================================================== ## Standard errors: ## Direct Indirect Total ## INC 0.32127932 0.5736416 0.6326029 ## HOVAL 0.09114185 0.2072449 0.2372854 ## ======================================================== ## Z-values: ## Direct Indirect Total ## INC -3.275214 -2.100399 -3.568012 ## HOVAL -3.052101 0.633285 -0.619208 ## ## p-values: ## Direct Indirect Total ## INC 0.0010558 0.035694 0.0003597 ## HOVAL 0.0022725 0.526548 0.5357794 You see that income has a statistically significant direct and indirect effect on crime. This means that tracts with higher income have lower crime and tracts whose neighboring tracts have higher income also have lower crime On the other hand, housing values have only a statistically significant direct effect on crime. Tracts with more expensive houses have lower crime but tracts whose neighboring tracts have more expensive houses do not imply lower crime. And the total effect of housing values on crime across the city is not significant. So if housing values go up in tracts citywide, there is no statistical evidence that crime will go down (or up) Try a likelihood ratio test with the null hypothesis being that you should restrict the model spatialreg::LR.Sarlm(model.sdem, model.slxm) ## ## Likelihood ratio for spatial linear models ## ## data: ## Likelihood ratio = 4.3832, df = 1, p-value = 0.03629 ## sample estimates: ## Log likelihood of model.sdem Log likelihood of model.slxm ## -181.7790 -183.9706 The relatively small \\(p\\)-value suggests you shouldn’t restrict the spatial Durbin model to just the spatially-lagged X model although the evidence is not overwhelming More information: https://youtu.be/b3HtV2Mhmvk Video explaining the types of spatial regression models and how to implement them in R https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2420725 What Regional Scientists Need to Know About Spatial Econometrics "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
