[["tuesday-november-29-2022.html", "Tuesday November 29, 2022 Comparing interpolation methods Evaluating the accuracy of the interpolation Block cross validation", " Tuesday November 29, 2022 “The problem of nonparametric estimation consists in estimation, from the observations, of an unknown function belonging to a sufficiently large class of functions.” - A.B. Tsybakov Comparing interpolation methods Let’s start by reviewing the steps involved with statistical spatial interpolation. Here you consider a data set of monthly average near-surface air temperatures during April across the Midwest. The data set is available on my website in the file MidwestTemps.txt. Start by importing the data as a data frame. L &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/MidwestTemps.txt&quot; t.df &lt;- readr::read_table(L) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## lon = col_double(), ## lat = col_double(), ## temp = col_double() ## ) The data frame contains three columns. The first two are longitude (lon) and latitude (lat) and the third is average air temperatures (temp) in tens of °F. These are the climate observations at specified locations and you want a continuous field of climate values across the domain. Convert the data frame to a simple feature data frame by specifying which columns you want as coordinates (first X then Y). t.sf &lt;- t.df |&gt; sf::st_as_sf(coords = c(&quot;lon&quot;, &quot;lat&quot;), crs = 4326) Next include the spatial coordinates as attributes (X and Y) in the simple feature data frame. They were removed as attributes by the sf::st_as_sf() function. t.sf$X &lt;- t.df$lon t.sf$Y &lt;- t.df$lat Check to see if there are duplicated coordinates. t.sf$geometry |&gt; duplicated() |&gt; any() ## [1] FALSE Plot the climatological temperatures at the observation locations on a map. sts &lt;- USAboundaries::us_states() tmap::tm_shape(t.sf) + tmap::tm_text(text = &quot;temp&quot;, size = .6) + tmap::tm_shape(sts) + tmap::tm_borders() There is a clear trend in temperatures with the coolest air to the north. Besides this north-south trend, there appears to be some clustering (spatial autocorrelation) of the temperatures due to local variations. Next, compute and plot the sample variogram (omni-directional) using the residuals after removing the trend. The trend term is specified in the formula as temp ~ X + Y. library(gstat) t.v &lt;- variogram(temp ~ X + Y, data = t.sf) plot(t.v) The sample variogram values confirm spatial autocorrelation as there is an increase in the semi-variance for increasing lag distance out to about 150 km. Next, check for anisotropy. Specify four directions and compute the corresponding directional sample variograms. t.vd &lt;- variogram(temp ~ X + Y, data = t.sf, alpha = c(0, 45, 90, 135)) df &lt;- t.vd |&gt; as.data.frame() |&gt; dplyr::mutate(direction = factor(dir.hor)) library(ggplot2) ggplot(data = df, mapping = aes(x = dist, y = gamma, color = direction)) + geom_point() + geom_smooth(alpha = .2) + scale_y_continuous(limits = c(0, NA)) + ylab(expression(paste(&quot;Variogram [&quot;, gamma,&quot;(h)]&quot;))) + xlab(&quot;Lag distance (h)&quot;) + theme_minimal() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; The four sample variograms are all quite similar providing no strong evidence to reject the assumption of isotropy. Next, fit a variogram model to the variogram sample. Plot the sample variogram again to eyeball initial estimates of the model parameters. plot(t.v) Choose a nugget of .5, a partial sill of 2.5, and a range of 150. Next set the initial parameters for an exponential model then fit the model. Plot the sample variogram and the variogram model. t.vmi &lt;- vgm(model = &quot;Exp&quot;, psill = 2.5, range = 150, nugget = .5) t.vmi ## model psill range ## 1 Nug 0.5 0 ## 2 Exp 2.5 150 t.vm &lt;- fit.variogram(object = t.v, model = t.vmi) t.vm ## model psill range ## 1 Nug 0.000000 0.00000 ## 2 Exp 3.042847 73.81357 plot(t.v, t.vm) Next, make a grid of locations at which values will be interpolated. Add the coordinates as attributes to the resulting sfc. grid.sfc &lt;- sf::st_make_grid(t.sf, n = c(100, 100), what = &quot;centers&quot;) XY &lt;- grid.sfc |&gt; sf::st_coordinates() grid.sf &lt;- grid.sfc |&gt; sf::st_as_sf() |&gt; dplyr::mutate(X = XY[, 1], Y = XY[, 2]) Next, interpolate the observed temperatures to the grid locations using the method of universal kriging. t.int &lt;- krige(temp ~ X + Y, locations = t.sf, newdata = grid.sf, model = t.vm) ## [using universal kriging] The interpolated values at the grid locations are returned in the simple feature data frame you assigned as t.int. Take a glimpse of the the contents of the file. t.int |&gt; dplyr::glimpse() ## Rows: 10,000 ## Columns: 3 ## $ var1.pred &lt;dbl&gt; 56.69103, 56.62371, 56.64901, 56.81402, 57.02691, 57.22771, … ## $ var1.var &lt;dbl&gt; 1.2599718, 0.9075707, 0.7767522, 0.9968012, 1.3129199, 1.587… ## $ geometry &lt;POINT [°]&gt; POINT (-98.4204 38.62435), POINT (-98.3212 38.62435), … There are 10,000 rows (100 by 100 grid locations). The first column labeled var1.pred contains the interpolated temperatures. The second column contains the variance of the interpolated temperatures and the third column is the simple feature column. The trend term captures the north-south temperature gradient and the variogram captures the local spatial autocorrelation. Together they make up the interpolated values. To see this, you refit the interpolation first without the variogram model and second without the trend. First, rename the columns. t.int &lt;- t.int |&gt; dplyr::rename(uk.pred = var1.pred, uk.var = var1.var) Next, use the krige() function but do not include the model = argument. t.trend &lt;- krige(temp ~ X + Y, locations = t.sf, newdata = grid.sf) ## [ordinary or weighted least squares prediction] Add the interpolated temperature trend (located in t.trend$var1.pred) to the t.int simple feature data frame. t.int &lt;- t.int |&gt; dplyr::mutate(trend.pred = t.trend$var1.pred) Next, again use the krige() function but do not include the trend term. That is interpolate using ordinary kriging. t.ok &lt;- krige(temp ~ 1, locations = t.sf, newdata = grid.sf, model = t.vm) ## [using ordinary kriging] Again add the interpolated temperatures from ordinary kriging to the t.int simple feature data frame. t.int &lt;- t.int |&gt; dplyr::mutate(ok.pred = t.ok$var1.pred) Now we have three interpolations of the temperatures in the t.int simple feature data frame all labeled with .pred. t.int |&gt; dplyr::glimpse() ## Rows: 10,000 ## Columns: 5 ## $ uk.pred &lt;dbl&gt; 56.69103, 56.62371, 56.64901, 56.81402, 57.02691, 57.22771,… ## $ uk.var &lt;dbl&gt; 1.2599718, 0.9075707, 0.7767522, 0.9968012, 1.3129199, 1.58… ## $ geometry &lt;POINT [°]&gt; POINT (-98.4204 38.62435), POINT (-98.3212 38.62435),… ## $ trend.pred &lt;dbl&gt; 57.25042, 57.23585, 57.22128, 57.20672, 57.19215, 57.17758,… ## $ ok.pred &lt;dbl&gt; 54.96613, 55.37918, 55.66495, 55.74478, 55.73389, 55.69921,… Map the interpolations tmap::tm_shape(t.int) + tmap::tm_dots(title = &quot;°F&quot;, shape = 15, size = 2, col = c(&quot;uk.pred&quot;, &quot;trend.pred&quot;, &quot;ok.pred&quot;), n = 9, palette = &quot;OrRd&quot;) + tmap::tm_shape(sts) + tmap::tm_borders() + tmap::tm_shape(t.sf) + tmap::tm_text(&quot;temp&quot;, col = &quot;white&quot;, size = .5) + tmap::tm_layout(legend.outside = TRUE, legend.outside.position = &quot;bottom&quot;) The trend term (middle panel) captures the north-south temperature gradient and ordinary kriging (right panel) captures the local spatial autocorrelation. Together they make the universal kriging (left panel) interpolated surface. The pattern obtained with ordinary kriging is similar to that obtained using inverse distance weighting. Inverse distance weighting (IDW) is a deterministic method for interpolation. The values assigned to locations are calculated with a weighted average of the values available at the observed locations. The weights are proportional to the inverse of the distance to each location. The function krige() performs IDW when there is no trend term and no variogram model given as arguments to the function. t.idw &lt;- krige(temp ~ 1, locations = t.sf, newdata = grid.sf) ## [inverse distance weighted interpolation] The IDW interpolation is not statistical so there is no estimate of the uncertainty on the interpolated values. This shows up as NA values in the var1.pred column. t.idw |&gt; dplyr::glimpse() ## Rows: 10,000 ## Columns: 3 ## $ var1.pred &lt;dbl&gt; 55.68144, 55.98071, 56.06647, 55.89950, 55.59632, 55.30513, … ## $ var1.var &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ geometry &lt;POINT [°]&gt; POINT (-98.4204 38.62435), POINT (-98.3212 38.62435), … Put the IDW interpolated values into the t.int simple feature data frame and compare them to the universal kriging interpolated values on a map. t.int &lt;- t.int |&gt; dplyr::mutate(idw.pred = t.idw$var1.pred) tmap::tm_shape(t.int) + tmap::tm_dots(title = &quot;°F&quot;, shape = 15, size = 2, col = c(&quot;uk.pred&quot;, &quot;idw.pred&quot;), n = 9, palette = &quot;OrRd&quot;) + tmap::tm_shape(sts) + tmap::tm_borders() + tmap::tm_layout(legend.outside = TRUE) IDW tends to create more ‘bulls-eye’ patterns in the interpolations compared with universal kriging. It also tends to over smooth at the larger scales. t.int &lt;- t.int |&gt; dplyr::mutate(diff.pred = idw.pred - uk.pred) tmap::tm_shape(t.int) + tmap::tm_dots(title = &quot;°F&quot;, shape = 15, size = 2, col = &quot;diff.pred&quot;, n = 9, palette = &quot;BrBG&quot;) + tmap::tm_shape(sts) + tmap::tm_borders() ## Variable(s) &quot;diff.pred&quot; contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette. Relative to universal kriging, IDW over estimates the temperatures in the coldest regions and under estimates the temperatures in the warmest regions. At the largest scales IDW is too smooth and at the smallest scales it is too coarse. By taking into account to different models (trend at the largest scale and autocorrelation at the smallest scales) universal kriging produces a ‘goldilocks’ surface. Finally, simple kriging is ordinary kriging with a known mean value. This is done by specifying a value for the beta = argument. Here you specify the average value over all observed temperatures. krige(temp ~ 1, beta = mean(t.sf$temp), locations = t.sf, newdata = grid.sf, model = t.vm) ## [using simple kriging] ## Simple feature collection with 10000 features and 2 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -98.4204 ymin: 38.62435 xmax: -88.5996 ymax: 45.42565 ## Geodetic CRS: WGS 84 ## First 10 features: ## var1.pred var1.var geometry ## 1 55.03658 1.2219584 POINT (-98.4204 38.62435) ## 2 55.42840 0.8890350 POINT (-98.3212 38.62435) ## 3 55.70226 0.7662341 POINT (-98.222 38.62435) ## 4 55.78486 0.9850783 POINT (-98.1228 38.62435) ## 5 55.78267 1.2960863 POINT (-98.0236 38.62435) ## 6 55.75725 1.5646021 POINT (-97.9244 38.62435) ## 7 55.71768 1.7733525 POINT (-97.8252 38.62435) ## 8 55.66063 1.9329095 POINT (-97.726 38.62435) ## 9 55.58293 2.0570339 POINT (-97.6268 38.62435) ## 10 55.48552 2.1558398 POINT (-97.5276 38.62435) Evaluating the accuracy of the interpolation How do you evaluate how good the interpolated surface is? If you use the variogram model to predict at the observation locations, you will get the observed values back when the nugget is zero. For example, here you interpolate to the observation locations by setting newdata = t.sf instead of grid.sf. You then compute the correlation between the interpolated value and the observed value. t.int2 &lt;- krige(temp ~ X + Y, locations = t.sf, newdata = t.sf, model = t.vm) ## [using universal kriging] cor(t.int2$var1.pred, t.sf$temp) ## [1] 1 This is not helpful. Instead you use cross validation. Cross validation in this context is a procedure to assess how well the interpolation does at estimating the values at the observed locations when those values are not used in setting the interpolation procedure. Cross validation partitions the data into disjoint subsets and the interpolation procedure is set using one subset of the data (training set) and interpolations are made using the procedure on the other subset (testing set). Leave-one-out cross validation (LOOCV) uses all but one observation for setting the procedure and the left-out observation is used for interpolation. This process is repeated with every observation taking turns being left out. The krige.cv() function from the {gstat} package is used for cross validating the kriging procedure. Interpolations are made at the observation locations. The arguments are the same as in krige() except the nfold = argument. Values for the argument range between 2 and the number of observations (here 131). The default is 131 which is LOOCV. For example with nfold = 3 cross validation cuts the set of observations into 3rds (3 folds). Each observation gets put into one of the three folds with the interpolation procedure set using observations from the two folds and interpolations made on the remaining observation. This is repeated three times with each third taking turns being left out. xv3 &lt;- krige.cv(temp ~ X + Y, locations = t.sf, model = t.vm, nfold = 3) xv3 |&gt; head() ## Simple feature collection with 6 features and 6 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -90.74 ymin: 39.29 xmax: -89.02 ymax: 41.84 ## Geodetic CRS: WGS 84 ## var1.pred var1.var observed residual zscore fold geometry ## 1 51.61631 1.611274 51.6 -0.0163150 -0.01285293 3 POINT (-90.74 41.24) ## 2 54.83830 1.908870 55.9 1.0616962 0.76844375 2 POINT (-89.87 39.29) ## 3 53.45415 2.039303 54.7 1.2458501 0.87241859 2 POINT (-89.02 39.84) ## 4 50.73864 2.293980 51.1 0.3613630 0.23858817 1 POINT (-89.52 41.84) ## 5 51.59009 1.836865 52.2 0.6099053 0.45001172 2 POINT (-90.05 41.17) ## 6 54.17323 1.780450 55.2 1.0267654 0.76949584 2 POINT (-90.74 39.72) xv3 |&gt; tail() ## Simple feature collection with 6 features and 6 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -91.16 ymin: 43.04 xmax: -88.55 ymax: 44.97 ## Geodetic CRS: WGS 84 ## var1.pred var1.var observed residual zscore fold geometry ## 126 47.14633 1.894213 46.1 -1.0463308 -0.7602468 2 POINT (-88.55 44.04) ## 127 47.44554 2.315590 49.0 1.5544563 1.0215219 1 POINT (-89.44 43.52) ## 128 47.78241 1.966596 49.4 1.6175947 1.1534853 2 POINT (-91.16 43.04) ## 129 42.88314 2.193358 43.6 0.7168588 0.4840374 3 POINT (-90.94 44.97) ## 130 47.91986 2.265369 46.0 -1.9198561 -1.2755551 1 POINT (-90.92 43.57) ## 131 47.75269 2.591623 48.3 0.5473124 0.3399768 1 POINT (-88.74 43.19) The output is the same as before but now the data frame has a column indicating the fold (the set of observations that were left out). Using cross validation you are able to compare the interpolated value against the observed value at the observation locations. Here you use three statistics for comparisons. The correlation (r), the root-mean-squared error (rmse), and the mean absolute error (mae). These statistics are estimates of how skillful universal kriging is in producing the interpolated surface. krige.cv(temp ~ X + Y, locations = t.sf, model = t.vm, nfold = 131) |&gt; sf::st_drop_geometry() |&gt; dplyr::summarize(r = cor(var1.pred, observed), rmse = sqrt(mean((var1.pred - observed)^2)), mae = mean(abs(var1.pred - observed))) ## r rmse mae ## 1 0.9438537 1.324621 1.037809 The correlation is .944, the rmse is 1.32°F and the mae is 1.04°F. How do these skill metrics compare to interpolations from ordinary kriging. krige.cv(temp ~ 1, locations = t.sf, model = t.vm) |&gt; sf::st_drop_geometry() |&gt; dplyr::summarize(r = cor(var1.pred, observed), rmse = sqrt(mean((var1.pred - observed)^2)), mae = mean(abs(var1.pred - observed))) ## r rmse mae ## 1 0.9248236 1.597617 1.195622 With ordinary kriging the skill values are worse. The correlation is lower and the rmse and mae are larger. How do these skill metrics compare to interpolations from a trend-only interpolation. krige.cv(temp ~ X + Y, locations = t.sf) |&gt; sf::st_drop_geometry() |&gt; dplyr::summarize(r = cor(var1.pred, observed), rmse = sqrt(mean((var1.pred - observed)^2)), mae = mean(abs(var1.pred - observed))) ## r rmse mae ## 1 0.9057414 1.698922 1.351733 Even worse. The correlation is lower and the rmse and mae are higher. What about inverse-distance weighting interpolation? krige.cv(temp ~ 1, locations = t.sf) |&gt; sf::st_drop_geometry() |&gt; dplyr::summarize(r = cor(var1.pred, observed), rmse = sqrt(mean((var1.pred - observed)^2)), mae = mean(abs(var1.pred - observed))) ## r rmse mae ## 1 0.9294513 1.785536 1.346272 Better than the trend-only interpolation but not as good as universal kriging. All four interpolations result in high correlation between observed and interpolated values that exceed .9 and root-mean-squared errors (RMSE) less than 1.8. But the universal kriging interpolation gives the highest correlation and the lowest RMSE and mean-absolute errors. For a visual representation of the goodness of fit you plot the observed versus interpolated values from the cross validation procedure. krige.cv(temp ~ X + Y, locations = t.sf, model = t.vm) |&gt; dplyr::rename(interpolated = var1.pred) |&gt; ggplot(mapping = aes(x = observed, y = interpolated)) + geom_point() + geom_abline(intercept = 0, slope = 1) + geom_smooth(method = lm, color = &quot;red&quot;) + ylab(&quot;Interpolated temperatures (°F)&quot;) + xlab(&quot;Observed temperatures (°F)&quot;) + theme_minimal() ## `geom_smooth()` using formula &#39;y ~ x&#39; The black line represents a perfect prediction and the red line is the best fit line when you regress the interpolated temperatures onto the observed temperatures. The fact that the two lines nearly coincide indicates the interpolation is good. The nfold = argument, which by default is set to the number of observations and does a LOOCV, allows you to divide the data into different size folds (instead of N-1). These skill metrics are based on a fixed variogram model that uses all the observations when fitting. Thus cross validation using the krige.cv() function is a partial cross validation. With kriging the data is used in two ways (1) to fit the variogram model, and (2) to interpolate the values. To perform a full LOOCV you need to refit the variogram after removing the observation for which you want the interpolation. Here is one way to do that using a for() loop. vmi &lt;- vgm(model = &quot;Exp&quot;, psill = 2.5, range = 150, nugget = .5) int &lt;- NULL for(i in 1:nrow(t.sf)){ t &lt;- t.sf[-i, ] v &lt;- variogram(temp ~ X + Y, data = t) vm &lt;- fit.variogram(object = v, model = vmi) int[i] &lt;- krige(temp ~ X + Y, locations = t, newdata = t.sf[i, ], model = vm)$var1.pred } ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] Now compare the observed values with the interpolated values. data.frame(interpolated = int, observed = t.sf$temp) |&gt; dplyr::summarize(r = cor(interpolated, observed), rmse = sqrt(mean((interpolated - observed)^2)), mae = mean(abs(interpolated - observed))) ## r rmse mae ## 1 0.9433904 1.329906 1.041374 These values are slightly worse (r is lower, and rmse and mae are larger). This will always be the case when using full cross validation but these skill estimates represent how well the procedure will perform on a new set of independent observations. Block cross validation Unfortunately, with cross validation in the context of spatial data, the observations are not independent. As such it is better to create spatial areas for training separate from the spatial areas for testing. The function blockCV::spatialBlock() function creates spatially separated folds based on a pre-specified distance (cell size of the blocks) from raster and vector spatial data objects. It assigns blocks to the training and testing folds with random, checkerboard, or systematic patterns (default is random). The range must be specified in units of meters. The argument k = specifies the number of folds with a default value of 5. sb &lt;- t.sf |&gt; blockCV::spatialBlock(theRange = 200000) ## The best folds was in iteration 73: ## train test ## 1 103 28 ## 2 106 25 ## 3 105 26 ## 4 104 27 ## 5 106 25 ## Warning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not ## give correct results for longitude/latitude data The output shows how many observations are in the training and testing sets for each fold. The $plot list is a ggplot2 object. Render the plot then add the observation locations to see the block assignments. sb$plots + geom_sf(data = t.sf, alpha = .5) ## Warning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not ## give correct results for longitude/latitude data Now you need to repeat the full cross validation for() loop with some minor changes. First you need to add the fold and observation identification for each observation. Then rearrange the observations by these two indices. t.sf2 &lt;- t.sf |&gt; dplyr::mutate(foldID = sb$foldID, obsID = 1:nrow(t.sf)) |&gt; dplyr::arrange(foldID, obsID) t.sf2 |&gt; head() ## Simple feature collection with 6 features and 5 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -92.79 ymin: 40.59 xmax: -89.97 ymax: 41.82 ## Geodetic CRS: WGS 84 ## # A tibble: 6 × 6 ## temp geometry X Y foldID obsID ## &lt;dbl&gt; &lt;POINT [°]&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 51.6 (-90.74 41.24) -90.7 41.2 1 1 ## 2 52.2 (-90.05 41.17) -90.0 41.2 1 5 ## 3 53.1 (-90.97 40.59) -91.0 40.6 1 9 ## 4 51.7 (-90.64 40.92) -90.6 40.9 1 13 ## 5 51.9 (-89.97 41.82) -90.0 41.8 1 14 ## 6 51.5 (-92.79 41.07) -92.8 41.1 1 23 Now repeat the loop on the new t.sf2 spatial data frame. This time subset on foldID. Keep all folds not equal to i for training and then use the ith fold for interpolation. vmi &lt;- vgm(model = &quot;Exp&quot;, psill = 2.5, range = 150, nugget = .5) int &lt;- NULL for(i in 1:5){ t &lt;- t.sf2[t.sf2$foldID != i, ] v &lt;- variogram(temp ~ X + Y, data = t) vm &lt;- fit.variogram(object = v, model = vmi) int_i &lt;- krige(temp ~ X + Y, locations = t, newdata = t.sf2[t.sf2$foldID == i, ], model = vm)$var1.pred int &lt;- c(int, int_i) } ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] ## [using universal kriging] Finally create a data frame and compute the skill metrics. data.frame(interpolated = int, observed = t.sf2$temp) |&gt; dplyr::summarize(r = cor(interpolated, observed), rmse = sqrt(mean((interpolated - observed)^2)), mae = mean(abs(interpolated - observed))) ## r rmse mae ## 1 0.9074143 1.723653 1.380181 The skill metrics are worse but more representative of how well the interpolation will work with a different but similarly spatial correlated temperature field. An introduction to ‘block’ cross validation in the context of species distribution modeling is available here https://cran.r-project.org/web/packages/blockCV/vignettes/BlockCV_for_SDM.html "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
