[["index.html", "GIS5122: Applied Spatial Statistics Fall 2022 ", " GIS5122: Applied Spatial Statistics Fall 2022 James B. Elsner Date compiled: 2022-06-08 tidycensus::get_acs(geography = “county”, variables = “B01003_001”) will get you the latest 2016-2020 ACS estimates https://walker-data.com/census-r/wrangling-census-data-with-tidyverse-tools.html "],["syllabus.html", "Syllabus GIS5122: Applied Spatial Statistics Contact information Course description Expected learning outcomes Materials Class meetings Grades Ethics Outline of topics Schedule (subject to change with notice) Reference materials Reproducible research", " Syllabus GIS5122: Applied Spatial Statistics Contact information Instructor Name: Professor James B. Elsner Instructor Location: Bellamy Building, Room 323a Lesson Hours: TR 3:05-4:20 p.m. Student Hours: TR 9-10 a.m., 2-3 p.m. Email: jelsner@fsu.edu Links to my professional stuff (if you are curious) Website GitHub Twitter Course description This course is for students who want to learn how to analyze, map, and model spatial and geographical data using the R programming language. It assumes that students know basic statistics through multiple linear regression. And that students have some prior experience with using R. Students without any knowledge of R should look various online tutorials (see below). In this course you will get a survey of the methods used to describe, analyze, and model spatial data. Focus will be on applications. Emphasis is given to how spatial statistical methods are related through the concept of spatial autocorrelation. Expected learning outcomes Learn how and when to apply statistical methods and models to spatial data, learn various packages in R for analyzing and modeling spatial data, and learn how to interpret the results of a spatial data model. Materials Access to the internet and a computer Lesson and assignment files on GitHub No required textbook Many excellent online resources are available. Here are some of my favorites Data Science https://r4ds.had.co.nz/ Tidyverse https://dominicroye.github.io/en/2020/a-very-short-introduction-to-tidyverse/ Statistics https://tinystats.github.io/teacups-giraffes-and-statistics/index.html Census data https://walker-data.com/census-r/wrangling-census-data-with-tidyverse-tools.html Class meetings During each lesson I will work through and explain the R code within an xx-Lesson.Rmd file in the classroom. The notes in the lesson files are comprehensive, so you can work through them on your own if you can’t make it to class. The notes are written using the markdown language. Markdown is a way to write content for the Web. An R markdown file has the suffix .Rmd (R markdown file). The file is opened using the RStudio application. Grades You are responsible for: Reading and running the code in the lesson R markdown files (.Rmd) files. You can do this during the remote lessons as I talk and run my code or outside of class on your own Completing and returning the lab assignments on time Grades are determined only by how well you do on the assignments. based on the following standard: A: Outstanding: few, in any, errors/omissions B: Good: only minor errors/omissions C: Satisfactory: minor omissions, at least one major error/omission D: Poor: several major errors/omissions F: Fail: many major errors/omissions I’ll use the +/- grading system. Grades will be posted as they are recorded on FSU Canvas Ethics Academic honor code https://fda.fsu.edu/academic-resources/academic-integrity-and-grievances/academic-honor-policy Americans With Disabilities Act Students with disabilities needing academic accommodation should: (1) register with and provide documentation to the Student Disability Resource Center; (2) bring a letter indicating the need for accommodation and what type. This should be done during the first week of classes. Diversity and inclusiveness It is my intent to present notes and data that are respectful of diversity: gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, and culture. Outline of topics Working with data and making graphs (~ 4 lessons) Working with spatial data and making maps (~ 5 lessons) Quantifying spatial autocorrelation and spatial regression (~ 5 lessons) Analyzing and modeling point pattern data (~ 6 lessons) Estimating variograms and interpolating spatially (~ 6 lessons) Other topics (~ 2 lessons) Schedule (subject to change with notice) Week Dates Topic 1 August 23, 25 Syllabus and setup 2 August 30, September 1 Data frames 3 September 6, 8 4 September 13, 15 5 September 20, 22 6 September 27, 29 7 October 4, 6 8 October 11, 13 9 October 18, 20 10 October 25, 27 11 November 1, 3 12 November 8, 10 13 November 15, 17 14 November 29, December 1 28 dates 26 lessons NEEDS TO BE CHANGED Assignment Due Date (5 pm) 1 August 30 2 September 6 3 September 13 4 September 27 5 October 4 6 October 18 7 October 25 8 November 1 9 November 8 10 November 15 11 November 29 Reference materials Bivand, R. S., E. J. Pebesma, and V. G. Gomez-Rubio, 2013: Applied Spatial Data Analysis with R, 2nd Edition, Springer. A source for much of the material in the lesson notes. Lovelace, R. Nowosad, J. and Muenchow, J. Geocomputation with R. https://geocompr.robinlovelace.net/ A source for some of the material in the lesson notes. Healy, K., 2018: Data Visualization: A practical introduction, https://socviz.co/. This book teaches you how to really look at your data. A source for some of the early material in the lesson notes. Waller, L. A., and C. A. Gotway, 2004: Applied Spatial Statistics for Public Health Data, John Wiley &amp; Sons, Inc. (Available as an e-book in the FSU library). Good overall reference material for analyzing and modeling spatial data. Analyzing US Census Data: Methods, Maps, and Models in R https://walker-data.com/census-r/index.html Cheat Sheets: https://rstudio.com/resources/cheatsheets/ R Cookbook: How to do specific things: https://rc2e.com/ R for Geospatial Processing: https://bakaniko.github.io/FOSS4G2019_Geoprocessing_with_R_workshop/ Spatial Data Science: https://keen-swartz-3146c4.netlify.com/ Maps/graphs Inset maps: https://geocompr.github.io/post/2019/ggplot2-inset-maps/ {cartography} package in R: https://riatelab.github.io/cartography/docs/articles/cartography.html geovisualization with {mapdeck}: https://spatial.blog.ryerson.ca/2019/11/21/geovis-mapdeck-package-in-r/ 3D elevation with {rayshader}: https://www.rayshader.com/ 3D elevation to 3D printer: https://blog.hoxo-m.com/entry/2019/12/19/080000 Accelerate your plots with {ggforce}: https://rviews.rstudio.com/2019/09/19/intro-to-ggforce/ Summary statistics and ggplot: https://ggplot2tutor.com/summary_statistics/summary_statistics/ Space-time statistics Space-time Bayesian modeling package: https://cran.r-project.org/web/packages/spTimer/spTimer.pdf Working with space-time rasters: https://github.com/surfcao/geog5330/blob/master/week12/raster.Rmd Bayesian models Bayesian Linear Mixed Models: Random intercepts, slopes and missing data: https://willhipson.netlify.com/post/bayesian_mlm/bayesian_mlm/ Doing Bayesian Data Analysis in {brms} and the {tidyverse}: https://bookdown.org/ajkurz/DBDA_recoded/ Spatial models with INLA: https://becarioprecario.bitbucket.io/inla-gitbook/index.html Geospatial Health Data: Modeling and Visualization with {RINLA} and {shiny}: https://paula-moraga.github.io/book-geospatial/ Bayesian workflow: https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html#1_questioning_authority Spatial data Progress in the R ecosystem for representing and handling spatial data https://link.springer.com/article/10.1007/s10109-020-00336-0 Google earthengine: https://earthengine.google.com/ Burden of roof: Revisiting housing costs with {tidycensus}: https://austinwehrwein.com/data-visualization/housing/ The Care and Feeding of Spatial Data: https://docs.google.com/presentation/d/1BHlrSZWmw9tRWfYFVsRLNhAoX6KzhOhsnezTqL-R0sU/edit#slide=id.g6aeb55b281_0_550 Accessing remotely sensed imagery: https://twitter.com/mouthofmorrison/status/1212840820019208192/photo/1 Spatial data sets from Brazil: https://github.com/ipeaGIT/geobr Machine learning Supervised machine learning case studies: https://supervised-ml-course.netlify.com/ Machine learning for spatial prediction: https://www.youtube.com/watch?v=2pdRk4cj1P0&amp;feature=youtu.be Spatial networks Spatial Networks in R with {sf} and {tidygraph}: https://www.r-spatial.org/r/2019/09/26/spatial-networks.html Travel times/distances: https://github.com/rCarto/osrm Making network graphs in R - {ggraph} and {tidygraph} introduction https://youtu.be/geYZ83Aidq4 Transport planning/routing https://docs.ropensci.org/stplanr/index.html https://www.urbandemographics.org/post/r5r-fast-multimodal-transport-routing-in-r/ Time series forecasting https://weecology.github.io/MATSS/ Movement https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/1365-2656.13116 Bookdown Introduction: https://bookdown.org/yihui/bookdown/introduction.html Learning more: https://ropensci.org/blog/2020/04/07/bookdown-learnings/ Climate data https://cran.r-project.org/web/packages/climate/vignettes/getstarted.html https://www.ncdc.noaa.gov/teleconnections/enso/indicators/soi/data.csv USGS water data: https://waterdata.usgs.gov/blog/dataretrieval/ Reference books Anselin, L., 2005: Spatial Regression Analysis in R, Spatial Analysis Laboratory, Center for Spatially Integrated Social Science. Baddeley, A., and R. Turner, 2005: spatstat: An R Package for Analyzing Spatial Point Patterns, Journal of Statistical Software, v12. Blangiardo, M., and M. Cameletti, 2015: Spatial and Spatio-temporal Bayesian Models with R-INLA, John Wiley &amp; Sons, Inc., New York. An introduction to Bayesian models for spatial data. Cressie, N. A. C., 1993: Statistics for Spatial Data, Wiley Series in Probability and Mathematical Statistics, John Wiley &amp; Sons, Inc., New York. A mathematical treatment of spatial data analysis. Cressie, N. A. C., and C. K. Wikle, 2011: Statistics for Spatio-Temporal Data, Wiley Series in Probability and Mathematical Statistics, John Wiley &amp; Sons, Inc., New York. A mathematical treatment of space-time statistics with an emphasis on Bayesian models. Diggle, P. J., 2003: Statistical Analysis of Spatial Point Patterns, Second Edition, Arnold Publishers. An introduction to the concepts and methods of statistical analysis of spatial point patterns. Fotherhingham, A. S., C. Brunsdon, and M. Charlton, 2000: Quantitative Geography: Perspectives on Spatial Data Analysis, SAGE Publications, London. A survey of spatial data analysis from the perspective of modern geography. Haining, R., 2003: Spatial Data Analysis: Theory and Practice, Cambridge University Press. A confluence of geographic information science and applied spatial statistics. Illian, J., A. Penttinen, H. Stoyan, and D. Stoyan, 2008: Statistical Analysis and Modeling of Spatial Point Patterns, Wiley Series in Statistics in Practice, John Wiley &amp; Sons, Inc., New York. A mathematical treatment of spatial point processes. Ripley, B. D., 1981: Spatial Statistics, Wiley, New York. A reference book on spatial data analysis with emphasis on point pattern analysis. Wickham, H., 2009: ggplot2: Elegant Graphics for Data Analysis, Springer UseR! Series, Springer, New York. An introduction to the ggplot package for graphics. Recent research examples More hots Stronger tornadoes Reproducible research A scientific paper has at least two goals: announce a new result and convince readers that the result is correct. Scientific papers should describe the results and provide a clear protocol to allow repetition and extension. Analysis and modeling tools should integrate text with code to make it easier to provide a clear protocol of what was done. Such tools make doing research efficient. Changes are made with little effort. Such tools allow others to build on what you’ve done. Research achieves more faster. Collaboration is easier. Code sharing leads to greater research impact. Research impact leads to promotion &amp; tenure. Free and open source software for geospatial data has progressed at an astonishing rate. High performance spatial libraries are now widely available. However, much of it is still not easy to script. Open source Geographic Information Systems (GIS) like QGIS (see https://qgis.org) have greatly reduced the ‘barrier to entry’ but emphasis on the graphical user interface (GUI) makes reproducible research difficult. Instead here we will focus on a command line interface (CLI) to help you create reproducible work flows. You might be interested in this article: Practical reproducibility in geography and geosciences "],["tuesday-august-23-2022.html", "Tuesday, August 23, 2022 Today Install R and RStudio on your computer Download course materials Read the syllabus About RStudio Lab assignments Getting started with R Are you completely new to R?", " Tuesday, August 23, 2022 “Any fool can write code that a computer can understand. Good programmers write code that humans can understand.” — Martin Fowler Today In this lesson I tell you what this course is about. In do so I give you a bit of history on my journey into data science. I give details about lessons, assignments, and grading, and about how to get the most out of this course. I will talk a bit about why I present the course in this way. Is Milwaukee snowier than Madison? Is global warming making hurricanes stronger? Are tornadoes more likely to form over smooth terrain? Understand what this course is about, how it is structured, and what I expect from you Getting set to work with R and RStudio Install R and RStudio on your computer First get R Go to http://www.r-project.org Select the CRAN (Comprehensive R Archive Network). Scroll to a mirror site Choose the appropriate file for your computer Follow the instructions to install R Then get RStudio Go to on http://rstudio.org Download RStudio Desktop Install and open RStudio Finally (not required for success in this class), learn git with R https://happygitwithr.com/install-git.html Download course materials Navigate to &lt;[https://github.com/jelsner/ASS-2022\\\\](https://github.com/jelsner/ASS-2022\\&gt; Click on the bright green Code button Download ZIP Unzip the file on your computer Open the ASS-2022.Rproj file Read the syllabus Open the 00-Syllabus.Rmd file under the Files tab About RStudio Written in HTML (like your Web browser) Top menus File &gt; New File &gt; R Markdown Tools &gt; Global Options &gt; Appearance Upper left panel is the markdown file. This is where you put your text and code Run code chunks from this panel Output from the operations can be placed in this panel or in the Console (see the gear icon above) All the text, code, and output can be rendered to an HTML file or a PDF or Word document (see the Knit button above) Upper right panel shows what is in your current environment and the history of the commands you issued This is also where you can connect to github Lower left panel is the Console I think of this as a sandbox where you try out small bits of code. If it works and is relevant to what you want to do you move it to the markdown file This is also where output from running code will be placed Not a place for plain text Lower right panel shows your project files, the plots that get made, and all the packages associated with the project The File tab shows the files in the project. The most important one is the .Rmd. The Plot tab currently shows a blank sheet The Packages tab shows all the packages that have been downloaded from CRAN and are associated with this project Lab assignments You will do all assignments inside a Rmd file. Get the assignment Rmd file from github and rename it to yourLastName_yourFirstName.Rmd Open the Rmd file with RStudio Replace ‘Your Name’ with your name in the preamble (YAML) Answer the questions by typing appropriate code between the code-chunk delimiters Select the Knit button to generate an HTML file Fix any errors Email your completed assignment Rmd file to jelsner@fsu.edu Getting started with R Applied statistics is the analysis and modeling of data. Use the c() function to input small bits of data into R. The function combines (concatenates) items in a list together. For example, consider a set of hypothetical annual land falling hurricane counts over a ten-year period. 2 3 0 3 1 0 0 1 2 1 You save these 10 integer values in your working directory by typing them into the console as follows. The console is the lower left window. counts &lt;- c(2, 3, 0, 3, 1, 0, 0, 1, 2, 1) counts ## [1] 2 3 0 3 1 0 0 1 2 1 You assign the values to an object called counts. The assignment operator is an equal sign (&lt;- or =). Values do not print. They are assigned to an object name. They are printed by typing the object name as we did on the second line. When printed the values are prefaced with a [1]. This indicates that the object is a vector and the first entry in the vector has a value of 2 (The number immediately to the right of [1]). Use the arrow keys to retrieve previous commands. Each command is stored in the history file. The up-arrow key moves backwards through the history file. The left and right arrow keys move the cursor along the line. Then you apply functions to data stored in an object. sum(counts) ## [1] 13 length(counts) ## [1] 10 sum(counts)/length(counts) ## [1] 1.3 mean(counts) ## [1] 1.3 The function sum() totals the number of hurricanes over all ten years, length() gives the number of elements in the vector. There is one element (integer value) for each year, so the function returns a value of 10. Other functions include sort(), min(), max(), range(), diff(), and cumsum(). Try these functions on the landfall counts. What does the range() function do? What does the function diff() do? diff(counts) ## [1] 1 -3 3 -2 -1 0 1 1 -1 The hurricane count data stored in the object counts is a vector. This means that R keeps track of the order that the data were entered. There is a first element, a second element, and so on. This is good for several reasons. The vector of counts has a natural order; year 1, year 2, etc. You don’t want to mix these. You would like to be able to make changes to the data item by item instead of entering the values again. Also, vectors are math objects so that math operations can be performed on them. For example, suppose counts contain the annual landfall count from the first decade of a longer record. You want to keep track of counts over other decades. This is done here as follows. d1 &lt;- counts d2 &lt;- c(0, 5, 4, 2, 3, 0, 3, 3, 2, 1) Most functions operate on each element of the data vector at the same time. d1 + d2 ## [1] 2 8 4 5 4 0 3 4 4 2 The first year of the first decade is added from the first year of the second decade and so on. What happens if you apply the c() function to these two vectors? Try it. c(d1, d2) ## [1] 2 3 0 3 1 0 0 1 2 1 0 5 4 2 3 0 3 3 2 1 If you are interested in each year’s count as a difference from the decade mean, you type d1 - mean(d1) ## [1] 0.7 1.7 -1.3 1.7 -0.3 -1.3 -1.3 -0.3 0.7 -0.3 In this case a single number (the mean of the first decade) is subtracted from a vector. The result is from subtracting the number from each entry in the data vector. This is an example of data recycling. R repeats values from one vector so that the vector lengths match. Here the mean is repeated 10 times. Are you completely new to R? The {swirl} package contains functions to get you started with the basics of R. To install the package use the install.packages() function with the name of the package in quotes. The function downloads the package from the Comprehensive R Archive Network (CRAN). You update packages using update.packages() function. To make the functions work in your current session use the library() function with the name of the package (without quotes). This needs to be done for every session, but only once per session. install.packages(&quot;swirl&quot;) library(swirl) Type: swirl() Choose the lesson: R Programming. Work through lessons 1:8 Getting help: https://www.r-project.org/help.html "],["thursday-august-25-2022.html", "Thursday, August 25, 2022 Today Review of class expectations Data science workflow with R markdown An introduction to using R Data frames", " Thursday, August 25, 2022 “The trouble with programmers is that you can never tell what a programmer is doing until it’s too late.” — Seymour Cray Today Review of class expectations Data science workflow with R markdown An introduction to using R Data frames Review of class expectations Lesson Hours: Mon/Wed 9:05 a.m. - 9:55 a.m., Lab Hours: Fri 9:05 a.m. - 9:55 a.m., Student Hours: Mon/Wed 9:55 a.m. - 10:30 a.m. The best way to contact me is through email: jelsner@fsu.edu. This course is a survey of methods to describe, analyze, and model spatial data using R. Focus is on applications. I emphasize how spatial statistical methods are related through the concept of spatial autocorrelation. During each lesson I will work through and explain the R code within an xx-Lesson.Rmd file. The notes in the files are comprehensive, so you can work through them on your own. The notes are written using the markdown language (I will explain this a bit today). Think of this as a master class in spatial data science using R. Grades are determined by how well you do on the weekly assignments. There are many online sites dedicated to all aspects of the R programming language. A list of some of the ones related to spatial analysis and modeling are in Lesson 1. You should now be set up with R and RStudio. If not, please stay after class today and I will help you. I will spend the first several lessons teaching you how I work with R. For some of you this material might be a review. On the other hand, if this is entirely new don’t get discouraged. Most of what you will do in this class does not involve writing complex code. Today I review how to work with small bits of data using functions from the {base} packages. The {base} packages are part of the initial installation of R. They form the scaffolding for working with the code, but much of what we will do in this class will involve functions from other packages. The one exception today is that I introduce functions from the {readr} package (as part of the {tidyverse} set of packages) that simplify importing data into R. These functions are similar to the corresponding functions in the {base} package. Data science workflow with R markdown A scientific paper is advertisement for a claim about the world. The proof is the procedure that was used to obtain the result that under girds the claim. The computer code is the exact procedure. Computer code is the recipe for what was done. It is the most efficient way to communicate precisely the steps involved. Communication to others and to our future self. When you use a spreadsheet, it’s hard to explain to someone precisely what you did. Click here, then right click here, then choose menu X, etc. The words you use to describe these types of procedures are not standard. If you’ve ever made a map using GIS you know how hard it is to make another (even similar one) with a new set of data. Running code with new data is simple. Code is an efficient way to communicate because all important information is given as plain text without ambiguity. Being able to code is a key skill for most technical jobs. The person most likely to reproduce our work a few months later is us. This is especially true for graphs and figures. These often have a finished quality to them as a result of tweaking and adjustments to the details. This makes it hard to reproduce later. The goal is to do as much of this tweaking as possible with the code we write, rather than in a way that is invisible (retrospectively). Contrast editing an image in Adobe Illustrator. In data science we toggle between: Writing code: Code to get our data into R, code to look at tables and summary statistics, code to make graphs, code to compute spatial statistics, code to model and plot our results. Looking at output: Our code is a set of instructions that produces the output we want: a table, a model, or a figure. It is helpful to be able to see that output. Taking notes: We also write text about what we are doing, why we are doing it, and what our results mean. To do be efficient we write our code and our comments together in the same file. This is where R markdown comes in (files that end with .Rmd). An R markdown file is a plain text document where text (such as notes or discussion) is interspersed with pieces, or chunks, of R code. When we Knit this file the code is executed (from the top to the bottom of the file) and the results supplement or replace the code with output. The resulting file is converted into a HTML, PDF, or Word document. The text in the markdown file they has simple format instructions. For example, the following symbols are used for emphasis italics, bold, and code font. When we create a new markdown document in R Studio, it contains a sample example. Lesson notes for this class are written in text using markdown formatting as needed. Text is interspersed with code. The format for code chunks is # lines of code here Three back-ticks (on a U.S. keyboard, the character under the escape key) followed by a pair of curly braces containing the name of the language we are using. The back-ticks-and-braces part signal that code is about to begin. We write our code as needed, and then end the chunk with a new line containing three more back-ticks. We can use the Insert button above to save time. In the markdown file, the lines between the first and second set of back ticks is grayed and a few small icons are noted in the upper-right corner of the grayed area. The green triangle is used to execute the code and either post the results in the console below or in the line below. When we keep our notes in this way, we are able to see everything together, the code, the output it produces, and our commentary or clarification on it. Also we can turn it into a good-looking document with one click. This is how we will do everything in this course. For example, select the Knit button above. Finally, note the Outline button in the upper right corner of the markdown file. We can organize and navigate through the markdown file section by section based on the pound symbol (#). An introduction to using R Applied spatial statistics is the analysis and modeling of data that was collected across space. To begin you need to know about data objects. The c() function is used to create a simple data object (vector object). The function combines (concatenates) individual values into a vector. The length of the vector is the number of data values. Consider a set of annual land falling hurricane counts over a ten-year period. In the first year there were two hurricanes, the next year there were three, and so on. 2 3 0 3 1 0 0 1 2 1 You save these ten values by assigning them to an object that you call counts. The assignment operator is an equal sign (&lt;- or =). counts &lt;- c(2, 3, 0, 3, 1, 0, 0, 1, 2, 1) By clicking on the Environment tab in the upper-right panel you see that the object counts with numerical values (num) 2 3, etc below word Values. The elements of the vector object are indexed between 1 and 10 (1:10). You print the values to the console by typing the name of the data object. counts ## [1] 2 3 0 3 1 0 0 1 2 1 When printed the values are prefaced with a [1]. This indicates that the object is a vector and the first element in the vector has a value of 2 (The number immediately to the right of [1]). Note: You can assign and print by wrapping the entire line of code in parentheses. ( counts &lt;- c(2, 3, 0, 3, 1, 0, 0, 1, 2, 1) ) ## [1] 2 3 0 3 1 0 0 1 2 1 You can use the arrow keys on your keyboard to retrieve previous commands. Each command is stored in the history file (click on the History tab in the upper-right panel). The up-arrow key moves backwards through the history file. The left and right arrow keys move the cursor along the line. You apply functions to data objects. A function has a name and parentheses. Inside the parentheses are the function arguments. Many functions have only a single argument, the data object. sum(counts) ## [1] 13 length(counts) ## [1] 10 sum(counts)/length(counts) ## [1] 1.3 mean(counts) ## [1] 1.3 The function sum() totals the hurricane counts over all years, length() returns the number of elements in the vector. Other functions include sort(), min(), max(), range(), diff(), and cumsum(). The object counts that you create is a vector in the sense that the elements are ordered. There is a first element, a second element, and so on. This is good for several reasons. The hurricane counts have a chronological order: year 1, year 2, etc and you want that ordered reflected in the data object. Also, you would like to be able to make changes to the data values by element. Also, vectors are math objects so that math operations can be performed on them in a natural way. For example, math tells us that a scalar multiplied by a vector is a vector where each element of the product has been multiplied by the scalar. The asterisk * is used for multiplication. 10 * counts ## [1] 20 30 0 30 10 0 0 10 20 10 Further, suppose counts contain the annual landfall count from the first decade of a longer record. You want to keep track of counts over other decades. d1 &lt;- counts d2 &lt;- c(0, 5, 4, 2, 3, 0, 3, 3, 2, 1) Most functions operate on each element of the data vector all at once. d1 + d2 ## [1] 2 8 4 5 4 0 3 4 4 2 The first year of the first decade is added to the first year of the second decade and so on. What happens if you apply the c() function to these two vectors? c(d1, d2) ## [1] 2 3 0 3 1 0 0 1 2 1 0 5 4 2 3 0 3 3 2 1 You get a vector with elements from both d1 and d2 in the order of first the first decade counts and then the second decade counts. If you are interested in each year’s count as a difference from the average number over the decade you type d1 - mean(d1) ## [1] 0.7 1.7 -1.3 1.7 -0.3 -1.3 -1.3 -0.3 0.7 -0.3 In this case a single number (the average of the first decade) is subtracted from each element of the vector. Suppose you are interested in the inter annual variability in the set of landfall counts. The variance is computed as \\[ \\hbox{var}(x) = \\frac{(x_1 - \\bar x)^2 + (x_2 - \\bar x)^2 + \\cdots + (x_n - \\bar x)^2}{n-1} = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar x)^2 \\] Although the var() function computes this, here you see how to do this using simple functions. The key is to find the squared differences and then sum. x &lt;- d1 xbar &lt;- mean(x) x - xbar ## [1] 0.7 1.7 -1.3 1.7 -0.3 -1.3 -1.3 -0.3 0.7 -0.3 (x - xbar)^2 ## [1] 0.49 2.89 1.69 2.89 0.09 1.69 1.69 0.09 0.49 0.09 sum((x - xbar)^2) ## [1] 12.1 n &lt;- length(x) n ## [1] 10 sum((x - xbar)^2)/(n - 1) ## [1] 1.344444 var(x) ## [1] 1.344444 Elements in a vector object must all have the same type. This type can be numeric, as in counts, character strings, as in simpsons &lt;- c(&#39;Homer&#39;, &#39;Marge&#39;, &#39;Bart&#39;, &#39;Lisa&#39;, &#39;Maggie&#39;) simpsons ## [1] &quot;Homer&quot; &quot;Marge&quot; &quot;Bart&quot; &quot;Lisa&quot; &quot;Maggie&quot; Character strings are made with matching quotes, either double, \", or single, '. If you mix types the values will be coerced into a common type, which is usually a character string. Arithmetic operations do not work on character strings. Returning to the land falling hurricane counts. Now suppose the National Hurricane Center (NHC) reanalyzes a storm, and that the 6th year of the 2nd decade is a 1 rather than a 0 for the number of landfalls. In this case you change the sixth element to have the value 1. d2[6] &lt;- 1 You assign to the 6th year of the decade a value of one. The square brackets [] are used to reference elements of the data vector. It is important to keep this straight: Parentheses () are used by functions and square brackets [] are used by data objects. d2 ## [1] 0 5 4 2 3 1 3 3 2 1 d2[2] ## [1] 5 d2[-4] ## [1] 0 5 4 3 1 3 3 2 1 d2[c(1, 3, 5, 7, 9)] ## [1] 0 4 3 3 2 The first line prints all the elements of the vector df2. The second prints only the 2nd value of the vector. The third prints all but the 4th value. The fourth prints the values with odd element numbers. To create structured data, for example the integers 1 through 99 you can use the : operator. 1:99 rev(1:99) 99:1 The seq() function is more general. You specify the sequence interval with the by = or length = arguments. seq(from = 1, to = 9, by = 2) ## [1] 1 3 5 7 9 seq(from = 1, to = 10, by = 2) ## [1] 1 3 5 7 9 seq(from = 1, to = 9, length = 5) ## [1] 1 3 5 7 9 The rep() function is used to create repetitive sequences. The first argument is a value or vector that we want repeated and the second argument is the number of times you want it repeated. rep(1, times = 10) ## [1] 1 1 1 1 1 1 1 1 1 1 rep(simpsons, times = 2) ## [1] &quot;Homer&quot; &quot;Marge&quot; &quot;Bart&quot; &quot;Lisa&quot; &quot;Maggie&quot; &quot;Homer&quot; &quot;Marge&quot; &quot;Bart&quot; ## [9] &quot;Lisa&quot; &quot;Maggie&quot; In the second example the vector simpsons containing the Simpson characters is repeated twice. To repeat each element of the vector use the each = argument. rep(simpsons, each = 2) ## [1] &quot;Homer&quot; &quot;Homer&quot; &quot;Marge&quot; &quot;Marge&quot; &quot;Bart&quot; &quot;Bart&quot; &quot;Lisa&quot; &quot;Lisa&quot; ## [9] &quot;Maggie&quot; &quot;Maggie&quot; More complicated patterns can be repeated by specifying pairs of equal length vectors. In this case, each element of the first vector is repeated the corresponding number of times specified by the element in the second vector. rep(c(&quot;long&quot;, &quot;short&quot;), times = c(2, 3)) ## [1] &quot;long&quot; &quot;long&quot; &quot;short&quot; &quot;short&quot; &quot;short&quot; To find the maximum number of landfalls during the first decade you type max(d1) ## [1] 3 What years had the maximum? d1 == 3 ## [1] FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE Notice the double equals signs (==). This is a logical operator that tests each value in d1 to see if it is equal to 3. The 2nd and 4th values are equal to 3 so TRUEs are returned. Think of this as asking R a question. Is the value equal to 3? R answers all at once with a vector of TRUE’s and FALSE’s. What years had fewer than 2 hurricanes? d1 &lt; 2 ## [1] FALSE FALSE TRUE FALSE TRUE TRUE TRUE TRUE FALSE TRUE Now the question is how do you get the vector element corresponding to the TRUE values? That is, which years have 3 landfalls? which(d1 == 3) ## [1] 2 4 The function which.max() can be used to get the first maximum. which.max(d1) ## [1] 2 You might also want to know the total number of landfalls in each decade and the number of years in a decade without a landfall. Or how about the ratio of the mean number of landfalls over the two decades. sum(d1) ## [1] 13 sum(d2) ## [1] 24 sum(d1 == 0) ## [1] 3 sum(d2 == 0) ## [1] 1 mean(d2)/mean(d1) ## [1] 1.846154 So there are 85% more landfalls during the second decade. Is this difference statistically significant? To remove an object from the environment use the rm() function. rm(d1, d2) Data frames Spatial data frames will be used throughout this course. A spatial data frame is a data frame plus information about the spatial geometry. Let’s start with data frames. A data frame stores data in a tabular format like a spreadsheet. It is a list of vectors each with the same length. It has column names (and sometimes row names). For example, you create a data frame object df containing three vectors n, s, b each with three elements using the data.frame() function. n &lt;- c(2, 3, 5) s &lt;- c(&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;) b &lt;- c(TRUE, FALSE, TRUE) df &lt;- data.frame(n, s, b) To see that the object is indeed a data frame you use the class() function with the name of the object inside the parentheses. class(df) ## [1] &quot;data.frame&quot; The object df is of class data.frame. Note that the object name shows up in our Environment under Data and it includes a little blue arrow indicating that you can view it by clicking on the row. The data frame shows up as a table (like a spreadsheet) in the View() mode (see the command in the console below). Caution: This is not advised for large data frames. The top line of the table is called the header. Each line below the header contains a row of data, which begins with the name (or number) of the row followed by the data values. Each data element is in a cell. To retrieve a data value from a cell, you enter its row and column coordinates in that order in the single square bracket [] operator and separated by a column. Here is the cell value from the first row, second column of df. df[1, 2] ## [1] &quot;aa&quot; You can print the column names (located in the top row in the View() mode) with the names() function. names(df) ## [1] &quot;n&quot; &quot;s&quot; &quot;b&quot; The list of names is a vector of length three containing the elements “n”, “s”, and “b” in that order. You access individual columns of a data frame as vectors by appending the dollar sign ($) to the object name. For example, to print the values of the column labeled s type df$s ## [1] &quot;aa&quot; &quot;bb&quot; &quot;cc&quot; Many of the packages we will use this semester include example data frames. The data frame called mtcars, for instance, contains information extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models). class(mtcars) ## [1] &quot;data.frame&quot; names(mtcars) ## [1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; ## [11] &quot;carb&quot; The number of data rows and data columns in the data frame are printed using the nrow() and ncol() functions. nrow(mtcars) ## [1] 32 ncol(mtcars) ## [1] 11 Further details of built-in data frames like mtcars is available in the documentation accessed with the help() (or ?) function. help(mtcars) If you type the name of the data frame in the console all the data are printed. mtcars ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 Instead, to get a glimpse of our data we used the functions head(), which prints the first six rows, or str(), which lists all the columns by data type. head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... "],["tuesday-august-30-2022.html", "Tuesday August 30, 2022 Today Working with data frames Working with data frames using functions from the {dplyr} package", " Tuesday August 30, 2022 “Feeling a little uncomfortable with your skills is a sign of learning, and continuous learning is what the tech industry thrives on!” — Vanessa Hurst Today Working with data frames Working with data frames using functions from the {dplyr} package Working with data frames Consider the data frame studentdata from the {LearnBayes} package. To access this data frame, you first install the package with the install.packages() function. You put the name of the package {LearnBayes} in quotes (single or double). Then to make the functions from the package available to your current session use the library() function with the name of the package (unquoted) inside the parentheses. if(!require(LearnBayes)) install.packages(pkgs = &quot;LearnBayes&quot;, repos = &quot;http://cran.us.r-project.org&quot;) ## Loading required package: LearnBayes library(LearnBayes) Note: The argument repos = in the install.packages() function directs where the package can be obtained on CRAN (comprehensive R archive network). The CRAN repository is set automatically when using RStudio and you can install packages by clicking on Packages &gt; Install in the lower-right panel. For interactive use you need to specify the repository and when you use the Knit button you don’t want to install packages that already exist on your computer so you add the conditional if() function that says “only install the package IF it is not (!) available”. Make a copy of the data frame by assigning it to an object with the name df and print the first six rows using the head() function. df &lt;- studentdata head(df) ## Student Height Gender Shoes Number Dvds ToSleep WakeUp Haircut Job Drink ## 1 1 67 female 10 5 10 -2.5 5.5 60 30.0 water ## 2 2 64 female 20 7 5 1.5 8.0 0 20.0 pop ## 3 3 61 female 12 2 6 -1.5 7.5 48 0.0 milk ## 4 4 61 female 3 6 40 2.0 8.5 10 0.0 water ## 5 5 70 male 4 5 6 0.0 9.0 15 17.5 pop ## 6 6 63 female NA 3 5 1.0 8.5 25 0.0 water Data frames are like spreadsheets with rows and columns. The rows are the observations (here each row is a student in an intro stats class at Bowling Green State University) and the columns are the variables. Here the variables are answers to questions like what is your height, choose a number between 1 and 10, what time did you go to bed last night, etc. The names of the columns are printed using the names() function. names(df) ## [1] &quot;Student&quot; &quot;Height&quot; &quot;Gender&quot; &quot;Shoes&quot; &quot;Number&quot; &quot;Dvds&quot; &quot;ToSleep&quot; ## [8] &quot;WakeUp&quot; &quot;Haircut&quot; &quot;Job&quot; &quot;Drink&quot; All columns are of the same length, but not all students answered all questions so some of the data frame cells contain the missing-value indicator NA. Data values in a data frame are stored in rows and columns and are accessed with bracket notation [row, column] where row is the row number and column is the column number like a matrix. For example here you specify the data value in the 10th row and 2nd column (Height column) of the df data frame. df[10, 2] ## [1] 65 By specifying only the row index and leaving the column index blank you get all values in that row which corresponds to all the responses given by the 10th student. df[10, ] ## Student Height Gender Shoes Number Dvds ToSleep WakeUp Haircut Job Drink ## 10 10 65 male 10 7 22 2.5 8.5 12 0 milk Drink preference was one of the questions. Responses across all students are available in the column labeled Drink as a vector of character values. You list all the different drink preferences by typing df$Drink ## [1] water pop milk water pop water water pop water milk milk water ## [13] pop milk pop water water pop water water water water water milk ## [25] pop water water pop water water water water pop water water water ## [37] pop milk pop water water water pop milk water water water pop ## [49] pop water milk pop pop water water pop milk pop pop water ## [61] water water water water water milk pop pop pop water water water ## [73] pop water pop pop water pop pop milk water pop water water ## [85] milk pop water water pop water water water milk water pop water ## [97] pop pop pop water water pop water pop milk milk water water ## [109] water water water pop water milk milk milk water milk pop water ## [121] pop pop pop pop water water water water water water milk water ## [133] pop milk water water water water water &lt;NA&gt; pop water water pop ## [145] milk milk water water pop water water water pop water &lt;NA&gt; water ## [157] water water water water milk milk water milk water water milk water ## [169] pop pop pop water pop pop water water milk milk water water ## [181] water pop pop water water pop pop water water milk water water ## [193] milk &lt;NA&gt; water pop milk pop milk water water water water water ## [205] water pop pop water milk water milk water milk water milk water ## [217] milk water pop water water milk water water pop milk milk water ## [229] milk water pop pop pop water water milk pop milk water milk ## [241] water water pop water water water pop pop water water pop water ## [253] water milk water pop water pop milk milk pop pop water water ## [265] water pop pop milk water water water water milk milk water water ## [277] milk milk milk pop water water &lt;NA&gt; water water water pop milk ## [289] water water pop water water milk pop milk milk water water water ## [301] pop water water &lt;NA&gt; water water water water water pop water water ## [313] water water pop water water water milk milk pop water water water ## [325] water water pop pop milk milk water water pop pop pop pop ## [337] water milk water water pop milk pop water water water pop water ## [349] water water water water water &lt;NA&gt; pop pop water milk water water ## [361] milk water water pop water water water water water water pop water ## [373] water milk water water milk milk milk water water water water pop ## [385] water water pop water pop milk pop water water &lt;NA&gt; water water ## [397] water water milk water pop milk water water water water water milk ## [409] pop pop pop water pop milk water water milk milk pop water ## [421] milk water pop milk water water water water pop water pop pop ## [433] pop milk pop water milk pop water pop pop pop water water ## [445] water water water water pop milk water water water pop milk milk ## [457] pop pop water water milk water milk pop water water water water ## [469] pop water milk water water water water water milk milk water water ## [481] pop water water milk water milk water pop pop water water pop ## [493] pop pop milk water water pop water water water water pop water ## [505] pop milk water &lt;NA&gt; milk water pop water water milk water water ## [517] water water water milk water water pop water pop water milk milk ## [529] milk milk pop water pop milk &lt;NA&gt; milk pop water water pop ## [541] milk pop water milk water pop water pop water pop water water ## [553] pop milk water water water water &lt;NA&gt; water water pop pop milk ## [565] water milk pop pop water water water pop pop pop pop water ## [577] water water water water pop pop water pop water water water water ## [589] milk water water water water pop pop water water water water water ## [601] water water pop water water &lt;NA&gt; milk pop water water water pop ## [613] water pop water pop water water pop pop water pop water milk ## [625] water pop pop pop water milk pop water pop water water milk ## [637] water water water water water water water pop pop pop pop water ## [649] pop water milk water water pop pop pop water ## Levels: milk pop water Some students left that response blank and therefore the response is coded with the missing-value indicator. The variable type depends on the question asked. For example, answers given to the question of student height result in a numeric variable, answers given to the question about drink preference result in a character (or factor) variable. For integer, character, and factor variables we summarize the set of responses with the table() function. table(df$Drink) ## ## milk pop water ## 113 178 355 There are 113 students who prefer milk, 178 prefer soda, and 355 prefer water. We use the plot() method to make a draft plot of this table. plot(x = df$Drink) Notice that the sum of the responses is 646, which is less than the total number of students (657). Students who left that question blank are ignored in the table() function. To include the missing values you add the argument useNA = \"ifany\" to the table() function. table(df$Drink, useNA = &quot;ifany&quot;) ## ## milk pop water &lt;NA&gt; ## 113 178 355 11 Note: When you want code executed directly within the text you separate the code using single back ticks. This is useful when you write reports that need periodic updates when new data becomes available. Instead if you hard code the values in the text then you need to search the document for these values during each update. Suppose you are interested in examining how long students reported sleeping. This was not asked directly. You compute it from the ToSleep and WakeUp times columns. You assign the result of the difference to a column we call SleepHrs. df$SleepHrs &lt;- df$WakeUp - df$ToSleep head(df) ## Student Height Gender Shoes Number Dvds ToSleep WakeUp Haircut Job Drink ## 1 1 67 female 10 5 10 -2.5 5.5 60 30.0 water ## 2 2 64 female 20 7 5 1.5 8.0 0 20.0 pop ## 3 3 61 female 12 2 6 -1.5 7.5 48 0.0 milk ## 4 4 61 female 3 6 40 2.0 8.5 10 0.0 water ## 5 5 70 male 4 5 6 0.0 9.0 15 17.5 pop ## 6 6 63 female NA 3 5 1.0 8.5 25 0.0 water ## SleepHrs ## 1 8.0 ## 2 6.5 ## 3 9.0 ## 4 6.5 ## 5 9.0 ## 6 7.5 Now you have a new numeric variable in the data frame called SleepHrs. You can’t table numeric variables, but the summary() method prints a set of summary statistics for the set of values. summary(df$SleepHrs) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 2.500 6.500 7.500 7.385 8.500 12.500 4 The average number of hours slept is 7.4 with a maximum of 12.5 and a minimum of 2.5. There are four students that did not answer either when they went to sleep or when they woke up questions. You use the hist() function to construct a histogram of sleep hours. hist(x = df$SleepHrs) The histogram function divides the number of sleep hours into one-hour bins and counts the number of students whose reported sleep hours falls into each bin. For example based on when they said they went to sleep and when the said they woke up, about 100 students slept between five and six hours the night before the survey. Since the gender of each student is reported, you can make comparisons between those who identify as male and those who identify as female. For instance, do men sleep more than women? You can answer this question graphically with box plots using the plot() method. You specify the character variable on the horizontal axis (x) to be gender with the x = argument and the numeric variable on the vertical axis (y) with the y = argument. plot(x = df$Gender, y = df$SleepHrs) The plot reveals little difference in the amount of sleep. Repeat for hair cut prices. plot(x = df$Gender, y = df$Haircut) Big difference. Finally, is the amount of sleep for a student related to when they go to bed? If you place numeric variables on the x and y axes then you get a scatter plot. plot(x = df$ToSleep, y = df$SleepHrs) The ToSleep variable is centered on midnight so that -2 means a student went to sleep at 10p. You describe the decreasing relationship with a line through the points. The least-squares line is fit using the lm() function and the line is drawn on the existing plot with the abline() function applied to the linear regression object model. model &lt;- lm(SleepHrs ~ ToSleep, data = df) plot(x = df$ToSleep, y = df$SleepHrs) abline(model) Tornadoes Most of the time you will start by getting your data stored in a file into R. Secondary source data should be imported directly from repositories on the Web. When there is no API (application programming interface) to the repository, you need to first download the data. For example, consider the regularly updated reports of tornadoes in the United States. The data repository is the Storm Prediction Center (SPC) https://www.spc.noaa.gov/wcm/index.html#data. Here you are interested in the file called 1950-2020_actual_tornadoes.csv. First you download the file from the site with the download.file() function specifying the location (url =) and a name you want the file to be called on your computer (destfile =). download.file(url = &quot;http://www.spc.noaa.gov/wcm/data/1950-2019_actual_tornadoes.csv&quot;, destfile = here::here(&quot;data&quot;, &quot;Tornadoes.csv&quot;)) A file called Tornadoes.csv should now be located in the directory data. Click on the Files tab in the lower-right panel, then select the data folder. Next you read (import) the file as a data frame using the readr::read_csv() function from the {tidyverse} group of packages. Torn.df &lt;- readr::read_csv(file = here::here(&quot;data&quot;, &quot;Tornadoes.csv&quot;)) ## Rows: 65162 Columns: 29 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): mo, dy, st, stf ## dbl (23): om, yr, tz, stn, mag, inj, fat, loss, closs, slat, slon, elat, el... ## date (1): date ## time (1): time ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. You preview the data frame using the head() function. head(Torn.df) ## # A tibble: 6 × 29 ## om yr mo dy date time tz st stf stn mag inj ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;time&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1950 01 03 1950-01-03 11:00 3 MO 29 1 3 3 ## 2 2 1950 01 03 1950-01-03 11:55 3 IL 17 2 3 3 ## 3 3 1950 01 03 1950-01-03 16:00 3 OH 39 1 1 1 ## 4 4 1950 01 13 1950-01-13 05:25 3 AR 5 1 3 1 ## 5 5 1950 01 25 1950-01-25 19:30 3 MO 29 2 2 5 ## 6 6 1950 01 25 1950-01-25 21:00 3 IL 17 3 2 0 ## # … with 17 more variables: fat &lt;dbl&gt;, loss &lt;dbl&gt;, closs &lt;dbl&gt;, slat &lt;dbl&gt;, ## # slon &lt;dbl&gt;, elat &lt;dbl&gt;, elon &lt;dbl&gt;, len &lt;dbl&gt;, wid &lt;dbl&gt;, ns &lt;dbl&gt;, ## # sn &lt;dbl&gt;, sg &lt;dbl&gt;, f1 &lt;dbl&gt;, f2 &lt;dbl&gt;, f3 &lt;dbl&gt;, f4 &lt;dbl&gt;, fc &lt;dbl&gt; Each row is a unique tornado report. Observations for each report include the day and time, the state (st), the maximum EF rating (mag), the number of injuries (inj), the number of fatalities (fat), estimated property losses (loss), estimated crop losses (closs), start and end locations in decimal degrees longitude and latitude, length of the damage path in miles (len), width of the damage in yards (wid). The total number of tornado reports in the data set is returned using the nrow() function. nrow(Torn.df) ## [1] 65162 To create a subset of the data frame that contains only tornadoes in years (yr) since 2001, you include the logical operator yr &gt;= 2001 inside the subset operator. The logical operator is placed in front of the comma since you want all rows where the result of the operator returns a value TRUE. Torn2.df &lt;- Torn.df[Torn.df$yr &gt;= 2001, ] You see that there are fewer rows (tornado reports) in this new data frame assigned the object name Torn2.df. You subset again, keeping only tornadoes with EF ratings (mag variable) greater than zero. Here you recycle the name Torn2.df. Torn2.df &lt;- Torn2.df[Torn2.df$mag &gt; 0, ] Now you compute the correlation between EF rating (mag) and path length (len) with the cor() function. The first argument is the vector of EF ratings and the second argument is the vector of path lengths. cor(Torn2.df$mag, Torn2.df$len) ## [1] 0.4857969 Path length is recorded in miles and path width in yards and the EF damage rating variable mag is numeric. To convert path length to kilometers, path width to meters, and the EF rating to a factor and then adding these changes as new columns, type Torn2.df$Length &lt;- Torn2.df$len * 1609.34 Torn2.df$Width &lt;- Torn2.df$wid * .9144 Torn2.df$EF &lt;- factor(Torn2.df$mag) Create side-by-side box plots of path length (in kilometers) by EF rating. plot(x = Torn2.df$EF, y = Torn2.df$Length/1000) Hurricane data Here you import the data directly from the Web by specifying the URL as a character string using the file = argument. loc &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/US.txt&quot; USHur.df &lt;- readr::read_table(file = loc) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Year = col_double(), ## All = col_double(), ## MUS = col_double(), ## G = col_double(), ## FL = col_double(), ## E = col_double() ## ) The dim() function returns the size of the data frame defined as the number of rows and the number of columns. dim(USHur.df) ## [1] 166 6 There are 166 rows and 6 columns in the data frame. Each row is a year and the columns include Year, number of hurricanes (All), number of major hurricanes (MUS), number of Gulf coast hurricanes (G), number of Florida hurricanes (FL), and number of East coast hurricanes (E) in that order. To get a glimpse of the data values you list the first six lines of the data frame using the head() function. head(USHur.df) ## # A tibble: 6 × 6 ## Year All MUS G FL E ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1851 1 1 0 1 0 ## 2 1852 3 1 1 2 0 ## 3 1853 0 0 0 0 0 ## 4 1854 2 1 1 0 1 ## 5 1855 1 1 1 0 0 ## 6 1856 2 1 1 1 0 The distribution of Florida hurricane counts by year is obtained using the table() function and specifying the FL column with df$FL. table(USHur.df$FL) ## ## 0 1 2 3 4 ## 93 43 24 5 1 There are 93 years without a FL hurricane, 43 years with exactly one hurricane, 24 years with two hurricanes, and so on. Rainfall data The data are monthly statewide average rainfall (in inches) for Florida starting in 1895 from http://www.esrl.noaa.gov/psd/data/timeseries/. Note: I put values into a text editor and then uploaded the file to the Web at location http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt. To import the data you use the readr::read_table() function and assign the object the name FLp.df. You type the name of the object to see that it is a tabled data frame (tibble) with 117 rows and 13 columns. loc &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt&quot; FLp.df &lt;- readr::read_table(file = loc) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Year = col_double(), ## Jan = col_double(), ## Feb = col_double(), ## Mar = col_double(), ## Apr = col_double(), ## May = col_double(), ## Jun = col_double(), ## Jul = col_double(), ## Aug = col_double(), ## Sep = col_double(), ## Oct = col_double(), ## Nov = col_double(), ## Dec = col_double() ## ) FLp.df ## # A tibble: 117 × 13 ## Year Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1895 3.28 3.24 2.50 4.53 4.25 4.5 7.45 6.10 4.67 3.09 2.65 1.59 ## 2 1896 3.93 3.02 2.57 0.498 2.7 11.2 8.22 5.89 4.35 2.96 3.52 2.07 ## 3 1897 1.84 6 2.12 4.39 2.28 5.22 7.21 6.83 11.1 4.10 1.75 2.68 ## 4 1898 0.704 2.01 1.26 1.32 1.51 3.29 8.95 13.1 5.23 5.88 2.19 3.89 ## 5 1899 4.52 5.92 1.90 3.40 1.11 5.80 9.26 6.71 5.13 5.88 0.751 1.94 ## 6 1900 3.21 4.37 6.8 4.32 3.89 9.99 7.50 4.49 4.93 5.23 1.22 4.29 ## 7 1901 2.34 4.21 5.37 2.14 4.15 10.4 6.42 10.9 8.33 1.71 0.841 2.49 ## 8 1902 0.633 4.81 4.29 1.38 2.36 6.22 5.24 4.80 9.54 5.21 3.02 3.52 ## 9 1903 5.06 5.58 5.45 0.429 4.74 7.01 6.63 6.96 7.47 1.75 2.7 1.70 ## 10 1904 4.96 3.02 1.59 1.66 2.49 6.59 6.27 7.53 4.5 4.41 2.87 1.84 ## # … with 107 more rows The first column is the year and the next 12 columns are the months. What was the statewide rainfall during June of 1900? FLp.df$Year == 1900 ## [1] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FLp.df$Jun[FLp.df$Year == 1900] ## [1] 9.993 What year had the wettest March? FLp.df$Mar ## [1] 2.499 2.570 2.125 1.259 1.898 6.800 5.370 4.291 5.451 1.591 3.849 3.191 ## [13] 0.562 0.779 2.792 1.899 2.180 3.932 5.553 1.528 2.598 0.889 2.027 2.497 ## [25] 5.409 1.388 1.981 2.422 2.181 5.969 1.858 4.329 2.400 4.392 3.374 7.449 ## [37] 5.312 3.659 3.898 3.363 0.960 3.103 4.257 1.764 1.407 3.515 3.918 6.123 ## [49] 4.441 5.685 0.637 4.152 7.133 6.822 2.043 4.018 3.293 3.852 3.090 2.404 ## [61] 1.643 1.325 4.601 6.416 8.701 6.357 2.489 3.808 1.707 3.237 4.042 1.826 ## [73] 1.193 1.569 5.991 8.388 2.142 4.494 5.516 2.525 2.353 2.553 2.002 4.226 ## [85] 2.143 5.043 3.176 5.379 7.213 4.710 2.537 4.297 8.443 5.101 3.349 2.672 ## [97] 7.097 3.299 5.097 3.839 3.395 7.575 2.754 6.042 1.790 3.207 6.824 2.700 ## [109] 6.642 0.994 6.027 0.496 1.213 3.568 2.662 5.995 4.063 max(FLp.df$Mar) ## [1] 8.701 which.max(FLp.df$Mar) ## [1] 65 FLp.df$Year[which.max(FLp.df$Mar)] ## [1] 1959 What month during 1965 was the wettest? How wet was it? FLp.df$Year == 1965 ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FLp.df[FLp.df$Year == 1965, ] ## # A tibble: 1 × 13 ## Year Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1965 1.80 4.58 4.04 2.54 1.08 10.0 8.54 7.14 6.69 4.66 1.58 2.76 which.max(FLp.df[FLp.df$Year == 1965, 2:12]) ## Jun ## 6 which.max(FLp.df[FLp.df$Year == 1965, 2:12]) ## Jun ## 6 max(FLp.df[FLp.df$Year == 1965, 2:12]) ## [1] 10.032 Working with data frames using functions from the {dplyr} package The functions in the {dplyr} package simplify working with data frames. The functions work only on data frames. The function names are English language verbs so they are easy to remember. The verbs help you to translate your thoughts into code. We consider the verbs one at a time using the airquality data frame. The data frame contains air quality measurements taken in New York City between May and September 1973. (?airquality). dim(airquality) ## [1] 153 6 head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 The columns include Ozone (ozone concentration in ppb), Solar.R (solar radiation in langleys), Wind (wind speed in mph), Temp (air temperature in degrees F), Month, and Day. We get summary statistics on the values in each column with the summary() method. summary(airquality) ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA&#39;s :37 NA&#39;s :7 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## Note that columns that have missing values are tabulated. For example, there are 37 missing ozone measurements and 7 missing radiation measurements. Importantly for making your code more human readable you can apply the summary() function on the airquality data frame using the pipe operator (|&gt;). airquality |&gt; summary() ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA&#39;s :37 NA&#39;s :7 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## You read the pipe as THEN. “take the airquality data frame THEN summarize the columns”. The pipe operator allows you to string together functions that when read by a human makes it easy to understand what is being done. Hypothetically, suppose the object of interest is called me and there was a function called wake_up(). I could apply this function called wake_up() in two ways. wake_up(me) # way number one me |&gt; wake_up() # way number two The second way involves a bit more typing but it is easier to read (the subject comes before the predicate) and thus easier to understand. This becomes clear when stringing together functions. Continuing with the hypothetical example, what happens to the result of me after the function wake_up() has been applied? I get_out_of_bed() and then get_dressed(). Again, you can apply these functions in two ways. get_dressed(get_out_of_bed(wake_up(me))) me |&gt; wake_up() |&gt; get_out_of_bed() |&gt; get_dressed() The order of the functions usually matters to the outcome. Note that I format the code to make it easy to read. Each line is gets only one verb and each line ends with the pipe (except the last one). Continuing… me |&gt; wake_up() |&gt; get_out_of_bed() |&gt; get_dressed() |&gt; make_coffee() |&gt; drink_coffee() |&gt; leave_house() Which is much better in terms of ‘readability’ then leave_house(drink_coffee(make_coffee(get_dressed(get_out_of_bed(wake_up(me)))))). Tibbles are data frames that make life a little easier. R is an old language, and some things that were useful 10 or 20 years ago now get in your way. To make a data frame a tibble (tabular data frame) use the as_tibble() function. class(airquality) ## [1] &quot;data.frame&quot; airquality &lt;- dplyr::as_tibble(airquality) class(airquality) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Click on airquality in the environment. It is a data frame. We will use the terms ‘tibble’ and ‘data frame’ interchangeably in this class. Now you are ready to look at some of the commonly used verbs and to see how to apply them to a data frame. The function select() chooses variables by name. For example, choose the month (Month), day (Day), and temperature (Temp) columns. airquality |&gt; dplyr::select(Month, Day, Temp) ## # A tibble: 153 × 3 ## Month Day Temp ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 5 1 67 ## 2 5 2 72 ## 3 5 3 74 ## 4 5 4 62 ## 5 5 5 56 ## 6 5 6 66 ## 7 5 7 65 ## 8 5 8 59 ## 9 5 9 61 ## 10 5 10 69 ## # … with 143 more rows The result is a data frame containing only the three columns with column names listed in the select() function. Suppose you want a new data frame with only the temperature and ozone concentrations. You include an assignment operator (&lt;-) and an object name (here df). df &lt;- airquality |&gt; dplyr::select(Temp, Ozone) df ## # A tibble: 153 × 2 ## Temp Ozone ## &lt;int&gt; &lt;int&gt; ## 1 67 41 ## 2 72 36 ## 3 74 12 ## 4 62 18 ## 5 56 NA ## 6 66 28 ## 7 65 23 ## 8 59 19 ## 9 61 8 ## 10 69 NA ## # … with 143 more rows The verbs take only data frames as input and return only data frames. The function filter() chooses observations based on specific values. Suppose we want only the observations where the temperature is at or above 80 F. airquality |&gt; dplyr::filter(Temp &gt;= 80) ## # A tibble: 73 × 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 45 252 14.9 81 5 29 ## 2 NA 186 9.2 84 6 4 ## 3 NA 220 8.6 85 6 5 ## 4 29 127 9.7 82 6 7 ## 5 NA 273 6.9 87 6 8 ## 6 71 291 13.8 90 6 9 ## 7 39 323 11.5 87 6 10 ## 8 NA 259 10.9 93 6 11 ## 9 NA 250 9.2 92 6 12 ## 10 23 148 8 82 6 13 ## # … with 63 more rows The result is a data frame with the same 6 columns but now only 73 observations. Each of the observations has a temperature of at least 80 F. Suppose you want a new data frame keeping only observations when temperature is at least 80 F and when winds are less than 5 mph. df &lt;- airquality |&gt; dplyr::filter(Temp &gt;= 80 &amp; Wind &lt; 5) df ## # A tibble: 8 × 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 135 269 4.1 84 7 1 ## 2 64 175 4.6 83 7 5 ## 3 66 NA 4.6 87 8 6 ## 4 122 255 4 89 8 7 ## 5 168 238 3.4 81 8 25 ## 6 118 225 2.3 94 8 29 ## 7 73 183 2.8 93 9 3 ## 8 91 189 4.6 93 9 4 The function arrange() orders the rows by values given in a particular column. airquality |&gt; dplyr::arrange(Solar.R) ## # A tibble: 153 × 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 16 7 6.9 74 7 21 ## 2 1 8 9.7 59 5 21 ## 3 23 13 12 67 5 28 ## 4 23 14 9.2 71 9 22 ## 5 8 19 20.1 61 5 9 ## 6 14 20 16.6 63 9 25 ## 7 9 24 13.8 81 8 2 ## 8 9 24 10.9 71 9 14 ## 9 4 25 9.7 61 5 23 ## 10 13 27 10.3 76 9 18 ## # … with 143 more rows The ordering is done from the lowest value of radiation to highest value. Here you see the first 10 rows. Note Month and Day are no longer chronological. Repeat but order by the value of air temperature. airquality |&gt; dplyr::arrange(Temp) ## # A tibble: 153 × 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 NA NA 14.3 56 5 5 ## 2 6 78 18.4 57 5 18 ## 3 NA 66 16.6 57 5 25 ## 4 NA NA 8 57 5 27 ## 5 18 65 13.2 58 5 15 ## 6 NA 266 14.9 58 5 26 ## 7 19 99 13.8 59 5 8 ## 8 1 8 9.7 59 5 21 ## 9 8 19 20.1 61 5 9 ## 10 4 25 9.7 61 5 23 ## # … with 143 more rows Importantly you can string the functions together. For example select the variables radiation, wind, and temperature then filter by temperatures above 90 F and arrange by temperature. airquality |&gt; dplyr::select(Solar.R, Wind, Temp) |&gt; dplyr::filter(Temp &gt; 90) |&gt; dplyr::arrange(Temp) ## # A tibble: 14 × 3 ## Solar.R Wind Temp ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 291 14.9 91 ## 2 167 6.9 91 ## 3 250 9.2 92 ## 4 267 6.3 92 ## 5 272 5.7 92 ## 6 222 8.6 92 ## 7 197 5.1 92 ## 8 259 10.9 93 ## 9 183 2.8 93 ## 10 189 4.6 93 ## 11 225 2.3 94 ## 12 188 6.3 94 ## 13 237 6.3 96 ## 14 203 9.7 97 The result is a data frame with three columns and 14 rows arranged by increasing temperatures above 90 F. The mutate() function adds new columns to the data frame. For example, create a new column called TempC as the temperature in degrees Celsius. Also create a column called WindMS as the wind speed in meters per second. airquality |&gt; dplyr::mutate(TempC = (Temp - 32) * 5/9, WindMS = Wind * .44704) ## # A tibble: 153 × 8 ## Ozone Solar.R Wind Temp Month Day TempC WindMS ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 41 190 7.4 67 5 1 19.4 3.31 ## 2 36 118 8 72 5 2 22.2 3.58 ## 3 12 149 12.6 74 5 3 23.3 5.63 ## 4 18 313 11.5 62 5 4 16.7 5.14 ## 5 NA NA 14.3 56 5 5 13.3 6.39 ## 6 28 NA 14.9 66 5 6 18.9 6.66 ## 7 23 299 8.6 65 5 7 18.3 3.84 ## 8 19 99 13.8 59 5 8 15 6.17 ## 9 8 19 20.1 61 5 9 16.1 8.99 ## 10 NA 194 8.6 69 5 10 20.6 3.84 ## # … with 143 more rows The resulting data frame has 8 columns (two new ones) labeled TempC and WindMS. On days when the temperature is below 60 F add a column giving the apparent temperature based on the cooling effect of the wind (wind chill) and then arrange from coldest to warmest apparent temperature. airquality |&gt; dplyr::filter(Temp &lt; 60) |&gt; dplyr::mutate(TempAp = 35.74 + .6215 * Temp - 35.75 * Wind^.16 + .4275 * Temp * Wind^.16) |&gt; dplyr::arrange(TempAp) ## # A tibble: 8 × 7 ## Ozone Solar.R Wind Temp Month Day TempAp ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 NA NA 14.3 56 5 5 52.5 ## 2 6 78 18.4 57 5 18 53.0 ## 3 NA 66 16.6 57 5 25 53.3 ## 4 NA 266 14.9 58 5 26 54.9 ## 5 18 65 13.2 58 5 15 55.2 ## 6 NA NA 8 57 5 27 55.3 ## 7 19 99 13.8 59 5 8 56.4 ## 8 1 8 9.7 59 5 21 57.3 The summarize() function reduces the data frame based on a function that computes a statistic. For examples, to compute the average wind speed during July or the average temperature during June type airquality |&gt; dplyr::filter(Month == 7) |&gt; dplyr::summarize(Wavg = mean(Wind)) ## # A tibble: 1 × 1 ## Wavg ## &lt;dbl&gt; ## 1 8.94 airquality |&gt; dplyr::filter(Month == 6) |&gt; dplyr::summarize(Tavg = mean(Temp)) ## # A tibble: 1 × 1 ## Tavg ## &lt;dbl&gt; ## 1 79.1 We’ve seen functions that compute statistics including sum(), sd(), min(), max(), var(), range(), median(). Others include: Summary function Description dplyr::n() Length of the column dplyr::first() First value of the column dplyr::last() Last value of the column dplyr::n_distinct() Number of distinct values Find the maximum and median wind speed and maximum ozone concentration values during the month of May. Also determine the number of observations during May. airquality |&gt; dplyr::filter(Month == 5) |&gt; dplyr::summarize(Wmax = max(Wind), Wmed = median(Wind), OzoneMax = max(Ozone), NumDays = dplyr::n()) ## # A tibble: 1 × 4 ## Wmax Wmed OzoneMax NumDays ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 20.1 11.5 NA 31 The result gives an NA for the maximum value of ozone (OzoneMax) because there is at least one missing value in the Ozone column. You fix this with the na.rm = TRUE argument in the function max(). airquality |&gt; dplyr::filter(Month == 5) |&gt; dplyr::summarize(Wmax = max(Wind), Wmed = median(Wind), OzoneMax = max(Ozone, na.rm = TRUE), NumDays = dplyr::n()) ## # A tibble: 1 × 4 ## Wmax Wmed OzoneMax NumDays ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 20.1 11.5 115 31 If you want to summarize separately for each month you use the group_by() function. You split the data frame by some variable (e.g., Month), apply a function to the individual data frames, and then combine the output. Find the highest ozone concentration by month. Include the number of observations (days) in the month. airquality |&gt; dplyr::group_by(Month) |&gt; dplyr::summarize(OzoneMax = max(Ozone, na.rm = TRUE), NumDays = dplyr::n()) ## # A tibble: 5 × 3 ## Month OzoneMax NumDays ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 5 115 31 ## 2 6 71 30 ## 3 7 135 31 ## 4 8 168 31 ## 5 9 96 30 Find the average ozone concentration when temperatures are above and below 70 F. Include the number of observations (days) in the two groups. airquality |&gt; dplyr::group_by(Temp &gt;= 70) |&gt; dplyr::summarize(OzoneAvg = mean(Ozone, na.rm = TRUE), NumDays = dplyr::n()) ## # A tibble: 2 × 3 ## `Temp &gt;= 70` OzoneAvg NumDays ## &lt;lgl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 FALSE 18.0 32 ## 2 TRUE 49.1 121 On average ozone concentration is higher on warm days (Temp &gt;= 70 F) days. Said another way; mean ozone concentration statistically depends on temperature. The mean is a model for the data. The statistical dependency of the mean implies that a model for ozone concentration will be improved by including temperature as an explanatory variable. In summary, the important verbs are Verb Description select() selects columns; pick variables by their names filter() filters rows; pick observations by their values arrange() re-orders the rows mutate() creates new columns; create new variables with functions of existing variables summarize() summarizes values; collapse many values down to a single summary group_by() allows operations to be grouped The six functions form the basis of a grammar for data. You can only alter a data frame by reordering the rows (arrange()), picking observations and variables of interest (filter() and select()), adding new variables that are functions of existing variables (mutate()), collapsing many values to a summary (summarise()), and conditioning on variables (group_by()). The syntax of the functions are all the same: The first argument is a data frame. This argument is implicit when using the |&gt; operator. The subsequent arguments describe what to do with the data frame. You refer to columns in the data frame directly (without using $). The result is a new data frame These properties make it easy to chain together many simple lines of code to do complex data manipulations and summaries all while making it easy to read by humans. "],["thursday-september-1-2022.html", "Thursday September 1, 2022 Today Making graphs More resources and additional examples", " Thursday September 1, 2022 “Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread.” — Hadley Wickham Today Making graphs Working with data frames is part of the iterative cycle of data science, along with visualizing, and modeling. The iterative cycle of data science: Generate questions about our data. Look for answers by visualizing and modeling the data after the data are in suitably arranged data frames. Use what we learn to refine our questions and/or ask new ones. Questions are tools to guide your investigation. When you ask a question, the question focuses your attention on a specific part of the data set and helps you decide what to do. For additional practice working with data frames using functions from the {tidyverse} set of packages. See http://r4ds.had.co.nz/index.html Cheat sheets http://rstudio.com/cheatsheets Before moving on, let’s consider another data frame. The data frame contains observations on Palmer penguins and is available from https://education.rstudio.com/blog/2020/07/palmerpenguins-cran/. You import the data frame using the read_csv() function. loc &lt;- &quot;https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv&quot; penguins &lt;- readr::read_csv(file = loc) ## Rows: 344 Columns: 8 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): species, island, sex ## dbl (5): bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, year ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. penguins ## # A tibble: 344 × 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen NA NA NA NA ## 5 Adelie Torgersen 36.7 19.3 193 3450 ## 6 Adelie Torgersen 39.3 20.6 190 3650 ## 7 Adelie Torgersen 38.9 17.8 181 3625 ## 8 Adelie Torgersen 39.2 19.6 195 4675 ## 9 Adelie Torgersen 34.1 18.1 193 3475 ## 10 Adelie Torgersen 42 20.2 190 4250 ## # … with 334 more rows, and 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt; The observations are 344 individual penguins each described by species (Adelie, Chinstrap, Gentoo), where it was found (island name), length of bill (mm), depth of bill (mm), body mass (g), male or female, and year. Each penguin belongs to one of three species. To see how many of the 344 penguins are in each species you use the table() function. table(penguins$species) ## ## Adelie Chinstrap Gentoo ## 152 68 124 There are 152 Adelie, 68 Chinstrap, and 124 Gentoo penguins. To create a data frame that includes only the female penguins you type ( df &lt;- penguins |&gt; dplyr::filter(sex == &quot;female&quot;) ) ## # A tibble: 165 × 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Adelie Torgersen 39.5 17.4 186 3800 ## 2 Adelie Torgersen 40.3 18 195 3250 ## 3 Adelie Torgersen 36.7 19.3 193 3450 ## 4 Adelie Torgersen 38.9 17.8 181 3625 ## 5 Adelie Torgersen 41.1 17.6 182 3200 ## 6 Adelie Torgersen 36.6 17.8 185 3700 ## 7 Adelie Torgersen 38.7 19 195 3450 ## 8 Adelie Torgersen 34.4 18.4 184 3325 ## 9 Adelie Biscoe 37.8 18.3 174 3400 ## 10 Adelie Biscoe 35.9 19.2 189 3800 ## # … with 155 more rows, and 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt; To create a data frame that includes only penguins that are not of species Adalie you type ( df &lt;- penguins |&gt; dplyr::filter(species != &quot;Adelie&quot;) ) ## # A tibble: 192 × 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Gentoo Biscoe 46.1 13.2 211 4500 ## 2 Gentoo Biscoe 50 16.3 230 5700 ## 3 Gentoo Biscoe 48.7 14.1 210 4450 ## 4 Gentoo Biscoe 50 15.2 218 5700 ## 5 Gentoo Biscoe 47.6 14.5 215 5400 ## 6 Gentoo Biscoe 46.5 13.5 210 4550 ## 7 Gentoo Biscoe 45.4 14.6 211 4800 ## 8 Gentoo Biscoe 46.7 15.3 219 5200 ## 9 Gentoo Biscoe 43.3 13.4 209 4400 ## 10 Gentoo Biscoe 46.8 15.4 215 5150 ## # … with 182 more rows, and 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt; To create a data frame containing only penguins that weigh more than 6000 grams you type ( df &lt;- penguins |&gt; dplyr::filter(body_mass_g &gt; 6000) ) ## # A tibble: 2 × 8 ## species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Gentoo Biscoe 49.2 15.2 221 6300 male ## 2 Gentoo Biscoe 59.6 17 230 6050 male ## # … with 1 more variable: year &lt;dbl&gt; To create a data frame with female penguins that have flippers longer than 220 mm we type ( df &lt;- penguins |&gt; dplyr::filter(flipper_length_mm &gt; 220 &amp; sex == &quot;female&quot;) ) ## # A tibble: 1 × 8 ## species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Gentoo Biscoe 46.9 14.6 222 4875 fema… ## # … with 1 more variable: year &lt;dbl&gt; To create a data frame containing rows where the bill length value is NOT missing. ( df &lt;- penguins |&gt; dplyr::filter(!is.na(bill_length_mm)) ) ## # A tibble: 342 × 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen 36.7 19.3 193 3450 ## 5 Adelie Torgersen 39.3 20.6 190 3650 ## 6 Adelie Torgersen 38.9 17.8 181 3625 ## 7 Adelie Torgersen 39.2 19.6 195 4675 ## 8 Adelie Torgersen 34.1 18.1 193 3475 ## 9 Adelie Torgersen 42 20.2 190 4250 ## 10 Adelie Torgersen 37.8 17.1 186 3300 ## # … with 332 more rows, and 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt; Note that this filtering will keep rows with other column values missing values but there will be no penguins where the bill_length value is NA. Finally, to compute the average bill length for each species. penguins |&gt; dplyr::group_by(species) |&gt; dplyr::summarize(AvgBL = mean(bill_length_mm, na.rm = TRUE)) ## # A tibble: 3 × 2 ## species AvgBL ## &lt;chr&gt; &lt;dbl&gt; ## 1 Adelie 38.8 ## 2 Chinstrap 48.8 ## 3 Gentoo 47.5 Making graphs The {ggplot2} package is a popular graphics tool among data scientists (e.g., New York Times and 538). Functionality is built on principles of good data visualization. Mapping data to aesthetics Layering Building plots step by step You make the functions available to your current working directory by typing library(ggplot2) Map data to aesthetics Consider the following numeric vectors (foo, bar and zaz). Create a data frame df using the data.frame() function. foo &lt;- c(-122.419416,-121.886329,-71.05888,-74.005941,-118.243685,-117.161084,-0.127758,-77.036871, 116.407395,-122.332071,-87.629798,-79.383184,-97.743061,121.473701,72.877656,2.352222, 77.594563,-75.165222,-112.074037,37.6173) bar &lt;- c(37.77493,37.338208,42.360083,40.712784,34.052234,32.715738,51.507351,38.907192,39.904211, 47.60621,41.878114,43.653226,30.267153,31.230416,19.075984,48.856614,12.971599,39.952584, 33.448377,55.755826) zaz &lt;- c(6471,4175,3144,2106,1450,1410,842,835,758,727,688,628,626,510,497,449,419,413,325,318) df &lt;- data.frame(foo, bar, zaz) head(df) ## foo bar zaz ## 1 -122.41942 37.77493 6471 ## 2 -121.88633 37.33821 4175 ## 3 -71.05888 42.36008 3144 ## 4 -74.00594 40.71278 2106 ## 5 -118.24368 34.05223 1450 ## 6 -117.16108 32.71574 1410 To make a scatter plot you use the ggplot() function. Note that the package name is {ggplot2} but the function is ggplot() (without the 2). Inside the ggplot() function you specify the data frame with the data = argument. You also specify what columns from the data frame are to be mapped to what ‘aesthetics’ with the aes() function using the mapping = argument. The aes() function is nested inside the ggplot() function or inside a layer function. For a scatter plot the aesthetics must include the x and y coordinates at a minimum, and for this example they are in the columns labeled foo and bar respectively. Then to render the scatter plot you include the function geom_point() as a layer with the + symbol. Numeric values are specified using the arguments x = and y = in the aes() function and are rendered as points on a plot. ggplot(data = df, mapping = aes(x = foo, y = bar)) + geom_point() You map data values to aesthetic attributes. The points in the scatter plot are geometric objects that get drawn. In {ggplot2} lingo, the points are geoms. More specifically, the points are point geoms that are denoted syntactically with the function geom_point(). All geometric objects have aesthetic attributes (aesthetics): x-position y-position color size transparency You create a mapping between variables in your data frame and the aesthetic attributes of geometric objects. In the scatter plot you mapped foo to the x-position aesthetic and bar to the y-position aesthetic. This may seem trivial foo is the x-axis and bar is on the y-axis. You certainly can do that in Excel. Here there is a deeper structure. Theoretically, geometric objects (i.e., the things you draw in a plot, like points) don’t just have attributes like position. They have a color, size, etc. For example here you map a new variable to the size aesthetic. ggplot(data = df, mapping = aes(x = foo, y = bar)) + geom_point(mapping = aes(size = zaz)) You changed the scatter plot to a bubble chart by mapping a new variable to the size aesthetic. Any visualization can be deconstructed into geom specifications and a mapping from data to the aesthetic attributes of the geometric objects. Build plots in layers The principle of layering is important. To create good visualizations you often need to: Plot multiple datasets, or Plot a dataset with additional contextual information contained in a second dataset, or Plot summaries or statistical transformations over the raw data Let’s modify the bubble chart by getting additional data and plotting it as a new layer below the bubbles. First get the data from the {maps} package using the map_data() function and specifying the name of the map (here \"World\") and assigning it to a data frame with the name df2. df2 &lt;- map_data(map = &quot;world&quot;) |&gt; dplyr::glimpse() ## Rows: 99,338 ## Columns: 6 ## $ long &lt;dbl&gt; -69.89912, -69.89571, -69.94219, -70.00415, -70.06612, -70.0… ## $ lat &lt;dbl&gt; 12.45200, 12.42300, 12.43853, 12.50049, 12.54697, 12.59707, … ## $ group &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ order &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 1… ## $ region &lt;chr&gt; &quot;Aruba&quot;, &quot;Aruba&quot;, &quot;Aruba&quot;, &quot;Aruba&quot;, &quot;Aruba&quot;, &quot;Aruba&quot;, &quot;Aruba… ## $ subregion &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … Plot the new data as a new layer underneath the bubbles. ggplot(data = df, aes(x = foo, y = bar)) + geom_polygon(data = df2, mapping = aes(x = long, y = lat, group = group)) + geom_point(mapping = aes(size = zaz), color = &quot;red&quot;) This is the same bubble chart but now with a new layer added. You changed the bubble chart into a new visualization called a “dot distribution map,” which is more insightful and visually interesting. The bubble chart is a modified scatter plot and the dot distribution map is a modified bubble chart. You used two of the data visualization principles (mapping &amp; layering) to build this plot: To create the scatter plot, you mapped foo to the x-aesthetic and mapped bar to the y-aesthetic To create the bubble chart, you mapped a zaz to the size-aesthetic To create the dot distribution map, you added a layer of polygon data under the bubbles. Iteration (step by step) The third principle is about process. The graphing process begins with mapping and layering but ends with iteration when you add layers that modify scales, legends, colors, etc. The syntax of ggplot layerability enables and rewards iteration. Instead of plotting the result of the above code for making a bubble chart, assign the result to an object called p1. Coping/paste the code from above then include the assignment operator p1 &lt;-. p1 &lt;- ggplot(data = df, mapping = aes(x = foo, y = bar)) + geom_polygon(data = df2, mapping = aes(x = long, y = lat, group = group)) + geom_point(aes(size = zaz), color = &quot;red&quot;) Now modify the axes labels saving the new plot to an object called p2. ( p2 &lt;- p1 + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) ) Next modify the scale label. p2 + scale_size_continuous(name = &quot;Venture Capital Investment\\n(USD, Millions)\\n&quot;) Of course you can do this all together with p1 + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) + scale_size_continuous(name = &quot;Venture Capital Investment\\n(USD, Millions)\\n&quot;) The facet_wrap() function is a layer to iterate (repeat) the entire plot conditional on another variable. It is like the dplyr::group_by() function in the data grammar. US tornadoes Consider the tornado records in the file Tornadoes.csv. Import the data using the readr::read_csv() function then create new columns called Year, Month and EF using the dplyr::mutate() function. ( Torn.df &lt;- readr::read_csv(file = here::here(&quot;data&quot;, &quot;Tornadoes.csv&quot;)) |&gt; dplyr::mutate(Year = yr, Month = as.integer(mo), EF = mag) ) ## Rows: 65162 Columns: 29 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): mo, dy, st, stf ## dbl (23): om, yr, tz, stn, mag, inj, fat, loss, closs, slat, slon, elat, el... ## date (1): date ## time (1): time ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 65,162 × 32 ## om yr mo dy date time tz st stf stn mag inj ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;time&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1950 01 03 1950-01-03 11:00 3 MO 29 1 3 3 ## 2 2 1950 01 03 1950-01-03 11:55 3 IL 17 2 3 3 ## 3 3 1950 01 03 1950-01-03 16:00 3 OH 39 1 1 1 ## 4 4 1950 01 13 1950-01-13 05:25 3 AR 5 1 3 1 ## 5 5 1950 01 25 1950-01-25 19:30 3 MO 29 2 2 5 ## 6 6 1950 01 25 1950-01-25 21:00 3 IL 17 3 2 0 ## 7 7 1950 01 26 1950-01-26 18:00 3 TX 48 1 2 2 ## 8 8 1950 02 11 1950-02-11 13:10 3 TX 48 2 2 0 ## 9 9 1950 02 11 1950-02-11 13:50 3 TX 48 3 3 12 ## 10 10 1950 02 11 1950-02-11 21:00 3 TX 48 4 2 5 ## # … with 65,152 more rows, and 20 more variables: fat &lt;dbl&gt;, loss &lt;dbl&gt;, ## # closs &lt;dbl&gt;, slat &lt;dbl&gt;, slon &lt;dbl&gt;, elat &lt;dbl&gt;, elon &lt;dbl&gt;, len &lt;dbl&gt;, ## # wid &lt;dbl&gt;, ns &lt;dbl&gt;, sn &lt;dbl&gt;, sg &lt;dbl&gt;, f1 &lt;dbl&gt;, f2 &lt;dbl&gt;, f3 &lt;dbl&gt;, ## # f4 &lt;dbl&gt;, fc &lt;dbl&gt;, Year &lt;dbl&gt;, Month &lt;int&gt;, EF &lt;dbl&gt; Next create a data frame (df) containing the number of tornadoes by year for the state of Kansas. ( df &lt;- Torn.df |&gt; dplyr::filter(st == &quot;KS&quot;) |&gt; dplyr::group_by(Year) |&gt; dplyr::summarize(nT = dplyr::n()) ) ## # A tibble: 70 × 2 ## Year nT ## &lt;dbl&gt; &lt;int&gt; ## 1 1950 30 ## 2 1951 77 ## 3 1952 19 ## 4 1953 29 ## 5 1954 68 ## 6 1955 96 ## 7 1956 57 ## 8 1957 63 ## 9 1958 49 ## 10 1959 65 ## # … with 60 more rows Then use the functions from the {ggplot2} package to plot the number of tornadoes by year using lines to connect the values in order of the variable on the x-axis. ggplot(data = df, mapping = aes(x = Year, y = nT)) + geom_line() Note: In the early production stage of research, I like to break the code into steps as above: (1) Import the data, (2) manipulate the data, and (3) plot the data. It is easier to document but it also introduces the potential for mistakes because of the intermediary objects in the environment (e.g., Torn.df, df). Below you bring together the above code to create the time series of Kansas tornado frequency without producing intermediary objects. readr::read_csv(file = here::here(&quot;data&quot;, &quot;Tornadoes.csv&quot;)) |&gt; dplyr::mutate(Year = yr, Month = as.integer(mo), EF = mag) |&gt; dplyr::filter(st == &quot;KS&quot;) |&gt; dplyr::group_by(Year) |&gt; dplyr::summarize(nT = dplyr::n()) |&gt; ggplot(mapping = aes(x = Year, y = nT)) + geom_line() + geom_point() Recall that the group_by() function allows you to repeat an operation depending on the value (or level) of some variable. For example to count the number of tornadoes by EF damage rating since 2007 and ignoring missing ratings Torn.df |&gt; dplyr::filter(Year &gt;= 2007, EF != -9) |&gt; dplyr::group_by(EF) |&gt; dplyr::summarize(Count = dplyr::n()) ## # A tibble: 6 × 2 ## EF Count ## &lt;dbl&gt; &lt;int&gt; ## 1 0 8597 ## 2 1 5180 ## 3 2 1354 ## 4 3 354 ## 5 4 74 ## 6 5 9 The result is a table listing the number of tornadoes grouped by EF rating. Instead of printing the table, you create a bar chart using the geom_col() function. Torn.df |&gt; dplyr::filter(Year &gt;= 2007, EF != -9) |&gt; dplyr::group_by(EF) |&gt; dplyr::summarize(Count = dplyr::n()) |&gt; ggplot(mapping = aes(x = EF, y = Count)) + geom_col() The geom_bar() function counts the number of cases at each x position so you don’t need the group_by() and summarize() functions. Torn.df |&gt; dplyr::filter(Year &gt;= 2007, EF != -9) |&gt; ggplot(mapping = aes(x = EF)) + geom_bar() Improve the bar chart and to make it ready for publication. Torn.df |&gt; dplyr::filter(Year &gt;= 2007, EF != -9) |&gt; dplyr::group_by(EF) |&gt; dplyr::summarize(Count = dplyr::n()) |&gt; ggplot(mapping = aes(x = factor(EF), y = Count, fill = Count)) + geom_bar(stat = &quot;identity&quot;) + xlab(&quot;EF Rating&quot;) + ylab(&quot;Number of Tornadoes&quot;) + scale_fill_continuous(low = &#39;green&#39;, high = &#39;orange&#39;) + geom_text(aes(label = Count), vjust = -.5, size = 3) + theme_minimal() + theme(legend.position = &#39;none&#39;) You create a set of plots with the facet_wrap() function. Here you create a set of bar charts showing the frequency of tornadoes by EF rating for each year in the data set since 2004. You add the function after the geom_bar() layer and use the formula syntax (~ Year) inside the parentheses. You interpret the syntax as “plot bar charts conditioned on the variable year.” Torn.df |&gt; dplyr::filter(Year &gt;= 2004, EF != -9) |&gt; ggplot(mapping = aes(x = factor(EF))) + geom_bar() + facet_wrap(~ Year) Hot days in Tallahassee and Las Vegas The data are daily weather observations from the Tallahassee International Airport. Import the data. loc &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/TLH_Daily1940-2021.csv&quot; TLH.df &lt;- readr::read_csv(file = loc) ## Rows: 29997 Columns: 7 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): STATION, NAME ## dbl (4): TAVG, TMAX, TMIN, TOBS ## date (1): DATE ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. The variables of interest are the daily high (and low) temperature in the column labeled TMAX (TMIN). The values are in degrees F. Rename the columns then select only the date and temperature columns. TLH.df &lt;- TLH.df |&gt; dplyr::rename(TmaxF = TMAX, TminF = TMIN, Date = DATE) |&gt; dplyr::select(Date, TmaxF, TminF) |&gt; dplyr::glimpse() ## Rows: 29,997 ## Columns: 3 ## $ Date &lt;date&gt; 1940-03-01, 1940-03-02, 1940-03-03, 1940-03-04, 1940-03-05, 194… ## $ TmaxF &lt;dbl&gt; 72, 77, 73, 72, 61, 66, 72, 56, 60, 72, 72, 65, 74, 63, 56, 73, … ## $ TminF &lt;dbl&gt; 56, 53, 56, 44, 45, 40, 36, 41, 33, 32, 37, 51, 59, 49, 37, 32, … Q: Based on these data, is it getting hotter in Tallahassee? Let’s compute the annual average high temperature and create a time series graph. You use the year() function from the {lubridate} package to get a column called Year. Since the data only has values through mid May 2022 you keep all observations with the Year column value less than 2021. You then use the group_by() function to group by Year, and the summarize() function to get the average daily maximum temperature for each year. df &lt;- TLH.df |&gt; dplyr::mutate(Year = lubridate::year(Date)) |&gt; dplyr::filter(Year &lt; 2022) |&gt; dplyr::group_by(Year) |&gt; dplyr::summarize(AvgT = mean(TmaxF, na.rm = TRUE)) |&gt; dplyr::glimpse() ## Rows: 82 ## Columns: 2 ## $ Year &lt;dbl&gt; 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950,… ## $ AvgT &lt;dbl&gt; 80.45425, 78.78904, 78.95808, 79.18356, 79.24863, 79.43014, 80.30… You now have a data frame with two columns: Year and AvgT (annual average daily high temperature in degrees F). If a day is missing a value it is skipped when computing the average because of the na.rm = TRUE argument in the mean() function. Next you use functions from the {ggplot2} package to make a time series graph. You specify the x aesthetic as Year and the y aesthetic as the AvgT and include point and line layers. ggplot(data = df, mapping = aes(x = Year, y = AvgT)) + geom_point(size = 3) + geom_line() + ylab(&quot;Average Annual Temperature in Tallahassee, FL (F)&quot;) You can go directly to the graph without saving the resulting data frame. That is, you pipe |&gt; the resulting data frame after applying the {dplyr} verbs to the ggplot() function. The object in the first argument of the ggplot() function is the result (data frame) from the code above. Here you also add a smooth curve through the set of averages with the geom_smooth() layer. TLH.df |&gt; dplyr::mutate(Year = lubridate::year(Date)) |&gt; dplyr::filter(Year &lt; 2022) |&gt; dplyr::group_by(Year) |&gt; dplyr::summarize(AvgT = mean(TmaxF, na.rm = TRUE)) |&gt; ggplot(mapping = aes(x = Year, y = AvgT)) + geom_point(size = 3) + geom_line() + ylab(&quot;Average Annual Temperature in Tallahassee, FL (F)&quot;) + geom_smooth() + theme_minimal() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Q: Is the frequency of extremely hot days increasing over time? Let’s consider a daily high temperature of 100 F and above as extremely hot. Here you count the number of days at or above 100F using the summarize() function together with the sum() function on the logical operator &gt;=. If a day is missing a temperature, you remove it with the na.rm = TRUE argument in the sum() function. TLH.df |&gt; dplyr::mutate(Year = lubridate::year(Date)) |&gt; dplyr::filter(Year &lt; 2022) |&gt; dplyr::group_by(Year) |&gt; dplyr::summarize(N100 = sum(TmaxF &gt;= 100, na.rm = TRUE)) |&gt; ggplot(mapping = aes(x = Year, y = N100, fill = N100)) + geom_bar(stat = &#39;identity&#39;) + scale_fill_continuous(low = &#39;orange&#39;, high = &#39;red&#39;) + geom_text(aes(label = N100), vjust = 1.5, size = 3) + scale_x_continuous(breaks = seq(1940, 2020, 10)) + labs(title = expression(paste(&quot;Number of days in Tallahassee, Florida at or above 100&quot;, {}^o, &quot; F&quot;)), subtitle = &quot;Last official 100+ day was September 18, 2019&quot;, x = &quot;&quot;, y = &quot;&quot;) + # ylab(expression(paste(&quot;Number of days in Tallahassee, FL at or above 100&quot;, {}^o, &quot; F&quot;))) + theme_minimal() + theme(axis.text.x = element_text(size = 11), legend.position = &quot;none&quot;) What does the histogram of daily high temperatures look like? ( gTLH &lt;- ggplot(data = TLH.df, mapping = aes(x = TmaxF)) + geom_histogram(binwidth = 1, aes(fill = ..count..)) + scale_fill_continuous(low = &#39;green&#39;, high = &#39;blue&#39;) + scale_x_continuous() + scale_y_continuous() + ylab(&quot;Number of Days&quot;) + xlab(expression(paste(&quot;Daily High Temperature in Tallahassee, FL (&quot;, {}^o, &quot; F)&quot;))) + theme_minimal() + theme(legend.position = &quot;none&quot;) ) ## Warning: Removed 2 rows containing non-finite values (stat_bin). Q: The most common high temperatures are in the low 90s, but there are relatively few 100+ days. Why? Compare the histogram of daily high temperatures in Tallahassee with a histogram of daily high temperatures in Las Vegas, Nevada. Here we repeat the code above but for the data frame LVG.df. We then use the operators from the {patchwork} package to plot them side by side. LVG.df &lt;- readr::read_csv(file = &quot;http://myweb.fsu.edu/jelsner/temp/data/LV_DailySummary.csv&quot;, na = &quot;-9999&quot;) ## New names: ## * `Measurement Flag` -&gt; `Measurement Flag...8` ## * `Quality Flag` -&gt; `Quality Flag...9` ## * `Source Flag` -&gt; `Source Flag...10` ## * `Time of Observation` -&gt; `Time of Observation...11` ## * `Measurement Flag` -&gt; `Measurement Flag...13` ## * ... ## Warning: One or more parsing issues, see `problems()` for details ## Rows: 23872 Columns: 16 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (7): STATION, STATION_NAME, ELEVATION, LATITUDE, LONGITUDE, Source Flag.... ## dbl (5): DATE, TMAX, Time of Observation...11, TMIN, Time of Observation...16 ## lgl (4): Measurement Flag...8, Quality Flag...9, Measurement Flag...13, Qual... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. LVG.df &lt;- LVG.df |&gt; dplyr::mutate(TmaxF = round(9/5 * TMAX/10 + 32), TminF = round(9/5 * TMIN/10 + 32), Date = as.Date(as.character(DATE), format = &quot;%Y%m%d&quot;)) |&gt; dplyr::select(Date, TmaxF, TminF) gLVG &lt;- ggplot(data = LVG.df, mapping = aes(x = TmaxF)) + geom_histogram(binwidth = 1, aes(fill = ..count..)) + scale_fill_continuous(low = &#39;green&#39;, high = &#39;blue&#39;) + scale_x_continuous() + scale_y_continuous() + ylab(&quot;Number of Days&quot;) + xlab(expression(paste(&quot;Daily High Temperature in Las Vegas, NV (&quot;, {}^o, &quot; F)&quot;))) + theme_minimal() + theme(legend.position = &quot;none&quot;) #install.packages(&quot;patchwork&quot;) library(patchwork) gTLH / gLVG ## Warning: Removed 2 rows containing non-finite values (stat_bin). US population and area by state The object us_states from the {spData} package is a data frame from the U.S. Census Bureau. The variables include the state GEOID and NAME, the REGION (South, West, etc), AREA (in square km), and total population in 2010 (total_pop_10) and in 2015 (total_pop_15). us_states &lt;- spData::us_states class(us_states) ## [1] &quot;sf&quot; &quot;data.frame&quot; head(us_states) ## GEOID NAME REGION AREA total_pop_10 total_pop_15 ## 1 01 Alabama South 133709.27 4712651 4830620 ## 2 04 Arizona West 295281.25 6246816 6641928 ## 3 08 Colorado West 269573.06 4887061 5278906 ## 4 09 Connecticut Norteast 12976.59 3545837 3593222 ## 5 12 Florida South 151052.01 18511620 19645772 ## 6 13 Georgia South 152725.21 9468815 10006693 ## geometry ## 1 -88.20006, -88.20296, -87.42861, -86.86215, -85.60516, -85.47047, -85.30449, -85.18440, -85.12219, -85.10534, -85.00710, -84.96343, -85.00187, -84.89184, -85.05875, -85.05382, -85.14183, -85.12553, -85.05817, -85.04499, -85.09249, -85.10752, -85.03562, -85.00250, -85.89363, -86.52000, -87.59894, -87.63494, -87.53262, -87.40697, -87.44659, -87.42958, -87.51832, -87.65689, -87.75552, -87.90634, -87.90171, -87.93672, -88.00840, -88.10087, -88.10727, -88.20449, -88.33228, -88.39502, -88.43898, -88.47323, -88.40379, -88.33093, -88.21074, -88.09789, -88.20006, 34.99563, 35.00803, 35.00279, 34.99196, 34.98468, 34.32824, 33.48288, 32.86132, 32.77335, 32.64484, 32.52387, 32.42254, 32.32202, 32.26340, 32.13602, 32.01350, 31.83926, 31.69496, 31.62023, 31.51823, 31.36288, 31.18645, 31.10819, 31.00068, 30.99346, 30.99322, 30.99742, 30.86586, 30.74347, 30.67515, 30.52706, 30.40649, 30.28044, 30.24971, 30.29122, 30.40938, 30.55088, 30.65743, 30.68496, 30.50975, 30.37725, 30.36210, 30.38844, 30.36942, 31.24690, 31.89386, 32.44977, 33.07312, 34.02920, 34.89220, 34.99563 ## 2 -114.71963, -114.53909, -114.46897, -114.50613, -114.67080, -114.70790, -114.67703, -114.72287, -114.62964, -114.55890, -114.49649, -114.53368, -114.46026, -114.41591, -114.25414, -114.13828, -114.34261, -114.47162, -114.63068, -114.63349, -114.57275, -114.59593, -114.67764, -114.65341, -114.68941, -114.71211, -114.66719, -114.73116, -114.73616, -114.57203, -114.37211, -114.23880, -114.15413, -114.04684, -114.05060, -112.96647, -112.35769, -111.41278, -110.50069, -110.47019, -109.62567, -109.04522, -109.04618, -109.04602, -109.04618, -109.04666, -109.04748, -109.04829, -109.05004, -109.82969, -111.07483, -111.56019, -112.39942, -113.12596, -113.78168, -114.81361, -114.80939, -114.71963, 32.71876, 32.75695, 32.84515, 33.01701, 33.03798, 33.09743, 33.27017, 33.39878, 33.42814, 33.53182, 33.69690, 33.92607, 33.99665, 34.10764, 34.17383, 34.30323, 34.45144, 34.71297, 34.86635, 35.00186, 35.13873, 35.32523, 35.48974, 35.61079, 35.65141, 35.80618, 35.87479, 35.94392, 36.10437, 36.15161, 36.14311, 36.01456, 36.02386, 36.19407, 37.00040, 37.00022, 37.00102, 37.00148, 37.00426, 36.99800, 36.99831, 36.99908, 36.18175, 35.17551, 34.52239, 33.62506, 33.06842, 32.08911, 31.33250, 31.33407, 31.33224, 31.48814, 31.75176, 31.97278, 32.17903, 32.49428, 32.61712, 32.71876 ## 3 -109.05008, -108.25065, -107.62562, -106.21757, -105.73042, -104.85527, -104.05325, -102.86578, -102.05161, -102.05174, -102.04845, -102.04539, -102.04464, -102.04224, -103.00220, -104.33883, -105.00055, -106.20147, -106.86980, -107.42092, -108.24936, -109.04522, -109.04187, -109.04176, -109.06006, -109.05151, -109.05061, -109.05008, 41.00066, 41.00011, 41.00212, 40.99773, 40.99689, 40.99805, 41.00141, 41.00199, 41.00238, 40.00308, 39.30314, 38.81339, 38.04553, 36.99308, 37.00010, 36.99354, 36.99326, 36.99412, 36.99243, 37.00001, 36.99901, 36.99908, 37.53073, 38.16469, 38.27549, 39.12609, 39.87497, 41.00066 ## 4 -73.48731, -72.99955, -71.80065, -71.79924, -71.78699, -71.79768, -71.86051, -71.94565, -72.38663, -72.45193, -72.96205, -73.13025, -73.17777, -73.33066, -73.38723, -73.49333, -73.65734, -73.72777, -73.48271, -73.55096, -73.48731, 42.04964, 42.03865, 42.02357, 42.00807, 41.65599, 41.41671, 41.32025, 41.33780, 41.26180, 41.27889, 41.25160, 41.14680, 41.16670, 41.11000, 41.05825, 41.04817, 40.98517, 41.10070, 41.21276, 41.29542, 42.04964 ## 5 -81.81169, -81.74565, -81.44351, -81.30505, -81.25771, -81.40189, -81.51740, -81.68524, -81.81169, 24.56874, 24.65988, 24.81336, 24.75519, 24.66431, 24.62354, 24.62124, 24.55868, 24.56874, -85.00250, -84.93696, -84.91445, -84.86469, -84.05732, -83.13143, -82.21487, -82.21032, -82.17008, -82.04794, -82.01213, -82.03966, -81.94981, -81.69479, -81.44412, -81.38550, -81.25671, -81.16358, -80.96618, -80.70973, -80.57487, -80.52509, -80.58781, -80.60421, -80.56643, -80.38370, -80.25366, -80.09391, -80.03136, -80.03886, -80.10957, -80.12799, -80.24453, -80.33942, -80.32578, -80.35818, -80.46883, -80.46455, -80.66403, -80.74775, -80.81213, -80.90058, -81.07986, -81.17204, -81.11727, -81.29033, -81.35056, -81.38381, -81.46849, -81.62348, -81.68954, -81.80166, -81.83314, -81.91171, -82.01368, -82.10567, -82.13787, -82.18157, -82.06658, -82.07635, -82.17524, -82.14707, -82.24989, -82.44572, -82.56925, -82.70782, -82.58463, -82.39338, -82.41392, -82.62959, -82.58652, -82.63362, -82.73802, -82.85113, -82.82816, -82.86081, -82.76264, -82.67479, -82.65414, -82.66872, -82.73024, -82.68886, -82.76055, -82.75970, -82.82707, -82.99614, -83.16958, -83.21807, -83.40025, -83.41277, -83.53764, -83.63798, -84.02427, -84.15728, -84.26936, -84.36611, -84.33375, -84.53587, -84.82420, -84.91511, -84.99326, -85.12147, -85.04507, -85.22161, -85.31921, -85.30133, -85.40505, -85.48776, -85.58824, -85.69681, -85.87814, -86.08996, -86.29870, -86.63295, -86.90968, -87.15539, -87.29542, -87.51832, -87.42958, -87.44659, -87.40697, -87.53262, -87.63494, -87.59894, -86.52000, -85.89363, -85.00250, 31.00068, 30.88468, 30.75358, 30.71154, 30.67470, 30.62357, 30.56858, 30.42458, 30.35891, 30.36325, 30.59377, 30.74773, 30.82750, 30.74842, 30.70971, 30.27384, 29.78469, 29.55529, 29.14796, 28.75669, 28.58517, 28.45945, 28.41086, 28.25773, 28.09563, 27.74004, 27.37979, 27.01859, 26.79634, 26.56935, 26.08716, 25.77225, 25.71709, 25.49943, 25.39801, 25.15323, 25.09203, 25.20907, 25.18726, 25.14744, 25.18604, 25.13967, 25.11880, 25.22228, 25.35495, 25.68751, 25.68983, 25.77675, 25.80332, 25.89716, 25.85271, 26.08823, 26.29452, 26.42716, 26.49083, 26.48393, 26.63744, 26.68171, 26.74266, 26.95826, 26.91687, 26.78980, 26.76295, 27.06063, 27.29859, 27.48762, 27.59602, 27.83752, 27.90140, 27.99847, 27.81670, 27.71061, 27.70681, 27.88630, 28.02013, 28.21708, 28.21901, 28.44196, 28.59084, 28.69566, 28.85016, 28.90561, 28.99309, 29.05419, 29.15843, 29.17807, 29.29036, 29.42049, 29.51724, 29.66849, 29.72306, 29.88607, 30.10327, 30.07271, 30.09766, 30.00866, 29.92372, 29.91009, 29.75829, 29.78330, 29.71496, 29.71585, 29.58699, 29.67776, 29.68149, 29.79712, 29.93849, 29.96123, 30.05554, 30.09689, 30.21562, 30.30357, 30.36305, 30.39630, 30.37242, 30.32775, 30.32350, 30.28044, 30.40649, 30.52706, 30.67515, 30.74347, 30.86586, 30.99742, 30.99322, 30.99346, 31.00068 ## 6 -85.60516, -84.32187, -83.61998, -83.10861, -83.12111, -83.23908, -83.32387, -83.33869, -83.16828, -83.00292, -82.90266, -82.74666, -82.71751, -82.59615, -82.55684, -82.34693, -82.25527, -82.19658, -82.04633, -81.91353, -81.94474, -81.84730, -81.82794, -81.70463, -81.61778, -81.49142, -81.50272, -81.42161, -81.41191, -81.28132, -81.11963, -81.15600, -81.11723, -81.00674, -80.88552, -80.84055, -81.13063, -81.13349, -81.17725, -81.27880, -81.27469, -81.42047, -81.40515, -81.44693, -81.44412, -81.69479, -81.94981, -82.03966, -82.01213, -82.04794, -82.17008, -82.21032, -82.21487, -83.13143, -84.05732, -84.86469, -84.91445, -84.93696, -85.00250, -85.03562, -85.10752, -85.09249, -85.04499, -85.05817, -85.12553, -85.14183, -85.05382, -85.05875, -84.89184, -85.00187, -84.96343, -85.00710, -85.10534, -85.12219, -85.18440, -85.30449, -85.47047, -85.60516, 34.98468, 34.98841, 34.98659, 35.00066, 34.93913, 34.87566, 34.78971, 34.68200, 34.59100, 34.47213, 34.48590, 34.26641, 34.15050, 34.03052, 33.94535, 33.83430, 33.75969, 33.63058, 33.56383, 33.44127, 33.36404, 33.30678, 33.22875, 33.11645, 33.09528, 33.00808, 32.93869, 32.83518, 32.61841, 32.55646, 32.28760, 32.24148, 32.11760, 32.10115, 32.03460, 32.01131, 31.72269, 31.62335, 31.51707, 31.36721, 31.28945, 31.01670, 30.90820, 30.81039, 30.70971, 30.74842, 30.82750, 30.74773, 30.59377, 30.36325, 30.35891, 30.42458, 30.56858, 30.62357, 30.67470, 30.71154, 30.75358, 30.88468, 31.00068, 31.10819, 31.18645, 31.36288, 31.51823, 31.62023, 31.69496, 31.83926, 32.01350, 32.13602, 32.26340, 32.32202, 32.42254, 32.52387, 32.64484, 32.77335, 32.86132, 33.48288, 34.32824, 34.98468 The object us_states has two classes: simple feature and data frame. It is a data frame that has spatial information stored in the column labeled geometry. More about this next lesson. Note also that the variable AREA is numeric with units (km^2). Thus in order to perform some operations you need to specify units or convert the column using as.numeric(). For example, if you want to filter by area keeping only states with an area greater than 300,000 square km you could do the following us_states |&gt; dplyr::mutate(Area = as.numeric(AREA)) |&gt; dplyr::filter(Area &gt; 300000) For now, suppose you want to plot area versus population for each state including state names on the plot. We note large differences between the minimum and maximum values for both variables. us_states |&gt; dplyr::summarize(rA = range(AREA), rP = range(total_pop_15)) ## rA rP ## 1 178.21 579679 ## 2 687714.28 38421464 Let’s start with a simple scatter plot using logarithmic scales. The variable AREA has units so you convert it to a numeric with the as.numeric() function. ggplot(data = us_states, mapping = aes(x = as.numeric(AREA), y = total_pop_15)) + geom_point() + scale_x_log10() + scale_y_log10() Next you use the {scales} package so the tic labels can be expressed in whole numbers with commas. ggplot(data = us_states, mapping = aes(x = as.numeric(AREA), y = total_pop_15)) + geom_point() + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) Next you add text labels. You can do this with geom_text() or geom_label() ggplot(data = us_states, mapping = aes(x = as.numeric(AREA), y = total_pop_15)) + geom_point() + geom_text(aes(label = NAME)) + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) The labels are centered on top of the points. To fix this you use functions from the {grepel} package. ggplot(data = us_states, mapping = aes(x = as.numeric(AREA), y = total_pop_15)) + geom_point() + ggrepel::geom_text_repel(aes(label = NAME)) + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) ## Warning: ggrepel: 8 unlabeled data points (too many overlaps). Consider ## increasing max.overlaps Finally, since the data object is a simple feature data frame you can make a map. ggplot() + geom_sf(data = spData::us_states, mapping = aes(fill = total_pop_15)) + scale_fill_continuous(labels = scales::comma) + theme_void() More resources and additional examples ggplot extensions https://exts.ggplot2.tidyverse.org/ Cheat sheets: https://rstudio.com/resources/cheatsheets/ More examples: https://geocompr.robinlovelace.net/ {spData} package. "],["tuesday-september-6-2022.html", "Tuesday September 6, 2022 Today Working with spatial data Geo-computation on simple features", " Tuesday September 6, 2022 “An awful lot of time I spend”coding” is actually spent copying and pasting (And much of the rest is spent googling).” – Meghan Duffy Today Working with spatial data Geo-computation on simple features Working with spatial data The vector model for data (vector data) represents things in the world using points, lines and polygons. These objects have discrete, well-defined borders and a high level of precision. Of course, precision does not imply accurate. The raster model for data (raster data) represents continuous fields (like elevation and rainfall) using a grid of cells (raster). A raster aggregates fields to a given resolution, meaning that they are consistent over space and scale-able. The smallest features within the field are blurred or lost. The choice of which data model to use depends on the application: Vector data tends to dominate the social sciences because human settlements and boundaries have discrete borders. Raster data (e.g., remotely sensed imagery) tends to dominate the environmental sciences because environmental conditions are typically continuous. Geographers, ecologists, demographers use vector and raster data. Here we use functions from the {sf} package to work with vector data and functions in the {terra} and {raster} packages to work with raster data sets. We will also look at functions from the new {stars} package that work with both vector and raster data models. R’s spatial ecosystem continues to evolve. Most changes build on what has already been done. Occasionally there is a significant change that builds from scratch. The introduction of the {sf} package in about 2018 (Edzer Pebesma) is a significant change. Simple features Simple features is a standard from the Open (‘open source’) Geospatial Consortium (OGC) to represent geographic information. It condenses geographic forms into a single geometry class. The standard is used in spatial databases (e.g., PostGIS), commercial GIS (e.g., ESRI) and forms the vector data basis for libraries such as GDAL. A subset of simple features forms the GeoJSON standard. The {sf} package supports these classes and includes plotting and other methods. Functions in the {sf} package work with all common vector geometry types: points, lines, polygons and their respective ‘multi’ versions (which group together features of the same type into a single feature). These functions also support geometry collections, which contain multiple geometry types in a single object. The raster data classes are not supported. The {sf} package incorporates the three main packages of the spatial R ecosystem: {sp} for the class system, {rgdal} for reading and writing data, and {rgeos} for spatial operations done with GEOS. Simple features are data frames with a special column for storing the spatial information. The spatial column is called geometry (or geom). The geometry column is referenced like a regular column. The difference is that the geometry column is a ‘list column’ of class sfc (simple feature column). And the sfc is a set of objects of class sfg (simple feature geometries). Simple Feature Anatomy green box is a simple feature: a single record, or data.frame row, consisting of attributes and geometry blue box is a single simple feature geometry (an object of class sfg) red box a simple feature list-column (an object of class sfc, which is a column in the data.frame) the geometries are given in well-known text (WKT) format Geometries are the building blocks of simple features. Well-known text (WKT) is the way simple feature geometries are coded. Well-known binaries (WKB) are hexadecimal strings readable by computers. GIS and spatial databases use WKB to transfer and store geometry objects. WKT is a human-readable text description of simple features. The two formats are exchangeable. See: https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry In WKT format a point is a coordinate in 2D, 3D or 4D space (see vignette(\"sf1\") for more information) such as: POINT (5 2) The first number is the x coordinate and the second number is the y coordinate. A line string is a sequence of points with a straight line connecting the points, for example: LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2) Each pair of x and y coordinates is separated by a comma. A polygon is a sequence of points that form a closed, non-intersecting ring. Closed means that the first and the last point of a polygon have the same coordinates. A polygon has one exterior boundary (outer ring) but it can have interior boundaries (inner rings). An inner ring is called a ‘hole’. Polygon without a hole - POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5)) Here there are two parentheses to start and two to end the string of coordinates. Polygon with one hole - POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5), (2 4, 3 4, 3 3, 2 3, 2 4)) Here the first set of coordinates defines the outer edge of the polygon and the next set of coordinates defines the hole. The outer edge vertexes are connected in a counterclockwise direction. The inner edge vertexes (defining the hole in the polygon) are connected in a clockwise direction. Simple features allow multiple geometries (hence the term ‘geometry collection’) using ‘multi’ version of each geometry type: Multi-point - MULTIPOINT (5 2, 1 3, 3 4, 3 2) Multi-string - MULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4)) Multi-polygon - MULTIPOLYGON (((1 5, 2 2, 4 1, 4 4, 1 5), (0 2, 1 2, 1 3, 0 3, 0 2))) The difference (syntax wise) between a polygon with a hole and a multi-polygon is that the vertexes of each polygon are connected in a counterclockwise direction. A collection of these is made: Geometry collection - GEOMETRYCOLLECTION (MULTIPOINT (5 2, 1 3, 3 4, 3 2), LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2))) Simple feature geometry (sfg) The sfg class represents the simple feature geometry types: point, line string, polygon (and their ‘multi’ equivalents, such as multi points) or geometry collection. Usually you don’t need to create geometries. Geometries are typically part of the spatial data we import. However, there are a set of functions to create simple feature geometry objects (sfg) from scratch, if needed. The names of these functions are simple and consistent, as they all start with the st_ prefix and end with the name of the geometry type in lowercase letters: A point - st_point() A linestring - st_linestring() A polygon - st_polygon() A multipoint - st_multipoint() A multilinestring - st_multilinestring() A multipolygon - st_multipolygon() A geometry collection - st_geometrycollection() An sfg object can be created from three data types: A numeric vector - a single point A matrix - a set of points, where each row contains a point - a multi-point or line string A list - any other set, e.g. a multi-line string or geometry collection To create point objects, you use the st_point() function from the {sf} package applied to a numeric vector. sf::st_point(c(5, 2)) # XY point ## POINT (5 2) sf::st_point(c(5, 2, 3)) # XYZ point ## POINT Z (5 2 3) To create multi-point objects, you use matrices constructed from the rbind() function. mp.matrix &lt;- rbind(c(5, 2), c(1, 3), c(3, 4), c(3, 2)) mp.matrix ## [,1] [,2] ## [1,] 5 2 ## [2,] 1 3 ## [3,] 3 4 ## [4,] 3 2 sf::st_multipoint(mp.matrix) ## MULTIPOINT ((5 2), (1 3), (3 4), (3 2)) ls.matrix &lt;- rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)) sf::st_linestring(ls.matrix) ## LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2) sf::st_linestring(ls.matrix) ## LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2) plot(sf::st_multipoint(mp.matrix)) plot(sf::st_linestring(ls.matrix)) To create a polygon, you use lists. poly.list &lt;- list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))) sf::st_polygon(poly.list) ## POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5)) poly.border &lt;- rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)) poly.hole &lt;- rbind(c(2, 4), c(3, 4), c(3, 3), c(2, 3), c(2, 4)) poly.with.hole.list &lt;- list(poly.border, poly.hole) sf::st_polygon(poly.with.hole.list) ## POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5), (2 4, 3 4, 3 3, 2 3, 2 4)) plot(sf::st_polygon(poly.list)) plot(sf::st_polygon(poly.with.hole.list)) Simple feature geometry column One sfg object contains a single simple feature geometry. A simple feature geometry column (sfc) is a list of sfg objects together with information about the coordinate reference system. For example, to combine two simple features into one object with two features, you use the st_sfc() function. This is important since sfg represents the geometry column in sf data frames. point1 &lt;- sf::st_point(c(5, 2)) point2 &lt;- sf::st_point(c(1, 3)) sf::st_sfc(point1, point2) ## Geometry set for 2 features ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 1 ymin: 2 xmax: 5 ymax: 3 ## CRS: NA ## POINT (5 2) ## POINT (1 3) In most cases, a sfc object contains objects of the same geometry type. Thus, when you convert sfg objects of type polygon into a simple feature geometry column, you end up with an sfc object of type polygon. A geometry column of multiple line strings would result in an sfc object of type multilinestring. An example with polygons. poly.list1 &lt;- list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))) polygon1 &lt;- sf::st_polygon(poly.list1) poly.list2 &lt;- list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2))) polygon2 &lt;- sf::st_polygon(poly.list2) sf::st_sfc(polygon1, polygon2) ## Geometry set for 2 features ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 0 ymin: 1 xmax: 4 ymax: 5 ## CRS: NA ## POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5)) ## POLYGON ((0 2, 1 2, 1 3, 0 3, 0 2)) plot(sf::st_sfc(polygon1, polygon2)) An example with line strings. mls.list1 &lt;- list(rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)), rbind(c(1, 2), c(2, 4))) mls1 &lt;- sf::st_multilinestring((mls.list1)) mls.list2 &lt;- list(rbind(c(2, 9), c(7, 9), c(5, 6), c(4, 7), c(2, 7)), rbind(c(1, 7), c(3, 8))) mls2 &lt;- sf::st_multilinestring((mls.list2)) sf::st_sfc(mls1, mls2) ## Geometry set for 2 features ## Geometry type: MULTILINESTRING ## Dimension: XY ## Bounding box: xmin: 1 ymin: 1 xmax: 7 ymax: 9 ## CRS: NA ## MULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 ... ## MULTILINESTRING ((2 9, 7 9, 5 6, 4 7, 2 7), (1 ... plot(sf::st_sfc(mls1, mls2)) An example with a geometry collection. sf::st_sfc(point1, mls1) ## Geometry set for 2 features ## Geometry type: GEOMETRY ## Dimension: XY ## Bounding box: xmin: 1 ymin: 1 xmax: 5 ymax: 5 ## CRS: NA ## POINT (5 2) ## MULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 ... A sfc object also stores information on the coordinate reference systems (CRS). To specify a certain CRS, you use the epsg or proj4string attributes. The default value of epsg and proj4string is NA (Not Available). sf::st_sfc(point1, point2) ## Geometry set for 2 features ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 1 ymin: 2 xmax: 5 ymax: 3 ## CRS: NA ## POINT (5 2) ## POINT (1 3) All geometries in an sfc object must have the same CRS. We add coordinate reference system as a crs = argument in st_sfc(). The argument accepts an integer with the epsg code (for example, 4326). ( sfc1 &lt;- sf::st_sfc(point1, point2, crs = 4326) ) ## Geometry set for 2 features ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 1 ymin: 2 xmax: 5 ymax: 3 ## Geodetic CRS: WGS 84 ## POINT (5 2) ## POINT (1 3) The epsg code is translated to a well-known text (WKT) representation of the CRS. sf::st_crs(sfc1) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;Horizontal component of 3D system.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] Here the WKT describes a two-dimensional geographic coordinate reference system (GEOCRS) with a latitude axis first, then a longitude axis. The coordinate system is related to Earth by the WGS84 geodetic datum. See: https://en.wikipedia.org/wiki/Well-known_text_representation_of_coordinate_reference_systems Simple feature data frames Features (geometries) typically come with attributes. The attributes might represent the name of the geometry, measured values, groups to which the geometry belongs, etc. The simple feature class, sf, is a combination of an attribute table (data.frame) and a simple feature geometry column (sfc). Simple features are created using the st_sf() function. Objects of class sf behave like regular data frames. methods(class = &quot;sf&quot;) ## [1] [ [[&lt;- $&lt;- aggregate as.data.frame ## [6] cbind coerce filter identify initialize ## [11] merge plot print rbind show ## [16] slotsFromS3 transform ## see &#39;?methods&#39; for accessing help and source code Simple features have two classes, sf and data.frame. This is central to the concept of simple features: most of the time a sf can be treated as, and behaves like, a data.frame. Simple features are, in essence, data frames but with a column containing the geometric information. I refer to simple feature objects redundantly as ‘simple feature data frames’ to distinguish them from S4 class spatial data frames. Many of these functions were developed for data frames including rbind() (for binding rows of data together) and $ (for creating new columns). The key feature of sf objects is that they store spatial and non-spatial data in the same way, as columns in a data.frame. The geometry column of {sf} objects is typically called geometry but any name can be used. Thus sf objects take advantage of R’s data analysis capabilities to be used on geographic data. It’s worth reviewing how to discover basic properties of vector data objects. For example, we get information about the size and breadth of the world simple feature data frame from the {spData} package using dim(), nrow(), etc. library(spData) ## To access larger datasets in this package, install the spDataLarge ## package with: `install.packages(&#39;spDataLarge&#39;, ## repos=&#39;https://nowosad.github.io/drat/&#39;, type=&#39;source&#39;)` dim(world) ## [1] 177 11 nrow(world) ## [1] 177 ncol(world) ## [1] 11 The data contains ten non-geographic columns (and one geometry column) with 177 rows each one representing a country. Extracting the attribute data from an sf object is the same as dropping its geometry. world.df &lt;- world |&gt; sf::st_drop_geometry() class(world.df) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Example: Temperatures at FSU and at the airport Suppose you measure a temperature of 25C at FSU and 22C at the airport at 9a on September 6, 2022. Thus, you have specific points in space (the coordinates), the name of the locations (FSU, Airport), temperature values and the date of the measurement. Other attributes might include a urbanity category (campus or city), or a remark if the measurement was made with an automatic station. Start by creating to sfg (simple feature geometry) point objects. FSU.point &lt;- sf::st_point(c(-84.29849, 30.44188)) TLH.point &lt;- sf::st_point(c(-84.34505, 30.39541)) Then combine the point objects into a sfc (simple feature column) object. our.geometry &lt;- sf::st_sfc(FSU.point, TLH.point, crs = 4326) Then create a data frame of attributes. our.attributes &lt;- data.frame(name = c(&quot;FSU&quot;, &quot;Airport&quot;), temperature = c(10, 5), date = c(as.Date(&quot;2021-01-27&quot;), as.Date(&quot;2021-01-27&quot;)), category = c(&quot;campus&quot;, &quot;airport&quot;), automatic = c(TRUE, FALSE)) Finally create a simple feature data frame. sfdf &lt;- sf::st_sf(our.attributes, geometry = our.geometry) The example illustrates the components of sf objects. First, you use coordinates to define the geometry of the simple feature geometry (sfg). Second, you can combine the geometries into a simple feature geometry column (sfc) which also stores the CRS. Third, you store the attribute information on the geometries in a data.frame. Fourth, you use the st_sf() function to combine the attribute table and the sfc object into a sf object. sfdf ## Simple feature collection with 2 features and 5 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -84.34505 ymin: 30.39541 xmax: -84.29849 ymax: 30.44188 ## Geodetic CRS: WGS 84 ## name temperature date category automatic geometry ## 1 FSU 10 2021-01-27 campus TRUE POINT (-84.29849 30.44188) ## 2 Airport 5 2021-01-27 airport FALSE POINT (-84.34505 30.39541) class(sfdf) ## [1] &quot;sf&quot; &quot;data.frame&quot; Given a simple feature data frame you create a non-spatial data frame with a geometry list-column but that is not of class sf using the as.data.frame() function. df &lt;- as.data.frame(sfdf) class(df) ## [1] &quot;data.frame&quot; In this case the geometry column is no longer a sfc. no longer has a plot method, and lacks all dedicated methods listed above for class sf In order to avoid any confusion it might be better to use the st_drop_geometry() column instead. Example: US states The object us_states from the {spData} package is a simple feature data frame from the U.S. Census Bureau. The variables include the name, region, area, and population. Simple feature data frames can be treated as regular data frames. But the geometry is “sticky”. For example when we create a new data frame containing only the population information the geometry column is included in the new data frame. df1 &lt;- us_states |&gt; dplyr::select(starts_with(&quot;total&quot;)) head(df1) ## Simple feature collection with 6 features and 2 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -114.8136 ymin: 24.55868 xmax: -71.78699 ymax: 42.04964 ## Geodetic CRS: NAD83 ## total_pop_10 total_pop_15 geometry ## 1 4712651 4830620 MULTIPOLYGON (((-88.20006 3... ## 2 6246816 6641928 MULTIPOLYGON (((-114.7196 3... ## 3 4887061 5278906 MULTIPOLYGON (((-109.0501 4... ## 4 3545837 3593222 MULTIPOLYGON (((-73.48731 4... ## 5 18511620 19645772 MULTIPOLYGON (((-81.81169 2... ## 6 9468815 10006693 MULTIPOLYGON (((-85.60516 3... The resulting data frame has the two population columns but also a column labeled geometry. When we use the summarize() function, a union of the geometries across rows is made. df2 &lt;- us_states |&gt; dplyr::filter(REGION == &quot;Midwest&quot;) |&gt; dplyr::summarize(TotalPop2010 = sum(total_pop_10), TotalPop2015 = sum(total_pop_15)) head(df2) ## Simple feature collection with 1 feature and 2 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -104.0577 ymin: 35.99568 xmax: -80.51869 ymax: 49.38436 ## Geodetic CRS: NAD83 ## TotalPop2010 TotalPop2015 geometry ## 1 66514091 67546398 MULTIPOLYGON (((-85.48703 4... Why use the {sf} package when {sp} is already tried and tested? Fast reading and writing of data Enhanced plotting performance sf objects are treated as data frames in most operations sf functions are combined using |&gt; operator and they work well with the {tidyverse} packages sf function names are consistent and intuitive (all begin with st_) These advantages led to the development of spatial packages (including {tmap}, {mapview} and {tidycensus}) that now support simple feature objects. It is easy to convert between the two classes. Consider the world S3 spatial data frame from the {spData} package. You convert it to a S4 spatial data frame with the as() method. world.sp &lt;- world |&gt; as(Class = &quot;Spatial&quot;) The method coerces simple features to Spatial* and Spatial*DataFrame objects. You convert a S4 spatial data frame into a simple feature data frame with the st_as_sf() function. world.sf &lt;- world.sp |&gt; sf::st_as_sf() You can create basic maps from simple feature data frames with the base plot() method (plot.sf()). The function creates a multi-panel one sub-plot for each variable. Geo-computation on simple features Geo-computation on simple features is done with routines in the geometry engine-open source (GEOS) library that functions in the {sf} package make use of. As an example, consider the file police.zip on my website that contains shapefiles in a folder called police. The variables include police expenditures (POLICE), crime (CRIME), income (INC), unemployment (UNEMP) and other socio-economic variables for counties in Mississippi. Input the data using the st_read() function from the {sf} package and then assign a geographic coordinate reference system (CRS) to it using the EPSG number 4326. download.file(url = &quot;http://myweb.fsu.edu/jelsner/temp/data/police.zip&quot;, destfile = here::here(&quot;data&quot;, &quot;police.zip&quot;)) unzip(here::here(&quot;data&quot;, &quot;police.zip&quot;), exdir = here::here(&quot;data&quot;)) sfdf &lt;- sf::st_read(dsn = here::here(&quot;data&quot;, &quot;police&quot;), layer = &quot;police&quot;) ## Reading layer `police&#39; from data source ## `/Users/jameselsner/Desktop/ClassNotes/ASS-2022/data/police&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 82 features and 21 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -91.64356 ymin: 30.19474 xmax: -88.09043 ymax: 35.00496 ## CRS: NA sf::st_crs(sfdf) &lt;- 4326 The geometries are polygons and there are 82 of them, one for each county. You transform the geographic coordinate system of the polygons to a specific projected CRS as suggested by the function suggest_crs() from the {crsuggest} package. crsuggest::suggest_crs(sfdf) ## # A tibble: 10 × 6 ## crs_code crs_name crs_type crs_gcs crs_units crs_proj4 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 6508 NAD83(2011) / Mississippi TM project… 6318 m +proj=tm… ## 2 3816 NAD83(NSRS2007) / Mississippi … project… 4759 m +proj=tm… ## 3 3815 NAD83(HARN) / Mississippi TM project… 4152 m +proj=tm… ## 4 3814 NAD83 / Mississippi TM project… 4269 m +proj=tm… ## 5 6510 NAD83(2011) / Mississippi West… project… 6318 us-ft +proj=tm… ## 6 6509 NAD83(2011) / Mississippi West project… 6318 m +proj=tm… ## 7 3600 NAD83(NSRS2007) / Mississippi … project… 4759 us-ft +proj=tm… ## 8 3599 NAD83(NSRS2007) / Mississippi … project… 4759 m +proj=tm… ## 9 2900 NAD83(HARN) / Mississippi West… project… 4152 us-ft +proj=tm… ## 10 2814 NAD83(HARN) / Mississippi West project… 4152 m +proj=tm… The function for transforming the CRS is st_transform() from the {sf} package. sfdf &lt;- sfdf |&gt; sf::st_transform(crs = 6508) The st_centroid() function computes the geographic center of each polygon in the spatial data frame. countyCenters.sf &lt;- sfdf |&gt; sf::st_centroid() ## Warning in st_centroid.sf(sfdf): st_centroid assumes attributes are constant ## over geometries of x The warning lets you know that the attributes attached to each polygon might result in misleading information when attached to the new geometry (points). Different geometries can mean different interpretations of the attribute. sf::st_geometry(countyCenters.sf) ## Geometry set for 82 features ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 347970.3 ymin: 1069814 xmax: 639099.2 ymax: 1565402 ## Projected CRS: NAD83(2011) / Mississippi TM ## First 5 geometries: ## POINT (607909.2 1565402) ## POINT (639099.2 1550199) ## POINT (577860 1552732) ## POINT (552191 1557790) ## POINT (478087.6 1564075) To get the centroid location for the state, you first join all the counties using the st_union() function, then use the st_centroid() function. stateCenter.sfc &lt;- sfdf |&gt; sf::st_union() |&gt; sf::st_centroid() The result is a simple feature geometry column (sfc) with a single row where the geometry contains the centroid location. Which county contains the geographic center of the state? Here you use the geometric binary predicate st_contains(). ( Contains &lt;- sfdf |&gt; sf::st_contains(stateCenter.sfc, sparse = FALSE) ) ## [,1] ## [1,] FALSE ## [2,] FALSE ## [3,] FALSE ## [4,] FALSE ## [5,] FALSE ## [6,] FALSE ## [7,] FALSE ## [8,] FALSE ## [9,] FALSE ## [10,] FALSE ## [11,] FALSE ## [12,] FALSE ## [13,] FALSE ## [14,] FALSE ## [15,] FALSE ## [16,] FALSE ## [17,] FALSE ## [18,] FALSE ## [19,] FALSE ## [20,] FALSE ## [21,] FALSE ## [22,] FALSE ## [23,] FALSE ## [24,] FALSE ## [25,] FALSE ## [26,] FALSE ## [27,] FALSE ## [28,] FALSE ## [29,] FALSE ## [30,] FALSE ## [31,] FALSE ## [32,] FALSE ## [33,] FALSE ## [34,] FALSE ## [35,] FALSE ## [36,] FALSE ## [37,] FALSE ## [38,] FALSE ## [39,] FALSE ## [40,] FALSE ## [41,] FALSE ## [42,] FALSE ## [43,] FALSE ## [44,] TRUE ## [45,] FALSE ## [46,] FALSE ## [47,] FALSE ## [48,] FALSE ## [49,] FALSE ## [50,] FALSE ## [51,] FALSE ## [52,] FALSE ## [53,] FALSE ## [54,] FALSE ## [55,] FALSE ## [56,] FALSE ## [57,] FALSE ## [58,] FALSE ## [59,] FALSE ## [60,] FALSE ## [61,] FALSE ## [62,] FALSE ## [63,] FALSE ## [64,] FALSE ## [65,] FALSE ## [66,] FALSE ## [67,] FALSE ## [68,] FALSE ## [69,] FALSE ## [70,] FALSE ## [71,] FALSE ## [72,] FALSE ## [73,] FALSE ## [74,] FALSE ## [75,] FALSE ## [76,] FALSE ## [77,] FALSE ## [78,] FALSE ## [79,] FALSE ## [80,] FALSE ## [81,] FALSE ## [82,] FALSE You include the sparse = FALSE argument so the result is a matrix containing TRUEs and FALSEs. Since there are 82 counties and one centroid the matrix has 82 rows and 1 column. All matrix entries are FALSE except the one containing the center. To map the result you first plot the county polygons, then add the county geometry for the center county and fill it red. Note that you use the matrix you called Contains to subset this county. Finally you add the location of the state centroid to the plot. library(ggplot2) ggplot(data = sfdf) + geom_sf() + geom_sf(data = sfdf[Contains, ], col = &quot;red&quot;) + geom_sf(data = stateCenter.sfc) + theme_void() The function st_area() returns a vector of the geographical area (in sq. units) of each of the spatial objects. Here county boundaries as polygons. sfdf |&gt; sf::st_area() ## Units: [m^2] ## [1] 1059845789 1129805491 1177795196 1063555913 1292308911 1837313947 ## [7] 1209563206 1050434214 1081610367 1111684631 1789727469 1873807962 ## [13] 1537935343 1182321227 1047492468 1387412534 1298534836 1305083703 ## [19] 1689710519 1539012925 2325208833 2002801262 1320449739 1821075582 ## [25] 1165056081 1566560661 1062455271 1313215090 1103428267 1584296774 ## [31] 1053624790 1195801329 1966286085 1077116415 2004569381 1127253959 ## [37] 1933657382 1811075898 1604263674 1162255863 2412017942 1136584592 ## [43] 1491632367 1497746748 1952707326 1894657055 1566911457 1653697972 ## [49] 2045637420 1824649519 1481229225 2294433604 1797593028 1798362792 ## [55] 1625787733 1254286842 1564126825 2051293154 2027900509 1366995434 ## [61] 1858641378 1066358984 1072169833 1117271183 1208280395 1493574538 ## [67] 1450255735 1398323080 1284051541 1225628132 1695604706 1798766462 ## [73] 1811701938 1955125869 1067233035 1063132758 2151445783 1205245026 ## [79] 1177906764 1834858322 1533395797 1238226953 The vector values have units of square meters (m^2), which are derived from the CRS. There is an attribute called AREA in the data frame but it is better to calculate it from the spatial polygons because then you are sure of the units. What happens when you apply the area function on the centroid object? countyCenters.sf |&gt; sf::st_area() ## Units: [m^2] ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [39] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [77] 0 0 0 0 0 0 Compute a 10 km buffer around the state and show the result with a plot. First use st_union(), then st_buffer(), then pipe the output (a simple feature data frame to ggplot()). sfdf |&gt; sf::st_union() |&gt; sf::st_buffer(dist = 10000) |&gt; ggplot() + geom_sf() + geom_sf(data = sfdf) + theme_void() Length of boundary lines for U.S. states. Transform the CRS to 2163 (US National Atlas Equal Area). Note that the geometry is multi-polygons. Convert the polygons to multi-linestrings, then use st_length() to get the total length of the lines. states &lt;- spData::us_states |&gt; sf::st_transform(crs = 2163) sf::st_length(states) # returns zeroes because geometry is polygon ## Units: [m] ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [39] 0 0 0 0 0 0 0 0 0 0 0 states |&gt; sf::st_cast(to = &quot;MULTILINESTRING&quot;) |&gt; sf::st_length() ## Units: [m] ## [1] 1710433.26 2298001.84 2102155.22 514571.45 2949041.87 1746686.74 ## [7] 2570625.84 1436260.52 1957704.71 2216035.60 1018647.33 2567370.61 ## [13] 2112070.19 2826608.77 2338551.23 747093.89 2296345.66 1771057.91 ## [19] 2334844.08 1497781.59 1275536.83 1998187.83 4959188.68 778627.31 ## [25] 1592462.66 1680675.61 3810365.04 407944.22 60329.77 1859164.04 ## [31] 1656525.21 1835778.08 1572793.19 1638356.40 3579150.33 1711366.62 ## [37] 2051115.13 782857.63 2378176.63 2221531.40 1467821.30 2198130.04 ## [43] 304698.28 1855108.54 1972765.77 2407304.31 2344371.69 1900586.84 ## [49] 2029017.32 "],["thursday-september-8-2022.html", "Thursday September 8, 2022 Today Spatial data subsets and joins Interpolation using areal weights", " Thursday September 8, 2022 “Hell isn’t other people’s code. Hell is your own code from 3 years ago.” – Jeff Atwood Today Spatial data subsets and joins Interpolating variables using areal weights S4 spatial data frames Spatial data subsets and joins Variables (stored as columns) in spatial data structures are referred to as ‘attributes’. With simple feature data frames you can create data subsets using [, subset() and $ from the {base} R packages and select() and filter() from the {dplyr} package. The [ operator subsets rows and columns. Indexes specify the elements you wish to extract from an object, e.g. object[i, j], with i and j typically being numbers representing rows and columns. Leaving i or j empty returns all rows or columns, so world[1:5, ] returns the first five rows and all columns of the simple feature data frame world (from the {spData} package). Examples world &lt;- spData::world world[c(1, 5, 9), ] # subset rows by row position ## Simple feature collection with 3 features and 10 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -180 ymin: -18.28799 xmax: 180 ymax: 71.35776 ## Geodetic CRS: WGS 84 ## # A tibble: 3 × 11 ## iso_a2 name_long continent region_un subregion type area_km2 pop lifeExp ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 FJ Fiji Oceania Oceania Melanesia Sove… 19290. 8.86e5 70.0 ## 2 US United Sta… North Am… Americas Northern… Coun… 9510744. 3.19e8 78.8 ## 3 ID Indonesia Asia Asia South-Ea… Sove… 1819251. 2.55e8 68.9 ## # … with 2 more variables: gdpPercap &lt;dbl&gt;, geom &lt;MULTIPOLYGON [°]&gt; world[, 1:3] # subset columns by column position ## Simple feature collection with 177 features and 3 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513 ## Geodetic CRS: WGS 84 ## # A tibble: 177 × 4 ## iso_a2 name_long continent geom ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;MULTIPOLYGON [°]&gt; ## 1 FJ Fiji Oceania (((-180 -16.55522, -179.9174 -16.50178… ## 2 TZ Tanzania Africa (((33.90371 -0.95, 31.86617 -1.02736, … ## 3 EH Western Sahara Africa (((-8.66559 27.65643, -8.817828 27.656… ## 4 CA Canada North America (((-132.71 54.04001, -133.18 54.16998,… ## 5 US United States North America (((-171.7317 63.78252, -171.7911 63.40… ## 6 KZ Kazakhstan Asia (((87.35997 49.21498, 86.82936 49.8266… ## 7 UZ Uzbekistan Asia (((55.96819 41.30864, 57.09639 41.3223… ## 8 PG Papua New Guinea Oceania (((141.0002 -2.600151, 141.0171 -5.859… ## 9 ID Indonesia Asia (((104.37 -1.084843, 104.0108 -1.05921… ## 10 AR Argentina South America (((-68.63401 -52.63637, -68.63335 -54.… ## # … with 167 more rows world[, c(&quot;name_long&quot;, &quot;lifeExp&quot;)] # subset columns by name ## Simple feature collection with 177 features and 2 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513 ## Geodetic CRS: WGS 84 ## # A tibble: 177 × 3 ## name_long lifeExp geom ## &lt;chr&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [°]&gt; ## 1 Fiji 70.0 (((-180 -16.55522, -179.9174 -16.50178, -179.7933 -… ## 2 Tanzania 64.2 (((33.90371 -0.95, 31.86617 -1.02736, 30.76986 -1.0… ## 3 Western Sahara NA (((-8.66559 27.65643, -8.817828 27.65643, -8.794884… ## 4 Canada 82.0 (((-132.71 54.04001, -133.18 54.16998, -133.2397 53… ## 5 United States 78.8 (((-171.7317 63.78252, -171.7911 63.40585, -171.553… ## 6 Kazakhstan 71.6 (((87.35997 49.21498, 86.82936 49.82667, 85.54127 4… ## 7 Uzbekistan 71.0 (((55.96819 41.30864, 57.09639 41.32231, 56.93222 4… ## 8 Papua New Guinea 65.2 (((141.0002 -2.600151, 141.0171 -5.859022, 141.0339… ## 9 Indonesia 68.9 (((104.37 -1.084843, 104.0108 -1.059212, 103.4376 -… ## 10 Argentina 76.3 (((-68.63401 -52.63637, -68.63335 -54.8695, -67.562… ## # … with 167 more rows Here you use logical vectors to create a subset. First create a logical vector sel_area. sel_area &lt;- world$area_km2 &lt; 10000 head(sel_area) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE summary(sel_area) ## Mode FALSE TRUE ## logical 170 7 Then select only cases from the world simple feature data frame where the elements of the sel_area vector are TRUE. small_countries &lt;- world[sel_area, ] This creates a new simple feature data frame, small_countries, containing nations whose surface area is smaller than 10,000 square kilometers. Note: there is no harm in keeping the geometry column because an operation on a {sf} object only changes the geometry when appropriate (e.g. by dissolving borders between adjacent polygons following aggregation). This means that the speed of operations with attribute data in {sf} objects is the same as with columns in a data frames. The {base} R function subset() provides another way to get the same result. small_countries &lt;- subset(world, area_km2 &lt; 10000) The {dplyr} verbs work on {sf} spatial data frames. The functions include dplyr::select() and dplyr::filter(). CAUTION! The {dplyr} and {raster} packages have a select() function. When using both packages in the same session, the function in the most recently attached package will be used, ‘masking’ the other function. This will generate error messages containing text like: unable to find an inherited method for function ‘select’ for signature “sf”. To avoid this error message, and prevent ambiguity, you should always use the long-form function name, prefixed by the package name and two colons dplyr::select(). The dplyr::select() function picks the columns by name or position. For example, we can select only two columns, name_long and pop, with the following command. world1 &lt;- world |&gt; dplyr::select(name_long, pop) names(world1) ## [1] &quot;name_long&quot; &quot;pop&quot; &quot;geom&quot; The result is a simple feature data frame with the geometry column. With the select() function you can subset and rename columns at the same time. Here you select the columns with names name_long and pop and give the pop column a new name (population). world |&gt; dplyr::select(name_long, population = pop) ## Simple feature collection with 177 features and 2 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513 ## Geodetic CRS: WGS 84 ## # A tibble: 177 × 3 ## name_long population geom ## &lt;chr&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [°]&gt; ## 1 Fiji 885806 (((-180 -16.55522, -179.9174 -16.50178, -179.793… ## 2 Tanzania 52234869 (((33.90371 -0.95, 31.86617 -1.02736, 30.76986 -… ## 3 Western Sahara NA (((-8.66559 27.65643, -8.817828 27.65643, -8.794… ## 4 Canada 35535348 (((-132.71 54.04001, -133.18 54.16998, -133.2397… ## 5 United States 318622525 (((-171.7317 63.78252, -171.7911 63.40585, -171.… ## 6 Kazakhstan 17288285 (((87.35997 49.21498, 86.82936 49.82667, 85.5412… ## 7 Uzbekistan 30757700 (((55.96819 41.30864, 57.09639 41.32231, 56.9322… ## 8 Papua New Guinea 7755785 (((141.0002 -2.600151, 141.0171 -5.859022, 141.0… ## 9 Indonesia 255131116 (((104.37 -1.084843, 104.0108 -1.059212, 103.437… ## 10 Argentina 42981515 (((-68.63401 -52.63637, -68.63335 -54.8695, -67.… ## # … with 167 more rows The dplyr::pull() function returns a single vector without the geometry. world |&gt; dplyr::pull(pop) ## [1] 885806 52234869 NA 35535348 318622525 17288285 ## [7] 30757700 7755785 255131116 42981515 17613798 73722860 ## [13] 13513125 46024250 37737913 13569438 10572466 10405844 ## [19] 143819666 382169 NA NA 56295 NA ## [25] 1212814 54539571 2145785 124221600 3419546 204213133 ## [31] 10562159 30973354 47791911 3903986 4757575 6013997 ## [37] 8809216 6281189 15923559 351694 30738378 763393 ## [43] 547928 NA 15903112 3534874 2862087 11439767 ## [49] 15411675 2168573 2370992 14546111 16962846 4063920 ## [55] 10286712 19148219 176460502 22239904 7228915 26962563 ## [61] 22531350 11805509 1725744 4390737 7079162 17585977 ## [67] 4515392 4871101 1875713 1129424 15620974 17068838 ## [73] 27212382 1295097 26920466 9891790 8215700 5603279 ## [79] 23589801 4294682 1917852 11143908 39113313 8809306 ## [85] 9070867 2374419 3782450 35006080 3960925 258850 ## [91] 15270790 68416772 6576397 51924182 92544915 25116363 ## [97] 50746659 2923896 1293859294 159405279 776448 28323241 ## [103] 185546257 32758020 8362745 5835500 5466241 78411092 ## [109] 19203090 2906220 9696110 9474511 45271947 38011735 ## [115] 8546356 9866468 3556397 19908979 2932367 1993782 ## [121] 1314545 80982500 7223938 10892413 77030628 2889104 ## [127] 4238389 8188649 556319 11209057 16865008 10401062 ## [133] 46480882 4657740 268050 575504 4509700 23504138 ## [139] 20771000 1364270000 NA 60789140 5643475 64613160 ## [145] 327386 9535079 3727000 100102249 30228017 411704 ## [151] 2061980 5461512 5418649 10525347 NA 127276000 ## [157] 6552584 26246327 30776722 NA NA 1152309 ## [163] 34318082 91812566 6204108 97366774 912164 NA ## [169] 38833338 11345357 3566002 2077495 7130576 621810 ## [175] 1821800 1354493 11530971 The filter() function keeps only rows matching given criteria, e.g., only countries with a very high average life expectancy. world |&gt; dplyr::filter(lifeExp &gt; 82) ## Simple feature collection with 9 features and 10 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -24.32618 ymin: -43.6346 xmax: 153.5695 ymax: 69.10625 ## Geodetic CRS: WGS 84 ## # A tibble: 9 × 11 ## iso_a2 name_long continent region_un subregion type area_km2 pop lifeExp ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 IL Israel Asia Asia Western … Coun… 22991. 8.22e6 82.2 ## 2 SE Sweden Europe Europe Northern… Sove… 450582. 9.70e6 82.3 ## 3 CH Switzerland Europe Europe Western … Sove… 46185. 8.19e6 83.2 ## 4 LU Luxembourg Europe Europe Western … Sove… 2417. 5.56e5 82.2 ## 5 ES Spain Europe Europe Southern… Sove… 502306. 4.65e7 83.2 ## 6 AU Australia Oceania Oceania Australi… Coun… 7687614. 2.35e7 82.3 ## 7 IT Italy Europe Europe Southern… Sove… 315105. 6.08e7 83.1 ## 8 IS Iceland Europe Europe Northern… Sove… 107736. 3.27e5 82.9 ## 9 JP Japan Asia Asia Eastern … Sove… 404620. 1.27e8 83.6 ## # … with 2 more variables: gdpPercap &lt;dbl&gt;, geom &lt;MULTIPOLYGON [°]&gt; Aggregation summarizes a data frame by a grouping variable. An example of aggregation is to calculate the number of people per continent based on country-level data (one row per country). This is done with the dplyr::group_by() and dplyr::summarize() functions. world |&gt; dplyr::group_by(continent) |&gt; dplyr::summarize(Population = sum(pop, na.rm = TRUE), nCountries = dplyr::n()) ## Simple feature collection with 8 features and 3 fields ## Geometry type: GEOMETRY ## Dimension: XY ## Bounding box: xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513 ## Geodetic CRS: WGS 84 ## # A tibble: 8 × 4 ## continent Population nCountries geom ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;GEOMETRY [°]&gt; ## 1 Africa 1154946633 51 MULTIPOLYGON (((43.1453 11.4620… ## 2 Antarctica 0 1 MULTIPOLYGON (((-180 -89.9, 180… ## 3 Asia 4311408059 47 MULTIPOLYGON (((104.37 -1.08484… ## 4 Europe 669036256 39 MULTIPOLYGON (((-180 64.97971, … ## 5 North America 565028684 18 MULTIPOLYGON (((-132.71 54.0400… ## 6 Oceania 37757833 7 MULTIPOLYGON (((-180 -16.55522,… ## 7 Seven seas (open ocean) 0 1 POLYGON ((68.935 -48.625, 68.86… ## 8 South America 412060811 13 MULTIPOLYGON (((-66.95992 -54.8… The two columns in the resulting table are Population and nCountries. The functions sum() and dplyr::n() were the aggregating functions. The result is a simple feature data frame with a single row representing attributes of the world and the geometry as a single multi-polygon through the geometric union operator. You can chain together functions to find the world’s three most populous continents and the number of countries they contain. world |&gt; dplyr::select(pop, continent) |&gt; dplyr::group_by(continent) |&gt; dplyr::summarize(Population = sum(pop, na.rm = TRUE), nCountries = dplyr::n()) |&gt; dplyr::top_n(n = 3, wt = Population) ## Simple feature collection with 3 features and 3 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -180 ymin: -34.81917 xmax: 180 ymax: 81.2504 ## Geodetic CRS: WGS 84 ## # A tibble: 3 × 4 ## continent Population nCountries geom ## * &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;MULTIPOLYGON [°]&gt; ## 1 Africa 1154946633 51 (((43.1453 11.46204, 42.71587 11.73564, 43.28… ## 2 Asia 4311408059 47 (((104.37 -1.084843, 104.0108 -1.059212, 103.… ## 3 Europe 669036256 39 (((-180 64.97971, -179.4327 65.40411, -179.88… If you want to create a new column based on existing columns use dplyr::mutate(). For example, if you want to calculate population density for each country divide the population column, here pop, by an area column, here area_km2 with unit area in square kilometers. world |&gt; dplyr::mutate(Population_Density = pop / area_km2) ## Simple feature collection with 177 features and 11 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513 ## Geodetic CRS: WGS 84 ## # A tibble: 177 × 12 ## iso_a2 name_long continent region_un subregion type area_km2 pop lifeExp ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 FJ Fiji Oceania Oceania Melanesia Sove… 1.93e4 8.86e5 70.0 ## 2 TZ Tanzania Africa Africa Eastern … Sove… 9.33e5 5.22e7 64.2 ## 3 EH Western … Africa Africa Northern… Inde… 9.63e4 NA NA ## 4 CA Canada North Am… Americas Northern… Sove… 1.00e7 3.55e7 82.0 ## 5 US United S… North Am… Americas Northern… Coun… 9.51e6 3.19e8 78.8 ## 6 KZ Kazakhst… Asia Asia Central … Sove… 2.73e6 1.73e7 71.6 ## 7 UZ Uzbekist… Asia Asia Central … Sove… 4.61e5 3.08e7 71.0 ## 8 PG Papua Ne… Oceania Oceania Melanesia Sove… 4.65e5 7.76e6 65.2 ## 9 ID Indonesia Asia Asia South-Ea… Sove… 1.82e6 2.55e8 68.9 ## 10 AR Argentina South Am… Americas South Am… Sove… 2.78e6 4.30e7 76.3 ## # … with 167 more rows, and 3 more variables: gdpPercap &lt;dbl&gt;, ## # geom &lt;MULTIPOLYGON [°]&gt;, Population_Density &lt;dbl&gt; world |&gt; dplyr::transmute(Population_Density = pop / area_km2) ## Simple feature collection with 177 features and 1 field ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513 ## Geodetic CRS: WGS 84 ## # A tibble: 177 × 2 ## Population_Density geom ## * &lt;dbl&gt; &lt;MULTIPOLYGON [°]&gt; ## 1 45.9 (((-180 -16.55522, -179.9174 -16.50178, -179.7933 -16.020… ## 2 56.0 (((33.90371 -0.95, 31.86617 -1.02736, 30.76986 -1.01455, … ## 3 NA (((-8.66559 27.65643, -8.817828 27.65643, -8.794884 27.12… ## 4 3.54 (((-132.71 54.04001, -133.18 54.16998, -133.2397 53.85108… ## 5 33.5 (((-171.7317 63.78252, -171.7911 63.40585, -171.5531 63.3… ## 6 6.33 (((87.35997 49.21498, 86.82936 49.82667, 85.54127 49.6928… ## 7 66.7 (((55.96819 41.30864, 57.09639 41.32231, 56.93222 41.8260… ## 8 16.7 (((141.0002 -2.600151, 141.0171 -5.859022, 141.0339 -9.11… ## 9 140. (((104.37 -1.084843, 104.0108 -1.059212, 103.4376 -0.7119… ## 10 15.4 (((-68.63401 -52.63637, -68.63335 -54.8695, -67.56244 -54… ## # … with 167 more rows The dplyr::transmute() function performs the same computation but also removes the other columns (except the geometry column). Subsetting (filtering) your data based on geographic boundaries The {USAboundaries} package has historical and contemporary boundaries for the United States provided by the U.S. Census Bureau. Individual states are extracted using the us_states() function. CAUTION: this function has the same name as the object us_states from the {spData} package. Here you use the argument states = to get only the state of Kansas. You then make a plot of the boundary and check the native coordinate reference system (CRS). KS.sf &lt;- USAboundaries::us_states(states = &quot;Kansas&quot;) library(ggplot2) ggplot(data = KS.sf) + geom_sf() sf::st_crs(KS.sf) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;Horizontal component of 3D system.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] The polygon geometry includes the border and the area inside the border. The CRS is described by the 4326 EPSG code and implemented using well-known text (WKT). You use a geometric operation to subset spatial data geographically (rather than on some attribute). For example here you subset the tornado tracks as line strings, keeping only those line strings that fall within the Kansas border defined by a polygon geometry. First import the tornado data. if(!&quot;1950-2020-torn-aspath&quot; %in% list.files(here::here(&quot;data&quot;))) { download.file(url = &quot;http://www.spc.noaa.gov/gis/svrgis/zipped/1950-2020-torn-aspath.zip&quot;, destfile = here::here(&quot;data&quot;, &quot;1950-2020-torn-aspath.zip&quot;)) unzip(here::here(&quot;data&quot;, &quot;1950-2020-torn-aspath.zip&quot;), exdir = here::here(&quot;data&quot;)) } Torn.sf &lt;- sf::st_read(dsn = here::here(&quot;data&quot;, &quot;1950-2020-torn-aspath&quot;), layer = &quot;1950-2020-torn-aspath&quot;) ## Reading layer `1950-2020-torn-aspath&#39; from data source ## `/Users/jameselsner/Desktop/ClassNotes/ASS-2022/data/1950-2020-torn-aspath&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 66244 features and 22 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -163.53 ymin: 17.7212 xmax: -64.7151 ymax: 61.02 ## Geodetic CRS: WGS 84 The geometries are line strings representing the approximate track of each tornado. The CRS has EPSG code of 4326, same as the Kansas polygon. To keep only the tornado tracks that fall within the border of Kansas you use the sf::st_intersection() function. The first argument (x =) is the simple feature data frame that you want to subset and the second argument (y =) defines the geometry over which the subset occurs. KS_Torn.sf &lt;- sf::st_intersection(x = Torn.sf, y = KS.sf) ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries You can use the pipe operator (|&gt;) to pass the first argument to the function. KS_Torn.sf &lt;- Torn.sf |&gt; sf::st_intersection(y = KS.sf) ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries You make a plot to see if things appear as you expect. ggplot() + geom_sf(data = KS.sf) + geom_sf(data = KS_Torn.sf) Note that no tornado track lies outside the state border. Line strings that lie outside the border are clipped at the border. However the attribute values represent the entire track. If you want the entire tornado track for all tornadoes that passed into (or through) the state, then you first use the geometric binary predict function sf::st_intersects(). With sparse = FALSE a matrix with a single column of TRUEs and FALSEs is returned. Here you use the piping operator to implicitly specify the x = argument as the Torn.sf data frame. Intersects &lt;- Torn.sf |&gt; sf::st_intersects(y = KS.sf, sparse = FALSE) head(Intersects) ## [,1] ## [1,] FALSE ## [2,] FALSE ## [3,] FALSE ## [4,] FALSE ## [5,] FALSE ## [6,] FALSE sum(Intersects) ## [1] 4377 Next you create a new data frame from the original data frame keeping only observations (rows) where Interects is TRUE. KS_Torn2.sf &lt;- Torn.sf[Intersects, ] ggplot() + geom_sf(data = KS.sf) + geom_sf(data = KS_Torn2.sf) Are tornadoes more common in some parts of Kansas than others? One way to answer this question is to see how far away the tornado centroid is from the center of the state. Start by computing the centers of the state polygon and the combined set of Kansas tornadoes using the sf::st_centroid() function. Note you first use the sf::st_combine() function on the tornadoes. geocenterKS &lt;- KS.sf |&gt; sf::st_centroid() ## Warning in st_centroid.sf(KS.sf): st_centroid assumes attributes are constant ## over geometries of x centerKStornadoes &lt;- KS_Torn.sf |&gt; sf::st_combine() |&gt; sf::st_centroid() Then make a map and compute the distance in meters using the sf::st_distance() function. ggplot() + geom_sf(data = KS.sf) + geom_sf(data = geocenterKS, col = &quot;blue&quot;) + geom_sf(data = centerKStornadoes, col = &quot;red&quot;) geocenterKS |&gt; sf::st_distance(centerKStornadoes) ## Units: [m] ## [,1] ## [1,] 2875.099 Less than 3 km! More examples: https://www.jla-data.net/eng/spatial-aggregation/ Mutating data frames with joins Combining data from different sources based on a shared variable is a common operation. The {dplyr} package has join functions that follow naming conventions used in database languages (like SQL). Given two data frames labeled x and y, the join functions add columns from y to x, matching rows based on the function name. inner_join(): includes all rows in x and y left_join(): includes all rows in x full_join(): includes all rows in x or y Join functions work the same on data frames and on simple feature data frames. The most common type of attribute join on spatial data takes a simple feature data frame as the first argument and adds columns to it from a data a frame specified as the second argument. For example, you combine data on coffee production with the spData::world simple feature data frame. Coffee production by country is in the data frame called spData::coffee_data. dplyr::glimpse(spData::coffee_data) ## Rows: 47 ## Columns: 3 ## $ name_long &lt;chr&gt; &quot;Angola&quot;, &quot;Bolivia&quot;, &quot;Brazil&quot;, &quot;Burundi&quot;, &quot;Came… ## $ coffee_production_2016 &lt;int&gt; NA, 3, 3277, 37, 8, NA, 4, 1330, 28, 114, NA, 1… ## $ coffee_production_2017 &lt;int&gt; NA, 4, 2786, 38, 6, NA, 12, 1169, 32, 130, NA, … It has 3 columns: name_long names major coffee-producing nations and coffee_production_2016 and coffee_production_2017 contain estimated values for coffee production in units of 60-kg bags per year. First select only the name and GDP (per person) from the spData::world simple feature data frame. ( world.sf &lt;- spData::world |&gt; dplyr::select(name_long, gdpPercap) ) ## Simple feature collection with 177 features and 2 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513 ## Geodetic CRS: WGS 84 ## # A tibble: 177 × 3 ## name_long gdpPercap geom ## &lt;chr&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [°]&gt; ## 1 Fiji 8222. (((-180 -16.55522, -179.9174 -16.50178, -179.7933… ## 2 Tanzania 2402. (((33.90371 -0.95, 31.86617 -1.02736, 30.76986 -1… ## 3 Western Sahara NA (((-8.66559 27.65643, -8.817828 27.65643, -8.7948… ## 4 Canada 43079. (((-132.71 54.04001, -133.18 54.16998, -133.2397 … ## 5 United States 51922. (((-171.7317 63.78252, -171.7911 63.40585, -171.5… ## 6 Kazakhstan 23587. (((87.35997 49.21498, 86.82936 49.82667, 85.54127… ## 7 Uzbekistan 5371. (((55.96819 41.30864, 57.09639 41.32231, 56.93222… ## 8 Papua New Guinea 3709. (((141.0002 -2.600151, 141.0171 -5.859022, 141.03… ## 9 Indonesia 10003. (((104.37 -1.084843, 104.0108 -1.059212, 103.4376… ## 10 Argentina 18798. (((-68.63401 -52.63637, -68.63335 -54.8695, -67.5… ## # … with 167 more rows The dplyr::left_join() function takes the data frame named by the argument x = and joins it to the data frame named by the argument y =. ( world_coffee.sf &lt;- dplyr::left_join(x = world.sf, y = spData::coffee_data) ) ## Joining, by = &quot;name_long&quot; ## Simple feature collection with 177 features and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513 ## Geodetic CRS: WGS 84 ## # A tibble: 177 × 5 ## name_long gdpPercap geom coffee_producti… ## &lt;chr&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [°]&gt; &lt;int&gt; ## 1 Fiji 8222. (((-180 -16.55522, -179.9174 -16… NA ## 2 Tanzania 2402. (((33.90371 -0.95, 31.86617 -1.0… 81 ## 3 Western Sahara NA (((-8.66559 27.65643, -8.817828 … NA ## 4 Canada 43079. (((-132.71 54.04001, -133.18 54.… NA ## 5 United States 51922. (((-171.7317 63.78252, -171.7911… NA ## 6 Kazakhstan 23587. (((87.35997 49.21498, 86.82936 4… NA ## 7 Uzbekistan 5371. (((55.96819 41.30864, 57.09639 4… NA ## 8 Papua New Guinea 3709. (((141.0002 -2.600151, 141.0171 … 114 ## 9 Indonesia 10003. (((104.37 -1.084843, 104.0108 -1… 742 ## 10 Argentina 18798. (((-68.63401 -52.63637, -68.6333… NA ## # … with 167 more rows, and 1 more variable: coffee_production_2017 &lt;int&gt; Because the two data frames share a common variable name (name_long) the join works without using the by = argument. The result is a simple feature data frame identical to the world.sf object but with two new variables indicating coffee production in 2016 and 2017. names(world_coffee.sf) ## [1] &quot;name_long&quot; &quot;gdpPercap&quot; &quot;geom&quot; ## [4] &quot;coffee_production_2016&quot; &quot;coffee_production_2017&quot; For a join to work there must be at least one variable name in common. Since the object listed in the x = argument is a simple feature data frame, the join function returns a simple feature data frame with the same number of rows (observations). Although there are only 47 rows of data in spData::coffee_data, all 177 of the country records in world.sf are kept intact in world_coffee.sf. Rows in the first dataset with no match are assigned NA values for the new coffee production variables. If you want to keep only countries that have a match in the key variable then use dplyr::inner_join(). Here you use the piping operator to implicitly specify the x = argument as the world.sf data frame. world.sf |&gt; dplyr::inner_join(spData::coffee_data) ## Joining, by = &quot;name_long&quot; ## Simple feature collection with 45 features and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -117.1278 ymin: -33.76838 xmax: 156.02 ymax: 35.49401 ## Geodetic CRS: WGS 84 ## # A tibble: 45 × 5 ## name_long gdpPercap geom coffee_producti… ## &lt;chr&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [°]&gt; &lt;int&gt; ## 1 Tanzania 2402. (((33.90371 -0.95, 31.86617 -1… 81 ## 2 Papua New Guinea 3709. (((141.0002 -2.600151, 141.017… 114 ## 3 Indonesia 10003. (((104.37 -1.084843, 104.0108 … 742 ## 4 Kenya 2753. (((39.20222 -4.67677, 39.60489… 60 ## 5 Dominican Republic 12663. (((-71.7083 18.045, -71.65766 … 1 ## 6 Timor-Leste 6263. (((124.9687 -8.89279, 125.07 -… 14 ## 7 Mexico 16623. (((-117.1278 32.53534, -116.72… 151 ## 8 Brazil 15374. (((-53.37366 -33.76838, -52.71… 3277 ## 9 Bolivia 6325. (((-69.52968 -10.95173, -68.66… 3 ## 10 Peru 11548. (((-69.89364 -4.298187, -70.39… 585 ## # … with 35 more rows, and 1 more variable: coffee_production_2017 &lt;int&gt; We can join in the other direction, starting with a regular data frame and adding variables from a simple features object. More information on attribute data operations such as these is given here: https://geocompr.robinlovelace.net/attr.html Interpolation using areal weights Areal-weighted interpolation estimates the value of some variable from a set of polygons to an overlapping but incongruent set of target polygons. For example, suppose you want demographic information given at the Census tract level to be estimated within the tornado damage path. Damage paths do not align with census tract boundaries so areal weighted interpolation is needed to get demographic estimates at the tornado level. The function sf::st_interpolate_aw() performs areal-weighted interpolation of polygon data. As an example, consider the number of births by county in North Carolina in over the period 1970 through 1974 (BIR74). The data are available as a shapefile as part of the {sf} package system file. Use the sf::st_read() function together with the system.file() function to import the data. Then create a map filling by the BIR74 variable. nc.sf &lt;- sf::st_read(system.file(&quot;shape/nc.shp&quot;, package = &quot;sf&quot;)) ## Reading layer `nc&#39; from data source ## `/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/sf/shape/nc.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 100 features and 14 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 ## Geodetic CRS: NAD27 ggplot(data = nc.sf) + geom_sf(mapping = aes(fill = BIR74)) Next construct a 20 by 10 grid of polygons that overlap the state using the sf::st_make_grid() function. The function takes the bounding box from the nc.sf simple feature data frame and constructs a two-dimension grid using the dimensions specified with the n = argument. g.sfc &lt;- sf::st_make_grid(nc.sf, n = c(20, 10)) ggplot() + geom_sf(data = g.sfc, col = &quot;red&quot;) + geom_sf(data = nc.sf, fill = &quot;transparent&quot;) The result is overlapping but incongruent sets of polygons as a sfc (simple feature column). Then you use the sf::st_interpolate_aw() function with the first argument a simple feature data frame for which you want to aggregate a particular variable and the argument to = to the set of polygons for which you want the variable to be aggregated. The name of the variable must be put in quotes inside the subset operator []. The argument extensive = if FALSE (default) assumes the variable is spatially intensive (like population density) and the mean is preserved. a1.sf &lt;- sf::st_interpolate_aw(nc.sf[&quot;BIR74&quot;], to = g.sfc, extensive = FALSE) ## Warning in st_interpolate_aw.sf(nc.sf[&quot;BIR74&quot;], to = g.sfc, extensive = FALSE): ## st_interpolate_aw assumes attributes are constant or uniform over areas of x The result is a simple feature data frame with the same polygons geometry as the sfc grid and a single variable called (BIR74). ( p1 &lt;- ggplot() + geom_sf(data = a1.sf, mapping = aes(fill = BIR74)) + scale_fill_continuous(limits = c(0, 18000)) + labs(title = &quot;Intensive&quot;) ) Note that the average number of births across the state at the county level matches (roughly) the average number of births across the grid of polygons, but the sums do not match. mean(a1.sf$BIR74) / mean(nc.sf$BIR74) ## [1] 1.040669 sum(a1.sf$BIR74) / sum(nc.sf$BIR74) ## [1] 1.436123 An intensive variable is independent of the spatial units (e.g., population density, percentages); a variable that has been normalized in some fashion. An extensive variable depends on the spatial unit (e.g., population totals). Assuming a uniform population density, the number of people will depend on the size of the spatial area. Since the number of births in each county is an extensive variable, we change the extensive = argument to TRUE. a2.sf &lt;- sf::st_interpolate_aw(nc.sf[&quot;BIR74&quot;], to = g.sfc, extensive = TRUE) ## Warning in st_interpolate_aw.sf(nc.sf[&quot;BIR74&quot;], to = g.sfc, extensive = TRUE): ## st_interpolate_aw assumes attributes are constant or uniform over areas of x ( p2 &lt;- ggplot(a2.sf) + geom_sf(mapping = aes(fill = BIR74)) + scale_fill_continuous(limits = c(0, 18000)) + labs(title = &quot;Extensive&quot;) ) In this case you preserve the total number of births across the domain. You verify this ‘mass preservation’ property (pycnophylactic property) with a ratio of one. sum(a2.sf$BIR74) / sum(nc.sf$BIR74) ## [1] 1 Here you create a plot of both interpolations. library(patchwork) p1 / p2 Example: tornado paths and housing units Here you are interested in the number of houses (housing units) affected by tornadoes occurring in Florida 2014-2020. You begin by creating a polygon geometry for each tornado record. Import the data, transform the native CRS to 3857 (pseudo-Mercator), and filter on yr (year) and st (state). FL_Torn.sf &lt;- Torn.sf |&gt; sf::st_transform(crs = 3857) |&gt; dplyr::filter(yr &gt;= 2014, st == &quot;FL&quot;) Next change the geometries from line strings to polygons to represent the tornado path (‘footprint’). The path width is given by the variable labeled wid. First we create new a new variable with the width in units of meters and then use the st_buffer() function with the dist = argument set to 1/2 the width. FL_Torn.sf &lt;- FL_Torn.sf |&gt; dplyr::mutate(Width = wid * .9144) FL_TornPath.sf &lt;- FL_Torn.sf |&gt; sf::st_buffer(dist = FL_Torn.sf$Width / 2) To see the change from line string track to polygon path plot both together for one of the tornadoes. ggplot() + geom_sf(data = FL_TornPath.sf[10, ]) + geom_sf(data = FL_Torn.sf[10, ], col = &quot;red&quot;) Now you want the number of houses within the path. The housing units are from the census data. You can access these data with the tidycensus::get_acs() function. The {tidycensus} package is an interface to the decennial US Census and American Community Survey APIs and the US Census Bureau’s geographic boundary files. Functions return Census and ACS data as simple feature data frames for all Census geographies. Note: You need to get an API key from U.S. Census. Then file.create(&quot;CensusAPI&quot;) # open then copy/paste your API key To ensure the file is only readable by you, not by any other user on the system use the function Sys.chmod() then read the key and install it. Sys.chmod(&quot;CensusAPI&quot;, mode = &quot;0400&quot;) key &lt;- readr::read_file(&quot;CensusAPI&quot;) tidycensus::census_api_key(key, install = TRUE, overwrite = TRUE) readRenviron(&quot;~/.Renviron&quot;) Make sure the file is listed in the file .gitignore so it doesn’t get included in your git public repository. The geometry is the tract level and the variable is the un-weighted sample housing units (B00002_001). Transform the CRS to that of the tornadoes. Census.sf &lt;- tidycensus::get_acs(geography = &quot;tract&quot;, variables = &quot;B00002_001&quot;, state = &quot;FL&quot;, year = 2015, geometry = TRUE) |&gt; sf::st_transform(crs = sf::st_crs(FL_TornPath.sf)) ## Getting data from the 2011-2015 5-year ACS ## Downloading feature geometry from the Census website. To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`. ## | | | 0% | |= | 1% | |= | 2% | |== | 2% | |== | 3% | |=== | 4% | |=== | 5% | |==== | 5% | |==== | 6% | |===== | 7% | |===== | 8% | |====== | 8% | |====== | 9% | |======= | 11% | |========= | 12% | |=========== | 15% | |=========== | 16% | |============ | 17% | |============== | 20% | |=============== | 22% | |================ | 23% | |================= | 25% | |=================== | 27% | |==================== | 28% | |==================== | 29% | |======================= | 33% | |======================== | 34% | |========================= | 36% | |============================ | 40% | |============================= | 42% | |============================== | 43% | |=============================== | 45% | |==================================== | 52% | |====================================== | 55% | |======================================= | 56% | |======================================== | 57% | |========================================= | 58% | |========================================= | 59% | |========================================== | 60% | |=========================================== | 61% | |=========================================== | 62% | |============================================ | 63% | |============================================ | 64% | |============================================= | 64% | |============================================== | 65% | |============================================== | 66% | |=============================================== | 67% | |================================================ | 68% | |================================================ | 69% | |================================================= | 70% | |================================================== | 71% | |================================================== | 72% | |=================================================== | 73% | |==================================================== | 74% | |===================================================== | 75% | |===================================================== | 76% | |====================================================== | 77% | |======================================================= | 78% | |======================================================= | 79% | |======================================================== | 80% | |======================================================== | 81% | |========================================================= | 82% | |========================================================== | 82% | |========================================================== | 83% | |============================================================ | 85% | |============================================================ | 86% | |============================================================= | 87% | |============================================================= | 88% | |=============================================================== | 90% | |================================================================= | 92% | |================================================================== | 95% | |======================================================================| 100% head(Census.sf) ## Simple feature collection with 6 features and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -9527976 ymin: 3452498 xmax: -9168692 ymax: 3522281 ## Projected CRS: WGS 84 / Pseudo-Mercator ## GEOID NAME variable estimate ## 1 12001001201 Census Tract 12.01, Alachua County, Florida B00002_001 112 ## 2 12001001519 Census Tract 15.19, Alachua County, Florida B00002_001 99 ## 3 12001001520 Census Tract 15.20, Alachua County, Florida B00002_001 85 ## 4 12001002207 Census Tract 22.07, Alachua County, Florida B00002_001 137 ## 5 12001002218 Census Tract 22.18, Alachua County, Florida B00002_001 111 ## 6 12005000805 Census Tract 8.05, Bay County, Florida B00002_001 159 ## geometry ## 1 MULTIPOLYGON (((-9171497 34... ## 2 MULTIPOLYGON (((-9171172 34... ## 3 MULTIPOLYGON (((-9171771 34... ## 4 MULTIPOLYGON (((-9177078 34... ## 5 MULTIPOLYGON (((-9175225 34... ## 6 MULTIPOLYGON (((-9527976 35... The column labeled estimate is the estimate of the number of housing units within the census tract. Finally you use the sf::st_interpolate_aw() function to spatially interpolate the housing units to the tornado path. awi.sf &lt;- sf::st_interpolate_aw(Census.sf[&quot;estimate&quot;], to = FL_TornPath.sf, extensive = TRUE) ## Warning in st_interpolate_aw.sf(Census.sf[&quot;estimate&quot;], to = FL_TornPath.sf, : ## st_interpolate_aw assumes attributes are constant or uniform over areas of x head(awi.sf) ## Simple feature collection with 6 features and 1 field ## Attribute-geometry relationship: 0 constant, 1 aggregate, 0 identity ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -9502241 ymin: 3489409 xmax: -9452129 ymax: 3606099 ## Projected CRS: WGS 84 / Pseudo-Mercator ## estimate geometry ## 1 1.801417e-02 POLYGON ((-9481985 3545304,... ## 2 3.493061e-05 POLYGON ((-9452129 3489418,... ## 3 3.493061e-05 POLYGON ((-9452129 3504835,... ## 4 6.396930e-03 POLYGON ((-9459939 3509978,... ## 5 2.001489e-05 POLYGON ((-9452129 3521558,... ## 6 5.666174e-02 POLYGON ((-9499599 3606097,... range(awi.sf$estimate, na.rm = TRUE) ## [1] 0.0000 175.6654 The tornado that hit the most houses occurred just east of downtown Orlando. awi.sf2 &lt;- awi.sf |&gt; dplyr::filter(estimate &gt; 175) tmap::tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing tmap::tm_shape(awi.sf2) + tmap::tm_borders() "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
