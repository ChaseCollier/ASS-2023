[["thursday-november-17-2022.html", "Thursday November 17, 2022 Spatial logistic regression Spatial data interpolation Computing the sample variogram", " Thursday November 17, 2022 “Beyond basic mathematical aptitude, the difference between good programmers and great programmers is verbal ability.” – Marissa Mayer Announcement Starting today all remaining lessons will be via Zoom. I will start the lesson at 8 a.m. and record it so you can watch it live or later at your convenience. Spatial logistic regression Spatial data interpolation Computing the sample (empirical) variogram Spatial logistic regression Spatial logistic regression is a popular model for point pattern data. The study domain is divided into a grid of cells; each cell is assigned the value one if it contains at least one event, and zero otherwise. Then a logistic regression is a model for the presence probability \\(p = P(Y = 1)\\) as a function of explanatory variables \\(X\\) in the form \\[ \\log \\frac{p}{1-p} = \\beta X \\] where the left-hand side is the logit (log of the odds ratio) and the \\(\\beta\\) are the coefficients on the explanatory variables to be determined. If your data are stored as ppp objects, a spatial logistic model can be fit directly using functions from the {spatstat} package. Let’s look at an example from the package (a good strategy when learning a new technique). Consider the locations of 57 copper ore deposits (events) and 146 line segments representing geological ‘lineaments.’ Lineaments are geological fault lines. Interest centers on being able to predict the probability of a copper ore from the distance to the fault line. The data are stored as a list in copper. The list contains a ppp object for the ore deposits and a psp object for the lineaments. library(spatstat) |&gt; suppressMessages() data(copper) copper$SouthPoints |&gt; plot() copper$SouthLines |&gt; plot(add = TRUE) First rotate the events (points and lines) by 90 degrees in the anticlockwise direction and save them as separate objects. C &lt;- rotate(copper$SouthPoints, pi/2) L &lt;- rotate(copper$SouthLines, pi/2) C |&gt; plot() L |&gt; plot(add = TRUE) You summarize the planar point pattern data object C. C |&gt; summary() ## Planar point pattern: 57 points ## Average intensity 0.01020691 points per square km ## ## Coordinates are given to 2 decimal places ## i.e. rounded to the nearest multiple of 0.01 km ## ## Window: rectangle = [-158.233, -0.19] x [-0.335, 35] km ## (158 x 35.34 km) ## Window area = 5584.45 square km ## Unit of length: 1 km There are 57 ore deposits over a region of size 5584 square km resulting in an intensity of about .01 ore deposits per square km. Next you create a distance map indicating the distance to the nearest fault line. This distance will be used as a covariate for a model of the probability of an ore deposit. D &lt;- L |&gt; distmap() D |&gt; plot() Spatial logistic regression models are fit with the slrm() function from the {spatstat} family of packages. model.slr &lt;- slrm(C ~ D) model.slr ## Fitted spatial logistic regression model ## Formula: C ~ D ## Fitted coefficients: ## (Intercept) D ## -4.72337865 0.07811134 The model says that the odds of a copper ore deposit along a lineament (D = 0) is exp(-4.723) = .00888. This is slightly less than the overall intensity of .01. The model also says that for every one unit (one kilometer) increase in distance from a lineament the expected change in the log odds is .0781 [exp(.0781) = 1.0812] or an 8.1% increase in the odds. Ore deposits are more likely between the lineaments. The fitted method produces an image (raster) of the window giving the local probability of an ore deposit. Values are the probability of finding an ore deposit in each pixel. model.slr |&gt; fitted() |&gt; plot() C |&gt; plot(add = TRUE) Integrating (summing) the predictions over the domain area equals the abundance (number of ore deposits). model.slr |&gt; fitted() |&gt; sum() ## [1] 57 See https://r-spatial.org/book/12-Interpolation.html Spatial data interpolation In situ observations of the natural world are made at specific locations in space (and time). But you often want estimates of the values everywhere. The temperature reported at the airport is 15C, but what is it at my house 10 miles away? Interpolation of the observed to values across the domain is needed. You must assume the observations are taken from a continuous field (surface). Data observed or measured at locations across a continuous field are called geostatistical data. Examples: concentrations of heavy metals across a farm field, surface air pressures in cities across the country, air temperatures within a city during the night. Local averaging, spline functions, or inverse-distance weighting are interpolation methods. If it is 20C five miles north of here and 30C files miles to the south, then it is 25C here. Key idea: These types of interpolation fail to take into account spatial autocorrelation and they do not contain estimates of uncertainty. Kriging is statistical spatial interpolation. It is the centerpiece of what is called ‘geostatistics.’ The resulting surface (kriged surface) has three parts. (1) Spatial trend: an increase or decrease in the values that depends on direction or a covariate (co-kriging); (2) Local spatial autocorrelation. (3) Random variation. This should now sound familiar. Together the three components provide a model that is used to estimate values everywhere within a specified domain. In short, geostatistics is used to quantify spatial correlation, predict values at locations where values were not observed, estimate uncertainty on the predicted values, and simulate data. As you’ve done with areal data (Moran’s I) and point pattern data (Ripley’s K), you will begin with quantifying spatial autocorrelation. First some definitions. Statistical interpolation assumes the observed values are spatially homogeneous. This implies stationarity and continuity Stationarity means that the average difference in values between pairs of observations separated by a given distance (lag) is constant across the domain Continuity means that the spatial autocorrelation depends only on the lag (and orientation) between observations. That is; spatial autocorrelation is independent of location and can be described by a single function Stationarity and continuity allow different parts of the region to be treated as “independent” samples Stationarity can be weak or intrinsic. Both assume the average of the difference in values at observations separated by a lag distance \\(h\\) is zero. That is, E\\([z_i - z_j]\\) = 0, where location \\(i\\) and location \\(j\\) are a (lag) distance \\(h\\) apart. This implies that the interpolated surface \\(Z(s)\\) is a random function with a constant mean (\\(m\\)) and a residual (\\(z\\)). \\[ Z(s) = m + z(s). \\] The expected value (average across all values) in the domain is \\(m\\) and the residuals are spatially correlated. Weak stationarity assumes that the covariance is a function of lag distance \\(h\\). \\[ \\hbox{cov}(z_i, z_j) = \\hbox{cov}(h) \\] where cov(\\(h\\)) is called the covariogram and \\(z_i\\) and \\(z_j\\) are residuals at different locations in \\(s\\). Intrinsic stationarity assumes the variance of the difference in values is a function of the lag distance. \\[ \\hbox{var}(z_i - z_j) = \\gamma(h), \\] where \\(\\gamma(h)\\) is called the variogram. This means that spatial autocorrelation is independent of location. These assumptions are needed to get started with statistical interpolation. If the assumptions are not met, we remove the trends in the data before spatially interpolating the residuals. Computing the covariogram and the correlogram In practice you focus on fitting a model to the variogram \\(\\gamma(h)\\). But to understand the variogram it helps to first consider the covariogram. This is because you are likely familiar with the idea of nearby things being more correlated than things farther away. To make things simple but without loss in generality, start with a 4 x 6 grid of equally spaced surface air temperatures across a field in degrees C. 21 21 20 19 18 19 26 25 26 27 29 28 32 33 34 35 30 28 34 35 35 36 32 31 Put the values into a data vector and determine the mean and variance. temps &lt;- c(21, 21, 20, 19, 18, 19, 26, 25, 26, 27, 29, 28, 32, 33, 34, 35, 30, 28, 34, 35, 35, 36, 32, 31) temps |&gt; mean() ## [1] 28.08333 temps |&gt; var() ## [1] 34.60145 Focusing only on the covariance function in the north-south direction, to compute the sample covariance function you first compute the covariance between the observed values one distance unit apart. Using math \\[ \\hbox{cov}(0, 1) = 1/|N(1)| \\sum (z_i - Z)(z_j - Z) \\] where \\(|N(1)|\\) is the number of distinct observation pairs with a distance separation of one unit in the north-south direction and where \\(Z\\) is the average over all observations. We let zero in cov(0, 1) refer to the direction and we let one refer to the distance one unit apart. With this grid of observations \\(|N(1)|\\) = 18. The equation for the covariance can be simplified to \\[ \\hbox{cov}(0, 1) = 1/|N(1)| \\sum z_i z_j - m_{-1} m_{+1} \\] where \\(m_{-1}\\) is the average temperature over all rows except the first (northern most) and \\(m_{+1}\\) is the average temperature over all rows except the last (southern most). To simplify the notation re-index the grid of temperatures using lexicographic (reading) order. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Then mp1 &lt;- mean(temps[1:18]) mm1 &lt;- mean(temps[7:24]) cc &lt;- sum(temps[1:18] * temps[7:24])/18 cc - mm1 * mp1 ## [1] 15.01852 Or more generally N &lt;- 18 k &lt;- 1:N 1/N * sum(temps[k] * temps[k + 6]) - mean(temps[k]) * mean(temps[k + 6]) ## [1] 15.01852 The covariance has units of the observed variable squared (here \\(^\\circ C^2\\)). You also have observation pairs two units of distance apart. So you compute the cov(0, 2) in a similar way. \\[ \\hbox{cov}(0, 2) = 1/|N(2)| \\sum z_i z_j - m_{-2} m_{+2} \\] where \\(m_{-2}\\) is the average temperature over all rows except the first two and \\(m_{+2}\\) is the average temperature over all rows except the last two. \\(|N(2)|\\) is the number of pairs two units apart. N &lt;- 12 k &lt;- 1:N 1/N * sum(temps[k] * temps[k + 12]) - mean(temps[k]) * mean(temps[k + 12]) ## [1] 2.9375 Similarly you have observation pairs three units apart so you compute cov(0, 3) as \\[ \\hbox{cov}(0, 3) = 1/|N(3)| \\sum z_i z_j - m_{-3} m_{+3} \\] N &lt;- 6 k &lt;- 1:N 1/N * sum(temps[k] * temps[k + 18]) - mean(temps[k]) * mean(temps[k + 18]) ## [1] 0.9444444 There are no observation pairs four units apart in the north-south direction so you are finished. The covariogram is a plot of the covariance values as a function of lag distance. Let \\(h\\) be the lag distance, then \\(h\\) cov(\\(h\\)) (0, 1) 15 (0, 2) 3 (0, 3) 1 It is convenient to have a measure of co-variability that is dimensionless. So you divide the covariance at lag distance \\(h\\) by the covariance at lag zero. This is the correlogram. Values of the correlogram range from 0 to +1. The covariogram is a decreasing function of lag distance. The variogram is the inverse (multiplicative) of the covariogram. Mathematically: var(\\(z_i - z_j\\)) for locations \\(i\\) and \\(j\\). The semivariogram is 1/2 the variogram. If location \\(i\\) is near location \\(j\\), the difference in the values will be small and so too will the variance of their differences, in general. If location \\(i\\) is far from location \\(j\\), the difference in values will be large and so too will the variance of their differences. In practice you have a set of observations and you compute a variogram. This is called the sample (empirical) variogram. Let \\(t_i = (x_i, y_i)\\) be the ith location and \\(h_{i,j} = t_j - t_i\\) be the vector connecting location \\(t_i\\) with location \\(t_j\\). Then the sample variogram is defined as \\[ \\gamma(h) = \\frac{1}{2N(h)} \\sum^{N(h)} (z_i - z_j)^2 \\] where \\(N(h)\\) is the number of observation pairs a distance of \\(h\\) units apart. The sample variogram assumes intrinsic stationarity so the raw observed values should not have a trend. If there is a trend in the observed value it needs to be removed before computing the variogram. The sample variogram is characterized by a set of points the values of which generally increase as \\(h\\) increases before leveling off (reaching a plateau). As an example, you compute and plot the sample variogram from the meuse.all data frame from the {gstat} package. First attach the data frame and look at the first six rows. library(gstat) ## ## Attaching package: &#39;gstat&#39; ## The following object is masked from &#39;package:spatstat.core&#39;: ## ## idw data(meuse.all) head(meuse.all) ## sample x y cadmium copper lead zinc elev dist.m om ffreq soil ## 1 1 181072 333611 11.7 85 299 1022 7.909 50 13.6 1 1 ## 2 2 181025 333558 8.6 81 277 1141 6.983 30 14.0 1 1 ## 3 3 181165 333537 6.5 68 199 640 7.800 150 13.0 1 1 ## 4 4 181298 333484 2.6 81 116 257 7.655 270 8.0 1 2 ## 5 5 181307 333330 2.8 48 117 269 7.480 380 8.7 1 2 ## 6 6 181390 333260 3.0 61 137 281 7.791 470 7.8 1 2 ## lime landuse in.pit in.meuse155 in.BMcD ## 1 1 Ah FALSE TRUE FALSE ## 2 1 Ah FALSE TRUE FALSE ## 3 1 Ah FALSE TRUE FALSE ## 4 0 Ga FALSE TRUE FALSE ## 5 0 Ah FALSE TRUE FALSE ## 6 0 Ga FALSE TRUE FALSE The data are the locations and heavy metal concentrations (ppm) in the soil at those locations along with other soil and landscape variables collected in a flood plain of the river Meuse, near the village Stein, NL. Heavy metal concentrations are bulk sampled from an area of approximately 15 m x 15 m. Next locate where the data are from. First convert the data frame to a spatial data frame and then use functions from the {tmap} package in view mode. meuse.sf &lt;- meuse.all |&gt; sf::st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;), crs = 28992) tmap::tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing tmap::tm_shape(meuse.sf) + tmap::tm_bubbles(size = &quot;zinc&quot;) ## Legend for symbol sizes not available in view mode. Then compute the sample variogram and save it as meuse.v. meuse.v &lt;- variogram(zinc ~ 1, data = meuse.all, locations = ~ x + y) meuse.v |&gt; class() ## [1] &quot;gstatVariogram&quot; &quot;data.frame&quot; The output is an object of class gstatVariogram and data.frame. Make a graph of the sample variogram and label the key features using functions from the {ggplot2} package. Map the variable gamma in the meuse.v data frame to the y aesthetic and the variable dist to the x aesthetic. library(ggplot2) ggplot(data = meuse.v, mapping = aes(x = dist, y = gamma)) + geom_point(size = 2) + scale_y_continuous(limits = c(0, 210000)) + geom_hline(yintercept = c(30000, 175000), color = &quot;red&quot;) + geom_vline(xintercept = 800, color = &quot;red&quot;) + xlab(&quot;Lag distance (h)&quot;) + ylab(expression(paste(gamma,&quot;(h)&quot;))) + geom_segment(aes(x = 0, y = 0, xend = 0, yend = 30000,), arrow = arrow(angle = 15, length = unit(.3, &quot;cm&quot;))) + geom_label(aes(x = 100, y = 10000, label = &quot;nugget&quot;)) + geom_segment(aes(x = 0, y = 10000, xend = 0, yend = 175000,), arrow = arrow(angle = 15, length = unit(.3, &quot;cm&quot;))) + geom_label(aes(x = 180, y = 150000, label = &quot;sill (partial sill)&quot;)) + geom_segment(aes(x = 0, y = 190000, xend = 800, yend = 190000,), arrow = arrow(angle = 15, length = unit(.3, &quot;cm&quot;))) + geom_label(aes(x = 250, y = 190000, label = &quot;range&quot;)) + theme_minimal() Lag (lag distance): Relative distance between observation locations (here units: meters) Nugget (nugget, nugget variance, or nugget effect): The height of the variogram at zero lag (here units ppm squared). The nugget is the variation in the values at the observation locations independent of spatial variation. It is related to the observation (or measurement) precision Sill: The height of the variogram at which the values are uncorrelated Relative nugget effect: The ratio of the nugget to the sill expressed as a percentage Range: The distance beyond which the values are uncorrelated. The range is indicated on the sample variogram as the position along the horizontal axis where values of the variogram reach a constant height Additional terms. - Isotropy: The condition in which spatial correlation is the same in all directions - Anisotropy: (an-I-so-trop-y) spatial correlation is stronger or more persistent in some directions - Directional variogram: Distance and direction are important in characterizing the spatial correlations. Otherwise the variogram is called omni-directional - Azimuth (\\(\\theta\\)): Defines the direction of the variogram in degrees. The azimuth is measured clockwise from north - Lag spacing: The distance between successive lags is called the lag spacing or lag increment - Lag tolerance: The distance allowable for observational pairs at a specified lag. With arbitrary observation locations there will be no observations exactly a lag distance from any observation. Lag tolerance provides a range of distances to be used for computing values of the variogram at a specified lag. Computing the sample variogram is the first step in modeling geostatistical data. The next step is fitting a model to the variogram. The model is important since the sample variogram estimates are made only at discrete lag distances (with specified lag tolerance and azimuth). You need a continuous function that varies smoothly across all lags. The statistical model replaces the discrete set of points. Variogram models come from different families. The fitting process first requires a decision about what family to choose and then given the family, a decision about what parameters (nugget, sill, range) to choose. An exponential variogram model reaches the sill asymptotically. The range (a) is defined as the lag distance at which gamma reaches 95% of the sill. c0 &lt;- .1 c1 &lt;- 2.1 a &lt;- 1.3 curve(c0 + c1*(1 - exp(-3*x/a)), from = .01, to = 3, xlab = &quot;h&quot;, ylab = expression(paste(hat(gamma), &quot;(h)&quot;)), las = 1) A spherical variogram model reaches the sill at x = 1. curve(c0 + c1*(3*x/2 - x^3/2), from = .01, to = 1, xlab = &quot;h&quot;, ylab = expression(paste(hat(gamma), &quot;(h)&quot;)), las = 1) A Gaussian variogram model is “S”-shaped (sigmodial). It is used when the data exhibit strong correlations at the shortest lag distances. The inflection point of the model occurs at \\(\\sqrt{a/6}\\). curve(c0 + c1*(1 - exp(-3*x^2/a^2)), from = .01, to = 3, xlab = &quot;h&quot;, ylab = expression(paste(hat(gamma), &quot;(h)&quot;)), las = 1) Other families include Linear models: \\(\\hat \\gamma(h)\\) = c0 + b * h. Power models: \\(\\hat \\gamma(h)\\) = c0 + b * h\\(^\\lambda\\). These models have no sill. Choosing a variogram family is largely done by looking at the shape of the sample variogram. Then, given a sample variogram computed from a set of spatial observations and a choice of family, the parameters of the variogram model are determined by weighted least-squares (WLS). Weighting is needed because the because the sample variogram estimates are computed using a varying number of point pairs. There are other ways to determine the parameters including by eye, and by the method of maximum likelihoods, but WLS is less erratic than other methods and it requires fewer assumptions about the distribution of the data. And the process can be automated and it often is in high-level packages, but it is important to understand what is in the black box. The final step in spatial statistical interpolation is called kriging. Kriging interpolates the observed data using the variogram model. It was developed by a South African miner (D.G. Krige) as a way to improve estimates of where ore reserves might be located. Extraction costs are reduced substantially if good predictions can be made of where the ore resides given samples taken across the mine. A kriged estimate is a weighted average of the observations where the weights are based on the variogram model. The kriged estimates are optimal in the sense that they minimize the error between observed and predicted. The type of kriging depends on the characteristics of the observations and the purpose of interpolation. Simple kriging assumes a known constant mean for the domain Ordinary kriging assumes an unknown constant mean Universal kriging assumes an unknown linear or nonlinear trend in the mean To review, the steps for spatial interpolation (statistical) are: Examine the observations for trends and isotropy Compute a sample (empirical) variogram Fit a variogram model to the sample variogram Create an interpolated surface using the variogram model together with the data (kriging) Computing the sample variogram The {gstat} package contains functions for spatial interpolation that take advantage of simple feature (and S4 class) spatial data data frames. Suppose we have the following set of observations (zobs) at locations (sx, sy). sx &lt;- c(1.1, 3.2, 2.1, 4.9, 5.5, 7, 7.8, 9, 2.3, 6.9) sy &lt;- c(3, 3.5, 6, 1.5, 5.5, 3.2, 1, 4.5, 1, 7) zobs &lt;- c(-0.6117, -2.4232, -0.42, -0.2522, -2.0362, 0.9814, 1.842, 0.1723, -0.0811, -0.3896) Create a data frame and plot the observed values at the locations using the geom_text() function. sf &lt;- data.frame(sx, sy, zobs) |&gt; sf::st_as_sf(coords = c(&quot;sx&quot;, &quot;sy&quot;), crs = 4326) ggplot(data = sf, mapping = aes(x = sx, y = sy, label = zobs)) + geom_text() + theme_minimal() Lag distance (distance between locations) is the independent variable in the variogram function. You get all pairwise distances by applying the dist() function to a matrix of spatial coordinates. cbind(sx, sy) |&gt; dist() ## 1 2 3 4 5 6 7 8 ## 2 2.158703 ## 3 3.162278 2.731300 ## 4 4.085340 2.624881 5.300000 ## 5 5.060632 3.047950 3.436568 4.044750 ## 6 5.903389 3.811824 5.643580 2.701851 2.745906 ## 7 6.992138 5.235456 7.582216 2.942788 5.053712 2.340940 ## 8 8.041144 5.885576 7.061161 5.080354 3.640055 2.385372 3.700000 ## 9 2.332381 2.657066 5.003998 2.647640 5.521775 5.189412 5.500000 7.559100 ## 10 7.045566 5.093133 4.903060 5.852350 2.051828 3.801316 6.067125 3.264966 ## 9 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 7.560423 cbind(sx, sy) |&gt; dist() |&gt; range() ## [1] 2.051828 8.041144 The dist() function computes a pairwise distance matrix. The distance between the first and second observation is 2.16 units and so on. The largest lag distance is 8.04 units and the smallest lag distance is 2.05 units. The functions in the {gstat} package work with simple feature objects. As another example, consider the dataset called topo from the {MASS} package. The data are topographic heights (feet) within a 310 sq ft domain. Examine the data with a series of plots. topo.df &lt;- MASS::topo p1 &lt;- ggplot(data = topo.df, mapping = aes(x = x, y = y, color = z)) + geom_point() + scale_color_viridis_c() + theme_minimal() p2 &lt;- ggplot(data = topo.df, mapping = aes(x = z, y = y)) + geom_point() + geom_smooth(method = lm, se = FALSE) + theme_minimal() p3 &lt;- ggplot(data = topo.df, mapping = aes(x = x, y = z)) + geom_point() + geom_smooth(method = lm, se = FALSE) + theme_minimal() p4 &lt;- ggplot(data = topo.df, mapping = aes(x = z)) + geom_histogram(bins = 13) + theme_minimal() library(patchwork) ## ## Attaching package: &#39;patchwork&#39; ## The following object is masked from &#39;package:spatstat.geom&#39;: ## ## area ( p1 + p2 ) / ( p3 + p4 ) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; Note the trend in the north-south direction and the skewness in the observed values. Examine the residuals after removing a first-order trend from the observations. topo.df$z1 &lt;- lm(z ~ x + y, data = topo.df) |&gt; residuals() p1 &lt;- ggplot(data = topo.df, mapping = aes(x = x, y = y, color = z1)) + geom_point() + scale_color_viridis_c() + theme_minimal() p2 &lt;- ggplot(data = topo.df, mapping = aes(x = z1, y = y)) + geom_point() + geom_smooth(method = lm, se = FALSE) + theme_minimal() p3 &lt;- ggplot(data = topo.df, mapping = aes(x = x, y = z1)) + geom_point() + geom_smooth(method = lm, se = FALSE) + theme_minimal() p4 &lt;- ggplot(data = topo.df, mapping = aes(x = z1)) + geom_histogram(bins = 13) + theme_minimal() ( p1 + p2 ) / ( p3 + p4 ) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; The north-south trend is removed and the observations have a more symmetric distribution. There appears to be some non-linear trend (U-shape) in the east-west direction. However, the residuals appear to show spatial autocorrelation (areas with above and below residuals). Compare the sample variograms using first the raw values and then the residuals after removing the first-order trend. topo.sf &lt;- topo.df |&gt; sf::st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;)) topo.v1 &lt;- variogram(z ~ 1, data = topo.sf, cutoff = 6) topo.v2 &lt;- variogram(z1 ~ 1, data = topo.sf, cutoff = 6) ggplot(data = topo.v1, mapping = aes(x = dist, y = gamma)) + geom_point(color = &quot;red&quot;) + geom_point(data = topo.v2, mapping = aes(x = dist, y = gamma), color = &quot;black&quot;) + scale_x_continuous(breaks = seq(0, 6, by = 1)) + xlab(&quot;Lag distance (h)&quot;) + ylab(expression(paste(gamma,&quot;(h)&quot;))) + theme_minimal() The semivariance (\\(\\gamma(u)\\)) is plotted against lag distance (\\(h\\)). Values increase with lag out to a distance of 3 from the sample variogram computed with the detrended data (black dots) but continue to increase from the sample variogram computed with the raw data (red dots). For short lag distances, the increasing variogram values are due to spatial autocorrelation but at large distances the increasing variogram values are due to trend. That is why you need to remove trends before computing and modeling the variogram. The units of the variogram values are squared units of the variable units (here square feet). Values are calculated using observational point pairs at lag distances within a lag tolerance. The number of point pairs depends on the lag. At short and long distances there are fewer observational point pairs, so the sample variogram values have greater variance for short and long lag distances. Plot the number of observational point pairs used to calculate the variogram. ggplot(data = topo.v2, mapping = aes(y = np, x = dist)) + geom_point() + xlab(&quot;Lag Distance&quot;) + ylab(&quot;Number of Observation Pairs&quot;) + theme_minimal() Summary: Modeling point pattern data is not straightforward so often the data are aggregated to areal units and spatial logistic regression is used to indicate the presence or absence of events at a particular scale. The scale is chosen so as to effectively remove clustering or inhibition processes. You saw how to implement a spatial logistic regression model with functions from the {spatstat} family of packages. The topic of spatial interpolation was introduced. There are many ways to interpolate. Kriging provides a statistical methodology that is useful for estimating uncertainty about the interpolated values and for simulation. The first steps are to examine the data for trends and compute the sample variogram on the residuals. If there is spatial autocorrelation in the residuals then the sample variogram values increase with lag distance out to some range. The next steps are fitting a variogram model to the sample variogram and creating an interpolated surface with the method of kriging. Kriging combines the variogram model with the observations. Class next Tuesday, November 22 will be via Zoom. I will start at 8a and record it so you can watch it live or at your convenience. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
