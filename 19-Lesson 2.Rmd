# Thursday April 6, 2023 {-}

**"Beyond basic mathematical aptitude, the difference between good programmers and great programmers is verbal ability."** â€“ Marissa Mayer

## Spatial data interpolation {-}

In situ observations of the natural world are made at specific locations in space (and time). But you often want estimates of the values everywhere. The temperature reported at the airport is 15C, but what is it at my house 10 miles away? 

Interpolation of the observed to values across the domain is needed.  You must assume the observations are taken from a _continuous_ field (surface). Data observed or measured at locations across a continuous field are called geostatistical data. Examples: concentrations of heavy metals across a farm field, surface air pressures in cities across the country, air temperatures within a city during the night.

Local averaging, spline functions, or inverse-distance weighting are interpolation methods. If it is 20C five miles north of here and 30C files miles to the south, then it is 25C here. 

_Key idea_: These types of interpolation fail to take into account spatial autocorrelation and they do not contain estimates of uncertainty.

Kriging is statistical spatial interpolation. It is the centerpiece of what is called 'geostatistics.' The resulting surface (kriged surface) has three parts. (1) Spatial trend: an increase or decrease in the values that depends on direction or a covariate (co-kriging); (2) Local spatial autocorrelation. (3) Random variation. This should now sound familiar. Together the three components provide a model that is used to estimate values everywhere within a specified domain.

In short, geostatistics is used to quantify spatial correlation, predict values at locations where values were not observed, estimate uncertainty on the predicted values, and simulate data.

As you've done with areal data (Moran's I) and point pattern data (Ripley's K), you will begin with quantifying spatial autocorrelation.

First some definitions.

- Statistical interpolation assumes the observed values are spatially homogeneous. This implies stationarity and continuity

- Stationarity means that the average difference in values between pairs of observations separated by a given distance (lag) is constant across the domain

- Continuity means that the spatial autocorrelation depends only on the lag (and orientation) between observations. That is; spatial autocorrelation is independent of location and can be described by a single function

- Stationarity and continuity allow different parts of the region to be treated as "independent" samples

Stationarity can be weak or intrinsic. Both assume the average of the difference in values at observations separated by a lag distance $h$ is zero. That is, E$[z_i - z_j]$ = 0, where location $i$ and location $j$ are a (lag) distance $h$ apart. This implies that the interpolated surface $Z(s)$ is a random function with a constant mean ($m$) and a residual ($z$).

$$
Z(s) = m + z(s).
$$
The expected value (average across all values) in the domain is $m$ and the residuals are spatially correlated.

Weak stationarity assumes that the covariance is a function of lag distance $h$.

$$
\hbox{cov}(z_i, z_j) = \hbox{cov}(h)
$$
where cov($h$) is called the covariogram and $z_i$ and $z_j$ are residuals at different locations in $s$.

Intrinsic stationarity assumes the variance of the difference in values is a function of the lag distance.

$$
\hbox{var}(z_i - z_j) = \gamma(h),
$$
where $\gamma(h)$ is called the variogram. This means that spatial autocorrelation is independent of location.

These assumptions are needed to get started with statistical interpolation. If the assumptions are not met, we remove the trends in the data before spatially interpolating the residuals.

Computing the covariogram and the correlogram

In practice you focus on fitting a model to the variogram $\gamma(h)$. But to understand the variogram it helps to first consider the covariogram. This is because you are likely familiar with the idea of nearby things being more correlated than things farther away.

To make things simple but without loss in generality, start with a 4 x 6 grid of equally spaced surface air temperatures across a field in degrees C.

  21  21  20  19  18  19 
  
  26  25  26  27  29  28 
  
  32  33  34  35  30  28   
  
  34  35  35  36  32  31   

Put the values into a data vector and determine the mean and variance.

```{r}
temps <- c(21, 21, 20, 19, 18, 19, 
           26, 25, 26, 27, 29, 28, 
           32, 33, 34, 35, 30, 28, 
           34, 35, 35, 36, 32, 31)
temps |>
  mean()
temps |>
  var()
```

Focusing only on the covariance function in the north-south direction, to compute the sample covariance function you first compute the covariance between the observed values one distance unit apart. Using math

$$
\hbox{cov}(0, 1) = 1/|N(1)| \sum (z_i - Z)(z_j - Z)
$$

where $|N(1)|$ is the number of distinct observation pairs with a distance separation of one unit in the north-south direction and where $Z$ is the average over all observations. We let zero in cov(0, 1) refer to the direction and we let one refer to the distance one unit apart. With this grid of observations $|N(1)|$ = 18.

The equation for the covariance can be simplified to

$$
\hbox{cov}(0, 1) = 1/|N(1)| \sum z_i z_j - m_{-1} m_{+1}
$$

where $m_{-1}$ is the average temperature over all rows except the first (northern most) and $m_{+1}$ is the average temperature over all rows except the last (southern most).

To simplify the notation re-index the grid of temperatures using lexicographic (reading) order.

 1   2   3   4   5   6
 
 7   8   9   10  11  12 
 
 13  14  15  16  17  18  
  
 19  20  21  22  23  24 

Then

```{r}
mp1 <- mean(temps[1:18])
mm1 <- mean(temps[7:24])
cc <- sum(temps[1:18] * temps[7:24])/18
cc - mm1 * mp1
```

Or more generally
```{r}
N <- 18
k <- 1:N
1/N * sum(temps[k] * temps[k + 6]) - mean(temps[k]) * mean(temps[k + 6])
```

The covariance has units of the observed variable squared (here $^\circ C^2$).

You also have observation pairs two units of distance apart. So you compute the cov(0, 2) in a similar way.

$$
\hbox{cov}(0, 2) = 1/|N(2)| \sum z_i z_j - m_{-2} m_{+2}
$$

where $m_{-2}$ is the average temperature over all rows except the first two and $m_{+2}$ is the average temperature over all rows except the last two. $|N(2)|$ is the number of pairs two units apart.

```{r}
N <- 12
k <- 1:N
1/N * sum(temps[k] * temps[k + 12]) - mean(temps[k]) * mean(temps[k + 12])
```

Similarly you have observation pairs three units apart so you compute cov(0, 3) as

$$
\hbox{cov}(0, 3) = 1/|N(3)| \sum z_i z_j - m_{-3} m_{+3}
$$

```{r}
N <- 6
k <- 1:N
1/N * sum(temps[k] * temps[k + 18]) - mean(temps[k]) * mean(temps[k + 18])
```

There are no observation pairs four units apart in the north-south direction so you are finished. The covariogram is a plot of the covariance values as a function of lag distance. Let $h$ be the lag distance, then

$h$    |  cov($h$)  
-------|--------  
(0, 1) |  15  
(0, 2) |   3  
(0, 3) |   1  

It is convenient to have a measure of co-variability that is dimensionless. So you divide the covariance at lag distance $h$ by the covariance at lag zero. This is the correlogram. Values of the correlogram range from 0 to +1.

The covariogram is a decreasing function of lag distance. The _variogram_ is the inverse (multiplicative) of the covariogram. 

Mathematically: var($z_i - z_j$) for locations $i$ and $j$. The semivariogram is 1/2 the variogram. If location $i$ is near location $j$, the difference in the values will be small and so too will the variance of their differences, in general. If location $i$ is far from location $j$, the difference in values will be large and so too will the variance of their differences.

In practice you have a set of observations and you compute a variogram. This is called the sample (empirical) variogram. Let $t_i = (x_i, y_i)$  be the ith location and $h_{i,j} = t_j - t_i$ be the vector connecting location $t_i$ with location $t_j$. Then the sample variogram is defined as

$$
\gamma(h) = \frac{1}{2N(h)} \sum^{N(h)} (z_i - z_j)^2
$$

where $N(h)$ is the number of observation pairs a distance of $h$ units apart.

The sample variogram assumes intrinsic stationarity so the raw observed values should not have a trend. If there is a trend in the observed value it needs to be removed before computing the variogram.

The sample variogram is characterized by a set of points the values of which generally increase as $h$ increases before leveling off (reaching a plateau).

As an example, you compute and plot the sample variogram from the `meuse.all` data frame from the {gstat} package. First attach the data frame and look at the first six rows.

```{r}
library(gstat)

data(meuse.all)
head(meuse.all)
```

The data are the locations and heavy metal concentrations (ppm) in the soil at those locations along with other soil and landscape variables collected in a flood plain of the river Meuse, near the village Stein, NL. Heavy metal concentrations are bulk sampled from an area of approximately 15 m x 15 m.

Next locate where the data are from. First convert the data frame to a spatial data frame and then use functions from the {tmap} package in view mode.

```{r}
meuse.sf <- meuse.all |>
  sf::st_as_sf(coords = c("x", "y"), 
               crs = 28992)

tmap::tmap_mode("view")

tmap::tm_shape(meuse.sf) +
  tmap::tm_bubbles(size = "zinc")
```

Then compute the sample variogram and save it as `meuse.v`.

```{r}
meuse.v <- variogram(zinc ~ 1,
                     data = meuse.all,
                     locations = ~ x + y)
meuse.v |>
  class()
```

The output is an object of class `gstatVariogram` and `data.frame`. 

Make a graph of the sample variogram and label the key features using functions from the {ggplot2} package. Map the variable `gamma` in the `meuse.v` data frame to the y aesthetic and the variable `dist` to the x aesthetic.

```{r}
library(ggplot2)

ggplot(data = meuse.v,
       mapping = aes(x = dist, y = gamma)) +
  geom_point(size = 2) +
  scale_y_continuous(limits = c(0, 210000)) +
  geom_hline(yintercept = c(30000, 175000), color = "red") +
  geom_vline(xintercept = 800, color = "red") +
  xlab("Lag distance (h)") + ylab(expression(paste(gamma,"(h)"))) +
  geom_segment(aes(x = 0, y = 0, xend = 0, yend = 30000,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = 100, y = 10000, label = "nugget")) +
  geom_segment(aes(x = 0, y = 10000, xend = 0, yend = 175000,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = 180, y = 150000, label = "sill (partial sill)")) +
  geom_segment(aes(x = 0, y = 190000, xend = 800, yend = 190000,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = 250, y = 190000, label = "range")) +
  theme_minimal()
```

- Lag (lag distance): Relative distance between observation locations (here units: meters)
- Nugget (nugget, nugget variance, or nugget effect): The height of the variogram at zero lag (here units ppm squared). The nugget is the variation in the values at the observation locations independent of spatial variation. It is related to the observation (or measurement) precision 
- Sill: The height of the variogram at which the values are uncorrelated
- Relative nugget effect: The ratio of the nugget to the sill expressed as a percentage
- Range: The distance beyond which the values are uncorrelated. The range is indicated on the sample variogram as the position along the horizontal axis where values of the variogram reach a constant height

Additional terms.
- Isotropy: The condition in which spatial correlation is the same in all directions
- Anisotropy: (an-I-so-trop-y) spatial correlation is stronger or more persistent in some directions
- Directional variogram: Distance and direction are important in characterizing the spatial correlations. Otherwise the variogram is called omni-directional
- Azimuth ($\theta$): Defines the direction of the variogram in degrees. The azimuth is measured clockwise from north
- Lag spacing: The distance between successive lags is called the lag spacing or lag increment
- Lag tolerance: The distance allowable for observational pairs at a specified lag. With arbitrary observation locations there will be no observations exactly a lag distance from any observation. Lag tolerance provides a range of distances to be used for computing values of the variogram at a specified lag.

Computing the sample variogram is the first step in modeling geostatistical data. The next step is fitting a model to the variogram. The model is important since the sample variogram estimates are made only at discrete lag distances (with specified lag tolerance and azimuth). You need a continuous function that varies smoothly across all lags. The statistical model replaces the discrete set of points.

Variogram models come from different families. The fitting process first requires a decision about what family to choose and then given the family, a decision about what parameters (nugget, sill, range) to choose. 

An exponential variogram model reaches the sill asymptotically. The range (a) is defined as the lag distance at which gamma reaches 95% of the sill.

```{r}
c0 <- .1
c1 <- 2.1
a <- 1.3
curve(c0 + c1*(1 - exp(-3*x/a)), 
      from = .01, to = 3, 
      xlab = "h", 
      ylab = expression(paste(hat(gamma), "(h)")), 
      las = 1)
```

A spherical variogram model reaches the sill at x = 1.

```{r}
curve(c0 + c1*(3*x/2 - x^3/2),
      from = .01, to = 1,
      xlab = "h",
      ylab = expression(paste(hat(gamma), "(h)")), 
      las = 1)
```

A Gaussian variogram model is "S"-shaped (sigmodial). It is used when the data exhibit strong correlations at the shortest lag distances.  The inflection point of the model occurs at $\sqrt{a/6}$.

```{r}
curve(c0 + c1*(1 - exp(-3*x^2/a^2)),
      from = .01, to = 3, 
      xlab = "h", 
      ylab = expression(paste(hat(gamma), "(h)")),
      las = 1)
```

Other families include

* Linear models: $\hat \gamma(h)$ = c0 + b * h.
* Power models:  $\hat \gamma(h)$ = c0 + b * h$^\lambda$.

These models have no sill.

Choosing a variogram family is largely done by looking at the shape of the sample variogram. Then, given a sample variogram computed from a set of spatial observations and a choice of family, the parameters of the variogram model are determined by weighted least-squares (WLS). Weighting is needed because the because the sample variogram estimates are computed using a varying number of point pairs.

There are other ways to determine the parameters including by eye, and by the method of maximum likelihoods, but WLS is less erratic than other methods and it requires fewer assumptions about the distribution of the data. And the process can be automated and it often is in high-level packages, but it is important to understand what is in the black box.

The final step in spatial statistical interpolation is called kriging. Kriging interpolates the observed data using the variogram model. It was developed by a South African miner (D.G. Krige) as a way to improve estimates of where ore reserves might be located. Extraction costs are reduced substantially if good predictions can be made of where the ore resides given samples taken across the mine.

A kriged estimate is a weighted average of the observations where the weights are based on the variogram model. The kriged estimates are optimal in the sense that they minimize the error between observed and predicted. The type of kriging depends on the characteristics of the observations and the purpose of interpolation.

- Simple kriging assumes a known constant mean for the domain 
- Ordinary kriging assumes an unknown constant mean
- Universal kriging assumes an unknown linear or nonlinear trend in the mean

To review, the steps for spatial interpolation (statistical) are:

1. Examine the observations for trends and isotropy
2. Compute a sample (empirical) variogram
3. Fit a variogram model to the sample variogram
4. Create an interpolated surface using the variogram model together with the data (kriging)

## Computing the sample variogram {-}

The {gstat} package contains functions for spatial interpolation that take advantage of simple feature (and S4 class) spatial data data frames.

Suppose we have the following set of observations (`zobs`) at locations (`sx`, `sy`).

```{r}
sx <- c(1.1, 3.2, 2.1, 4.9, 5.5, 7, 7.8, 9, 2.3, 6.9)
sy <- c(3, 3.5, 6, 1.5, 5.5, 3.2, 1, 4.5, 1, 7)
zobs <- c(-0.6117, -2.4232, -0.42, -0.2522, -2.0362, 0.9814, 1.842,
         0.1723, -0.0811, -0.3896)
```

Create a data frame and plot the observed values at the locations using the `geom_text()` function.
```{r}
sf <- data.frame(sx, sy, zobs) |>
  sf::st_as_sf(coords = c("sx", "sy"),
               crs = 4326)

ggplot(data = sf, 
       mapping = aes(x = sx, y = sy, label = zobs)) +
  geom_text() +
  theme_minimal()
```

Lag distance (distance between locations) is the independent variable in the variogram function. You get all pairwise distances by applying the `dist()` function to a matrix of spatial coordinates.

```{r}
cbind(sx, sy) |>
  dist()

cbind(sx, sy) |>
  dist() |>
  range()
```

The `dist()` function computes a pairwise distance matrix. The distance between the first and second observation is 2.16 units and so on. The largest lag distance is 8.04 units and the smallest lag distance is 2.05 units.

The functions in the {gstat} package work with simple feature objects.

As another example, consider the dataset called `topo` from the {MASS} package. The data are topographic heights (feet) within a 310 sq ft domain.

Examine the data with a series of plots.

```{r}
topo.df <- MASS::topo

p1 <- ggplot(data = topo.df,
             mapping = aes(x = x, y = y, color = z)) +
  geom_point() +
  scale_color_viridis_c() +
  theme_minimal()

p2 <- ggplot(data = topo.df,
             mapping = aes(x = z, y = y)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal()

p3 <- ggplot(data = topo.df,
             mapping = aes(x = x, y = z)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal()

p4 <- ggplot(data = topo.df,
             mapping = aes(x = z)) +
  geom_histogram(bins = 13) +
  theme_minimal()

library(patchwork)
( p1 + p2 ) / ( p3 + p4 )
```

Note the trend in the north-south direction and the skewness in the observed values. 

Examine the residuals after removing a first-order trend from the observations.

```{r}
topo.df$z1 <- lm(z ~ x + y, data = topo.df) |>
  residuals()

p1 <- ggplot(data = topo.df,
             mapping = aes(x = x, y = y, color = z1)) +
  geom_point() +
  scale_color_viridis_c() +
  theme_minimal()

p2 <- ggplot(data = topo.df,
             mapping = aes(x = z1, y = y)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal()

p3 <- ggplot(data = topo.df,
             mapping = aes(x = x, y = z1)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal()

p4 <- ggplot(data = topo.df,
             mapping = aes(x = z1)) +
  geom_histogram(bins = 13) +
  theme_minimal()

( p1 + p2 ) / ( p3 + p4 )
```

The north-south trend is removed and the observations have a more symmetric distribution. There appears to be some non-linear trend (U-shape) in the east-west direction. 

However, the residuals appear to show spatial autocorrelation (areas with above and below residuals). 

Compare the sample variograms using first the raw values and then the residuals after removing the first-order trend.

```{r}
topo.sf <- topo.df |>
  sf::st_as_sf(coords = c("x", "y"))

topo.v1 <- variogram(z ~ 1,
                     data = topo.sf,
                     cutoff = 6)
topo.v2 <- variogram(z1 ~ 1,
                     data = topo.sf,
                     cutoff = 6)

ggplot(data = topo.v1,
       mapping = aes(x = dist, y = gamma)) +
  geom_point(color = "red") +
  geom_point(data = topo.v2,
             mapping = aes(x = dist, y = gamma),
             color = "black") +
  scale_x_continuous(breaks = seq(0, 6, by = 1)) +
  xlab("Lag distance (h)") + 
  ylab(expression(paste(gamma,"(h)"))) +
  theme_minimal()
```

The semivariance ($\gamma(u)$) is plotted against lag distance ($h$). 

Values increase with lag out to a distance of 3 from the sample variogram computed with the detrended data (black dots) but continue to increase from the sample variogram computed with the raw data (red dots). 

For short lag distances, the increasing variogram values are due to spatial autocorrelation but at large distances the increasing variogram values are due to trend. That is why you need to remove trends before computing and modeling the variogram.

The units of the variogram values are squared units of the variable units (here square feet). Values are calculated using observational point pairs at lag distances within a lag tolerance. The number of point pairs depends on the lag. At short and long distances there are fewer observational point pairs, so the sample variogram values have greater variance for short and long lag distances.

Plot the number of observational point pairs used to calculate the variogram.

```{r}
ggplot(data = topo.v2,
       mapping = aes(y = np, x = dist)) +
  geom_point() +
  xlab("Lag Distance") + 
  ylab("Number of Observation Pairs") +
  theme_minimal()
```

Summary: 

Modeling point pattern data is not straightforward so often the data are aggregated to areal units and spatial logistic regression is used to indicate the presence or absence of events at a particular scale. The scale is chosen so as to effectively remove clustering or inhibition processes. You saw how to implement a spatial logistic regression model with functions from the {spatstat} family of packages.

The topic of spatial interpolation was introduced. There are many ways to interpolate. Kriging provides a statistical methodology that is useful for estimating uncertainty about the interpolated values and for simulation. The first steps are to examine the data for trends and compute the sample variogram on the residuals. If there is spatial autocorrelation in the residuals then the sample variogram values increase with lag distance out to some range. 

The next steps are fitting a variogram model to the sample variogram and creating an interpolated surface with the method of kriging. Kriging combines the variogram model with the observations.

Class next Tuesday, November 22 will be via Zoom. I will start at 8a and record it so you can watch it live or at your convenience.
