# Thursday October 6, 2022 {.unnumbered}

**"Give someone a program, you frustrate them for a day; teach them how to program, you frustrate them for a lifetime."** - David Leinweber

Today

- Spatial data as point patterns
- Working with point pattern objects using functions from the {spatstat} package
- Quantifying event intensity

## Spatial data as point patterns {-}

We now turn our attention to analyzing and modeling point pattern data. Starting with some theory, then how to work with functions from the {spatstat} package before focusing on spatial intensity.

We naturally seek to find patterns in a collection of events. The pattern that tends to catch our attention quickly is the grouping of events across space. Stars in the night sky as constellations.  A collection of events in a particular region begs for an explanation. Why do events occur more often in this particular region and not somewhere else?

Consider tornado reports over the past several years in the state of Kansas. Let the start position of a tornado be an _event location_. And let the damage rating (EF scale) provide a _mark_ on the event. Here you consider only events since 2007 with marks of 1, 2, 3, 4, and 5. More about the damage rating scale is available here https://en.wikipedia.org/wiki/Enhanced_Fujita_scale

Import and filter the data accordingly.

```{r}
Torn.sf <- sf::st_read(dsn = here::here("data", "1950-2020-torn-initpoint")) |>
  dplyr::filter(st == "KS", 
                yr >= 2007,
                mag > 0) 
```

Create a map of the event locations using functions from the {tmap} package. The state border is obtained as a simple feature data frame. The polygon geometry is plotted first with `tm_borders()` then the event locations are plotted with the `tm_bubbles()` and `size = "mag"`.

```{r}
KS.sf <- USAboundaries::us_states(states = "Kansas")

tmap::tm_shape(KS.sf) +
   tmap::tm_borders(col = "grey70") +
tmap::tm_shape(Torn.sf) +
   tmap::tm_bubbles(size = "mag", 
                    col = "red",
                    alpha = .4,
                    title.size = "EF Rating") +
tmap::tm_layout(legend.position = c("left", "top"),
                legend.outside = TRUE)
```

Based on this display of tornado genesis locations we ask: (1) Are certain areas of the state more (or less) likely to get a tornado? (2) Do tornadoes tend to cluster? (3) Are there places in the state that are safe from tornadoes?

These questions are similar but they are not identical. We will explore these canonical questions about point pattern data in the next few lessons.

To begin it is helpful to have some definitions. 

* Event: An occurrence of interest (e.g., tornado, accident, wildfire). 
* Event location: Location of event (e.g., latitude/longitude).
* Point: Any location in the study area where an event _could_ occur. Note: Event location is a particular point where an event _did_ occur. In a forest with a lake, the lake is a place where an event could not occur.
* Point pattern data: A collection of observed (or simulated) event locations together with a domain of interest.
* Domain: Study area that is often defined by data availability (e.g., state or county boundary) or by the extent of the events.
* Complete spatial randomness: Or CSR (not to be confused with CRS--coordinate reference system) defines the situation where an event has an equal chance of occurring at any point in the domain regardless of other nearby events. In this case we say they event locations have a uniform probability distribution (uniformly distributed) across space. Note: uniform chance does not mean that the events have an ordered pattern (e.g., trees in an orchard).

Consider a set of event locations that are randomly distributed within the unit plane. First create two vectors containing the x and y coordinates, then create a data frame that includes the name of the sample, and finally graph the locations using `ggplot()`.

```{r} 
library(ggplot2)

x <- runif(n = 50, min = 0, max = 1)
y <- runif(n = 50, min = 0, max = 1)
df1 <- data.frame(x, y, name = "Point Pattern 1")
ggplot(data = df1, 
       mapping = aes(x, y)) +
  geom_point(size = 2)
```

The plot shows a sample from a spatial point pattern process. A _spatial point process_ is a mechanism for producing a set of event locations across space. The pattern of locations produced by the point process is described as CSR. There are groups of event locations and some gaps. 

Let's repeat this process to create three additional samples. First you combine them into a single data frame with the `rbind()` function and then plot a four-panel figure using the `facet_wrap()` function.

```{r}
df2 <- data.frame(x = runif(n = 30, min = 0, max = 1),
                  y = runif(n = 30, min = 0, max = 1),
                  name = "Point Pattern 2")
df3 <- data.frame(x = runif(n = 30, min = 0, max = 1),
                  y = runif(n = 30, min = 0, max = 1),
                  name = "Point Pattern 3")
df4 <- data.frame(x = runif(n = 30, min = 0, max = 1),
                  y = runif(n = 30, min = 0, max = 1),
                  name = "Point Pattern 4")
df <- rbind(df1, df2, df3, df4)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_point() +
  facet_wrap(~ name)
```

Groups of nearby events illustrate that a certain degree of _clustering_ occurs by chance (without cause) making visual assessment of causal clustering difficult.

Complete spatial randomness sits on a spectrum between regularity and clustered. To illustrate this idea here you generate point pattern data that have more regularity than CSR and point pattern data that are more clustered than CSR. You do this using the `rMaternI()` and `rMaternClust()` functions from the {spatstat} package.
```{r}
if(!require(spatstat)) install.packages(pkgs = "spatstat", repos = "http://cran.us.r-project.org")

m1 <- spatstat.random::rMaternI(kappa = 100, r = .02)
df1 <- data.frame(x = m1$x, y = m1$y, name = "Regular Pattern 1")
m2 <- spatstat.random::rMaternI(kappa = 100, r = .02)
df2 <- data.frame(x = m2$x, y = m2$y, name = "Regular Pattern 2")
m3 <- spatstat.random::rMatClust(kappa = 30, r = .15, mu = 4)
df3 <- data.frame(x = m3$x, y = m3$y, name = "Cluster Pattern 1")
m4 <- spatstat.random::rMatClust(kappa = 30, r = .15, mu = 4)
df4 <- data.frame(x = m4$x, y = m4$y, name = "Cluster Pattern 2")
df <- rbind(df1, df2, df3, df4)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_point() +
  facet_wrap(~ name)
```

The difference in the arrangement of event locations between a regular and a cluster process is clear. But the difference in the arrangement of event locations between a CSR and regular process and the difference in the arrangement of event locations between a CSR and cluster process is not.

And spatial scale matters. A set of event locations can be regular on a small scale but clustered on a larger scale.

Probability models for spatial patterns motivate methods for detecting event clustering. A probability model generates a point pattern process. For example, we can think of crime as a point pattern process defined by location and influenced by environmental factors. The probability of a crime occurring at a particular location is the random variable and we can estimate the probability of a crime event at any location given factors that influence crime. 

More formally, a spatial point pattern process is a _stochastic_ (statistical) process where event location is the random variable. A sample of the process is a collection of events generated under the probability model.

A spatial point process is said to be _stationary_ if the statistical properties of the events are invariant to translation. This means that the relationship between two events depends only on the relative event locations (not on where the events occur in the domain). Relative location (or spatial lag) refers to distance and orientation of the events relative to one another. 

In the case where the statistical properties are independent of the orientation of event pairs, the process is said to be _isotropic_. 

The properties of stationarity and isotropy allow for replication within a data set. Under the assumption of a stationary process, two event pairs that are separated by the same distance should have the same relatedness. This is analogous to the assumption we make when we define our weights matrix for spatially aggregated data. The assumptions of stationarity and isotropy are starting points for modeling point pattern data. 

The Poisson distribution defines a model for complete spatial randomness (CSR). A point process is said to be 'homogeneous Poisson' under the following two criteria: 

1. The number of events, N, occurring within a finite domain A is a random variable described by a Poisson distribution with mean $\lambda$|A| for some positive constant $\lambda$, with |A| denoting the area of the domain, and 
2. The locations of the N events represent a random sample where each point in A is _equally likely_ to be chosen as an event location.

The first criteria of a Poisson distribution refers to a probability model describing the number of events. It expresses the probability of a given number of events occurring in a fixed interval of space when the events occur with a known constant rate.

The Poisson parameter defines the _intensity_ of the point process. Given a set of events, an estimate for the mean (rate) parameter of the Poisson distribution is given by the number of events divided by the domain area. 

The second criteria ensures the events are scattered about the domain without clustering or regularity.

The procedure to create a homogeneous Poisson point process follows directly from its definition. Step 1: Sample the total number of events from a Poisson distribution with a mean that is proportional to the domain area. Step 2: Place each event within the domain with coordinates given by a _uniform distribution_.

For example, let area |A| = 1, and the rate of occurrence $\lambda$ = 20, then
```{r}
lambda <- 20
N <- rpois(1, lambda)
x <- runif(N)
y <- runif(N)
df <- data.frame(x, y)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_point(size = 2) 
```

The set of events represents a sample from a homogeneous Poisson point process. The intensity of the events is specified first then the locations are placed uniformly inside the domain. The domain need not be regular. The actual number of events varies from one realization to the next. On average the number of events is 20 but it could vary between 10 and 35 or more.

This point pattern is CSR by construction. However, you are typically in the opposite position. You observe a set of events and you want to know if the events are regular or clustered. The null hypothesis is CSR and you need a test statistic that will summarize the evidence against this hypothesis. The null models are simple so you can use Monte Carlo methods to generate many samples and compare summary statistics from those samples with your observed data.

In some cases the homogeneous Poisson model is not restrictive enough. This means that you can easily reject the null hypothesis but not learn anything interesting about your data. For example, with health events (locations of people with heart disease) CSR is not an appropriate model because a null hypothesis that incidences are equally likely does not consider that people cluster (locations at risk are not uniform).

Each person has the same risk of heart disease regardless of location, and you expect more cases in areas with more people at risk. Clusters of cases in high population areas violate the CSR but not necessarily the constant risk hypothesis. The constant risk hypothesis requires the intensity of the spatial process be defined as a spatially varying function. That is, you define the intensity as $\lambda(s)$, where $s$ denotes location.

The intensity (density) function is a first-order property of the random process. If intensity varies (significantly) across the domain the data-generating process is said to be heterogeneous. The intensity function describes the expected number of events at any location. Events might be independent of one another, but groups of events appear because of the changing intensity.

## Working with point pattern objects using functions from the {spatstat} package {-}

The {spatstat} package contains many functions to analyze and model point pattern data. Point pattern data are defined in {spatstat} by an object of class `ppp` (for planar point pattern) which contains the coordinates of the events (event locations), optional values attached to the events (called 'marks'), and a description of the domain or 'window' over which the events are observed. See `?ppp.object()` for details.

Spatial statistics computed on a `ppp` object will be somewhat sensitive to the choice of the window (domain), so some thought should go into deciding what window should be used.

As an example, the data `swedishpines` is available in the package as a `ppp` object.
```{r}
library(spatstat)

class(swedishpines)
swedishpines
```

The data is a planar point pattern object with 71 events. 

Note: The events in a `ppp` object are called 'points' rather than events. This is in contrast to the theory that defines a point as represented by a _potential_ event not an _observed_ event.

All the events are contained within a rectangle window of size 9.6 by 10 meters.

There is a `plot()` method for `ppp` objects that provides a quick view the data and the domain window.

```{r}
plot(swedishpines)
```

Events are plotted as open circles inside a box. The plot is labeled with the name of the `ppp` object.

The function `convexhull()` from the {spatstat} package creates a convex hull around the events. A convex hull defines the minimum-area convex polygon that contains all the events. 

Here you compute the hull and add it to the plot.
```{r}
plot(swedishpines)
plot(convexhull(swedishpines), 
     add = TRUE)
```

The domain (window) for analysis and modeling should be somewhat larger than the convex hull. The function `ripras()` computes a spatial domain based on the event locations alone assuming the locations are independent and identically distributed.

Here you also overlay this polygon on the plot.
```{r}
plot(swedishpines)
plot(convexhull(swedishpines), 
     add = TRUE)
plot(ripras(swedishpines), 
     add = TRUE, lty = "dotted")
```

The window can have an arbitrary shape. A rectangle, a polygon, a collection of polygons including holes, or a binary image (mask). A window can be stored as a separate object of class `owin`. See `?owin.object()` for details.

Each event may carry information called a 'mark'. A mark can be continuous (e.g. tree height) or discrete (tree species).

A multitype point pattern is one in which the events are marked using a factor (e.g., tree species). The mark values are given in a vector of the same length as the vector of locations. That is, `marks[i]` is the mark attached to the location (`x[i]`, `y[i]`).

Consider the `ppp` object `demopat` from the {spatstat} package.

```{r}
plot(demopat)
marks(demopat)
```

Here the domain is defined as an irregular concave polygon with a hole. The distinction between inside and outside is important for spatial statistics computed using the events.

For a multitype pattern (where the marks are factors) you can use the `split()` function to separate the point pattern objects by mark type. Consider the Lansing Woods data set (`lansing`) with marks corresponding to tree species.

```{r}
data(lansing)
LW <- lansing

plot(split(LW))
```

## Quantifying event intensity {-}

The average intensity of events is defined as the number of events per unit area of the domain. The `summary()` method applied to a `ppp` object gives the average intensity.

```{r}
summary(swedishpines)
```

There are 71 events over a window area (spatial domain) of 9600 square units giving an average intensity of 71/9600 = .0074.

The average intensity might not represent the intensity of events locally. We need a way to describe the expected number of events at any location of the region. 

Counting the number of events in equal areas is one way. The quadrat method divides the domain into a grid of rectangular cells and the number of events in each cell is counted. Quadrat counting is done with the `quadratcount()` function.

```{r}
quadratcount(swedishpines)
```

By default the function divides the data into a 5 x 5 grid of cells. The event count in each cell is produced. To change the default number of cells in x and y directions you use the `nx =` and `ny =` arguments.

```{r}
quadratcount(swedishpines, 
             nx = 2, 
             ny = 3)
```

The plot method applied to the results of the `quadratcount()` functions adds the counts to a plot. Here you add the counts and include the event locations.

```{r}
plot(quadratcount(swedishpines))
plot(swedishpines, pty = 19, col = "red", 
     add = TRUE, main = "")
```

Grid cell areas will not be all equal when the domain boundaries are irregular like with the `demopat` ppp object.

```{r}
plot(quadratcount(demopat))
```

Areas near the borders are smaller than areas completely within the domain.

When the number of events is large, hexagon grid cells provide a useful alternative to rectangular grid cells. 

The process is: (1) tessellate the domain by a regular grid of hexagons, (2) count the number of events in each hexagon, and (3) use a color ramp to display the events per hexagon.

As an example here you generate 20K random values from the standard normal distribution for the x coordinate and the same number of random values for the y coordinate. You then use the `hexbin()` function from the {hexbin} package and specify 10 bins in the x direction to count the number of events in each hexagon and assign the result to the object `hbin`.
```{r}
if(!require(hexbin)) install.packages(pkgs = "hexbin", repos = "http://cran.us.r-project.org")

x <- rnorm(20000)
y <- rnorm(20000)
hbin <- hexbin::hexbin(x, y, xbins = 10) 
str(hbin)
```

The {hexbin} package uses S4 data classes so the output is stored in slots. Use the `plot()` method to make a graph.
```{r}
plot(hbin)
```

Hexagons have symmetric nearest neighbors (there is only rook contiguity). They have the most sides of any polygon that can tessellate the plane. They are generally more efficient than rectangles at covering the events. In other words it takes fewer of them to cover the same number of events. They are visually less biased for displaying local event intensity compared to squares/rectangles.

Here you generate a large number of random events in the two-dimensional plane. Use a normal distribution in the x-direction and a student t-distribution in the y-direction.
```{r}
set.seed(131)
x <- rnorm(7777)
y <- rt(7777, df = 3)
hbin2 <- hexbin::hexbin(x, y, xbins = 25)
plot(hbin2)
```

The {ggplot2} package has the `stat_binhex()` function so that also can be used for display.
```{r}
df <- data.frame(x, y)
ggplot(data = df, 
       mapping = aes(x, y)) +
  stat_binhex()
```

Another way to quantify the spatial intensity is with kernel density estimation (KDE). Let $s_i$ for $i$ = 1...$n$ be event locations, then an estimate for the intensity of the events at any location $s$ is given by

$$
\hat \lambda (s) = \frac{1}{nh}\sum_{i=1}^nK\Big(\frac{s - s_i}{h}\Big)
$$
where $K()$ is the kernel function and $h > 0$ is a smoothing parameter called the bandwidth. Typically the kernel function is a Gaussian probability density function.

To help visualize KDE you generate 25 event locations (`el`) uniformly on the real number line representing a one-dimensional spatial domain between 0 and 1 and then use kernel density estimation to get a continuous intensity function. The density estimation is is done using the function `density()` and here you compare the intensity function for increasing bandwidths specified with the `bw =` argument.
```{r}
el <- runif(25)
dd1 <- density(el, bw = .025)
dd2 <- density(el, bw = .05)
dd3 <- density(el, bw = .1)
df <- data.frame(x = c(dd1$x, dd2$x, dd3$x), 
                 y = c(dd1$y, dd2$y, dd3$y),
                bw = c(rep("h = .025", 512), 
                       rep("h = .05", 512),
                       rep("h = .1", 512)))
df2 <- data.frame(x = el, y = 0)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_line() +
  facet_wrap(~ bw, nrow = 3) +
  geom_point(mapping = aes(x, y), 
             data = df2, 
             color = "red")
```

As the bandwidth increases the curve (black line) representing the local intensity becomes smoother. The intensity is estimated at every location, not just at the location of the event. 

The density is a summation of the kernels with one kernel centered on top of each event location. Event locations are marked with a point along the x-axis and the kernel is a Gaussian probability density function. The kernel is placed on each event and the bandwidth specifies the distance between the inflection points of the kernel. The one-dimensional KDE extends to two (or more) dimensions.

Example: The distribution of trees in a tropical forest

The object `bei` is a planar point pattern object from the {spatstat} package containing the locations of trees in a tropical rain forest.

```{r}
summary(bei)
```

There are 3604 events (trees) over an area of 500,000 square meters giving an average intensity of .0073 trees per unit area.

The distribution of trees is not uniform (heterogeneous) as can be seen with a plot
```{r}
plot(bei)
```

The plot shows clusters of trees and large areas with few if any trees.

Elevation and elevation slope are factors associated with tree occurrence.

The point pattern data is accompanied by data (`bei.extra`) on elevation (`elev`) and slope of elevation (`grad`) across the region.

```{r}
plot(bei.extra)
```

These data are stored as `im` (image) objects. 
```{r}
class(bei.extra$elev)
```

The image object contains a list with 10 elements including the matrix of values (`v`).
```{r}
str(bei.extra$elev)
```

Specifying a spatial domain (window) allows focuses the analysis on a particular region. Suppose you want to model locations of a certain tree type, but only for trees located at elevations above 145 meters. The `levelset()` function creates a window from an image object using `thresh =` and `compare =` arguments.
```{r}
W <- levelset(bei.extra$elev, 
              thresh = 145, 
              compare = ">")
class(W)
```

The result is an object of class `owin`. The plot method displays the window as a mask, which is the region in black.
```{r}
plot(W)
```

You subset the `ppp` object by the window using the bracket operator (`[]`). Here you assign the reduced `ppp` object to `beiW` and then make a plot.
```{r}
beiW <- bei[W]
plot(beiW)
```

Now the analysis window is white and the event locations are plotted on top.

As another example you create a window where altitude is lower than 145 m and slope exceeds .1 degrees. In this case you use the `solutionset()` function. 
```{r}
V <- solutionset(bei.extra$elev <= 145 & 
                 bei.extra$grad > .1)
beiV <- bei[V]
plot(beiV)
```

You compute the spatial intensity function over the domain with the `density()` method using the default Gaussian kernel and fixed bandwidth determined by the window size.
```{r}
beiV |>
  density() |>
  plot()
```

The units of intensity are events per unit area (here square meters). The intensity values are computed on a grid ($v$) and are returned as a pixel image.

There are over 16K of the cells that have a value of `NA` as a result of the masking by elevation and slope.
```{r}
den <- beiV |>
  density()

sum(is.na(den$v))
```