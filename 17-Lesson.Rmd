# Tuesday November 1, 2022 {.unnumbered}

**"Weeks of coding can save you hours of planning."** - Unknown

Today

- Limitations of the distance functions
- Modeling point pattern data 
- Fitting and interpreting an inhibition model
- Fitting a log-Gaussian Cox model 

## Limitations of the distance functions {.unnumbered}

The distance functions ($G$, $K$, etc) that are used to quantify clustering are defined and estimated under the assumption that the process that produced the events is stationary (homogeneous). If this is true then you can treat any sub-region of the domain as an independent and identically distributed (iid) sample from the entire set of data.

If the spatial distribution of the event locations is influenced by event interaction then the functions will deviate from the theoretical model of CSR. But a deviation from CSR does not imply event interaction.

Moreover, the functions characterize the spatial arrangement of event locations 'on average' so variability in an interaction as a function of scale may not be detected.

As an example of the latter case, here you generate event locations at random with clustering on a small scale but with regularity on a larger scale. Then, on average, the event locations will be CSR as indicated by the $K$ function.

```{r}
set.seed(0112)

X <- rcell(nx = 15)
plot(X, main = "")
```

There are two 'local' clusters one in the north and one in the south. But overall the events appear to be more regular (inhibition) than CSR.

Interpretation of the process that created the event locations based on $K$ would be that the arrangement of events is CSR.

```{r}
X |>
  Kest() |>
  as.data.frame() |>
ggplot(mapping = aes(x = r, y = iso)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  xlab("Lag distance (km)") + ylab("K(r)") +
  theme_minimal()
```

The empirical curve (black line) coincides with the theoretical CSR line (red line) indicating CSR.

And the maximum absolute deviation test under the null hypothesis of CSR returns a large $p$-value so you fail to reject it.

```{r}
X |>
  mad.test(fun = Kest, 
           nsim = 99)
```

As an example of the former case, here you generate event locations that have no inter-event interaction but there is a trend in the spatial intensity.

```{r}
X <- rpoispp(function(x, y){ 300 * exp(-3 * x) })

X |>
  plot(main = "") 
```

By design there is a clear trend toward fewer events moving toward the east.

You compute and plot the $K$ function on these event locations.

```{r}
X |>
  Kest() |>
  as.data.frame() |>
ggplot(mapping = aes(x = r, y = iso)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  xlab("Lag distance (km)") + ylab("K(r)") +
  theme_minimal()
```

The $K$ function indicates clustering but this is an artifact of the trend in the intensity.

In the case of a known trend in the spatial intensity, you need to use the `Kinhom()` function. For example, compare the uncertainty envelopes from a homogeneous and inhomogeneous Poisson process.

Start by plotting the output from the `envelope()` function with `fun = Kest`. The `global = TRUE` argument indicates that the envelopes are simultaneous rather than point-wise (`global = FALSE` which is the default). Point-wise envelopes assume the estimates are independent (usually not a good assumption) across the range of distances so the standard errors will be smaller resulting in narrower bands.

```{r}
envelope(X, 
         fun = Kest, 
         nsim = 999, 
         rank = 1, 
         global = TRUE) |>
  as.data.frame() |>
ggplot(mapping = aes(x = r, y = obs)) +
  geom_ribbon(mapping = aes(ymin = lo, ymax = hi), 
              fill = "gray70") +
  geom_line() +
  geom_line(mapping = aes(y = theo), 
            color = "red", lty = 'dashed') +
  xlab("Lag distance (km)") + ylab("K(r)") +
  theme_minimal()
```

After a distance of about .15 units the empirical curve (black line) is outside the uncertainty band indicating the events are more clustered than CSR.

However when you use the `fun = Kinhom` the empirical curve is completely inside the uncertainty band.

```{r}
envelope(X, 
         fun = Kinhom, 
         nsim = 999, 
         rank = 1, 
        global = TRUE) |>
  as.data.frame() |>
ggplot(mapping = aes(x = r, y = obs)) +
  geom_ribbon(mapping = aes(ymin = lo, ymax = hi), 
              fill = "gray70") +
  geom_line() +
  geom_line(mapping = aes(y = theo), 
            color = "red", lty = 'dashed') +
  xlab("Lag distance (km)") + ylab("K(r)") +
  theme_minimal()
```

You conclude that the point pattern data are more consistent with an inhomogeneous Poisson process.

Let's return to the Kansas tornadoes (EF1+). You imported the data and created a point pattern object windowed by the state borders.

```{r}
ST.ppp |>
  plot()
```

There are more tornado reports in the west than in the east, especially across the southern part of the state indicating the process producing the events is not homogeneous. This means there are other factors contributing to local event intensity.

Evidence for clustering must account for this inhomogeneity. Here you do this by computing the envelope around the inhomogeneous Ripley K function using the argument `fun = Kinhom`.

```{r}
envelope(ST.ppp,
         fun = Kinhom,
         nsim = 99,
         rank = 1,
         global = TRUE) |>
as.data.frame() |>
ggplot(mapping = aes(x = r, y = obs)) +
  geom_ribbon(mapping = aes(ymin = lo, ymax = hi), 
              fill = "gray70") +
  geom_line() +
  geom_line(mapping = aes(y = theo), 
            color = "red", lty = 'dashed') +
  xlab("Lag distance (km)") + ylab("K(r)") +
  theme_minimal()
```

The output reveals no evidence of clustering at distances less than about 70 km. At greater distances there is some evidence of regularity indicated by the black line below the red line and just outside the uncertainty ribbon. This is due to the fact that tornado reports are more common near cities and towns and cities and towns tend to be spread out more regular than CSR.

Finally, the variance stabilized Ripley $K$ function called the $L$ function is often used instead of $K$. The sample version of $L$ is defined as

$$
\hat{L}(r) = \Big( \hat{K}(r)/\pi\Big)^{1/2}.
$$

For data that is CSR, the $L$ function has expected value $r$ and its variance is approximately constant in $r$. A common plot is a graph of $r - \hat{L}(r)$ against $r$, which approximately follows the horizontal zero-axis with constant dispersion if the data follow a homogeneous Poisson process.

## Modeling point pattern data {-}

Models are helpful for trying to understanding the processes leading to the event locations when event interaction is suspected. Event interaction means that an event at one location changes the probability of an event nearby.

Cluster models can be derived by starting with a Poisson model. For example, you begin with a homogeneous Poisson model $Y$ describing a set of events. A model is homogeneous Poisson when the event locations generated by the model are CSR.

Then consider each individual event $y_i$ in $Y$ to be a 'parent' that produces a set of 'offspring' events ($x_i$) according to some mechanism. The resulting set of offspring forms clustered point pattern data $X$. Said another way, the model is homogeneous Poisson at an unobserved level $Y$ (latent level) but clustered at the level of the observations ($X$).

One example of this parent-child process is the Matern cluster model. Parent events come from a homogeneous Poisson process with intensity $\kappa$ and then each parent has a Poisson ($\mu$) number of offspring that are iid within a radius $r$ centered on the parent.

For instance here you use the `rMatClust()` function from the {spatstat} package to produce a clustered `ppp` object. We use a disc radius of .1 units and an offspring rate equal to 5 (`mu = 5`).
```{r}
rMatClust(kappa = 10, 
               r = .1, 
               mu = 5) |>
  plot(main = "")
```

The result is a set of event locations and the process that produced them is  described as _doubly Poisson_. You can vary $\kappa$, $r$, and $\mu$ to generate more or fewer events.

Other clustered Poisson models include:
- Thomas model: each cluster consists of a Poisson number of random events with each event having an isotropic Gaussian displacement from its parent.  
- Gauss-Poisson model: each cluster is either a single event or a pair of events.  
- Neyman-Scott model: the cluster mechanism is arbitrary.

A Cox model is a homogeneous Poisson model with a random intensity function. Let $\Lambda(s)$ be a function with non-negative values defined at all locations $s$ inside the domain. Then, conditional on $\Lambda$ let $X$ be a Poisson model with an intensity function $\Lambda$. Then $X$ will be a sample from a Cox model.

An example of a Cox model is the mixed Poisson process in which a random variable $\Lambda$ is generated and then, conditional on $\Lambda$, a homogeneous Poisson process with intensity $\Lambda$ is generated. 

Following are two samples from a Cox point process.

```{r}
set.seed(3042)
par(mfrow = c(1, 2))
for (i in 1:2){
  lambda <- rexp(n = 1, rate = 1/100)
  X <- rpoispp(lambda)
  plot(X)
}
par(mfrow = c(1, 1))
```

The statistical moments of Cox models are defined in terms of the moments of $\Lambda$. For instance, the intensity function of $X$ is $\lambda(s)$ = E[$\Lambda(s)$], where E[] is the expected value.

Cox models are convenient for describing clustered point pattern data. A Cox model is over-dispersed relative to a Poisson model (i.e. the variance of the number of events falling in any region of size A, is greater than the mean number of events in those regions). The Matern cluster model and the Thomas models are Cox models. Another common type of a Cox model is the log-Gaussian Cox processes (LGCP) model in which logarithm of $\Lambda(s)$ is a Gaussian random function.

If you have a way of generating samples from a random function $\Lambda$ of interest, then you can use the `rpoispp()` function to generate the Cox process. The intensity argument `lambda` of `rpoispp()` can be a function of x or y or a pixel image.

Another way to generate clustered point pattern data is by 'thinning'. Thinning refers to deleting some of the events. With 'independent thinning' the fate of each event is independent of the fate of the other events. When independent thinning is applied to a homogeneous Poisson point pattern, the resulting point pattern consisting of the retained events is also Poisson. 
To simulate a inhibition process you can use a 'thinning' mechanism.

An example of this is Matern's Model I model. Here a homogeneous Poisson model first generates a point pattern $Y$, then any event in $Y$ that lies closer than a distance $r$ from another event is deleted. This results in point pattern data whereby close neighbor events do not exist.
```{r}
plot(rMaternI(kappa = 70, 
              r = .05), main = "")

X <- rMaternI(kappa = 70, 
              r = .05)

X |>
  Kest() |>
  plot()
```

Changing $\kappa$ and $r$ will change the event intensity.

The various spatial models for event locations can be described with math. For instance, expanding on the earlier notation you write that a homogeneous Poisson model with intensity $\lambda > 0$ has intensity $$\lambda(s, x) = \lambda$$ where $s$ is any location in the window W and $x$ is the set of events.

Then the inhomogeneous Poisson model has conditional intensity $$\lambda(s, x) = \lambda(s)$$. The intensity $\lambda(s)$ depends on a spatial trend or on an explanatory variable.

There is also a class of 'Markov' point process models that allow for clustering (or inhibition) due to event interaction. Markov refers to the fact that the interaction is limited to nearest neighbors. Said another way, a Markov point process generalizes a Poisson process in the case where events are pairwise dependent.

A Markov process with parameters $\beta > 0$ and $0 < \gamma < \infty$ with interaction radius $r > 0$ has conditional intensity $\lambda(s, x)$ given by

$$
\lambda(s, x) = \beta \gamma^{t(s, x)}
$$

where $t(s, x)$ is the number of events that lie within a distance $r$ of location $s$.

Three cases:
- If $\gamma = 1$, then $\lambda(s, x) = \beta$ No interaction between events,  $\beta$ can vary with $s$.
- If $\gamma < 1$, then $\lambda(s, x) < \beta$. Events inhibit nearby events.
- If $\gamma > 1$, then $\lambda(s, x) > \beta$. Events encourage nearby events.

Note the distinction between the interaction term $\gamma$ and the trend term $\beta$. Note: A similar distinction exists between autocorrelation $\rho$ and trend $\beta$ in spatial regression models.

More generally, you write the logarithm of the conditional intensity $\log[\lambda(s, x)]$ as linear expression with two components.

$$
\log\big[\lambda(s, x)\big] = \theta_1 B(s) + \theta_2 C(s, x)
$$

where the $\theta$'s are model parameters that need to be estimated.  

The term $B(s)$ depends only on location so it represents trend and explanatory variable (covariate) effects. It is the 'systematic component' of the model. The term $C(s, x)$ represents stochastic interactions (dependency) between events.

## Fitting and interpreting an inhibition model {-}

The {spatstat} package contains functions for fitting statistical models to point pattern data. Models can include trend (to account for non-stationarity), explanatory variables (covariates), _and_ event interactions of any order (in other words, interactions are not restricted to pairwise). Models are fit with the method of maximum likelihood and the method of minimum contrasts.

The method of maximum likelihood estimates the probability of the empirical $K$ curve given the theoretical curve for various parameter values. Parameter values are chosen so as to maximize the likelihood of the empirical curve.

The method of minimum contrasts derives a cost function as the difference between the theoretical and empirical $K$ curves. Parameter values for the theoretical curve are those that minimize this cost function.

The `ppm()` function is used to fit a spatial point pattern model. The syntax has the form `ppm(X, formula, interaction, ...)` where `X` is the point pattern object of class `ppp`, `formula` describes the systematic (trend and covariate) part of the model, and `interaction` describes the stochastic dependence between events (e.g., Matern process).

Recall a plot of the Swedish pine saplings. There was no indication of a trend (no systematic variation in the intensity of saplings).

```{r}
SP <- swedishpines
plot(SP)

intensity(SP)
```

There is no obvious spatial trend in the distribution of saplings and the average intensity is .0074 saplings per unit area.

A plot of the Ripley's $K$ function indicated regularity relative to CSR.

```{r}
SP |>
  Kest(correction = "iso") |>
  plot()
```

The red dashed line is the $K$ curve under CSR. The black line is the empirical curve. At lag distances of between 5 and 15 units the empirical curve is below the CSR curve indicating there are fewer events within other events at those scales than would be expected by chance.

This suggests a physical process whereby saplings tend to compete for sunlight, nutrients, etc. A process of between-event inhibition. If you suspect that the spatial distribution of event locations is influenced by inhibition you can model the process statistically.

A simple inhibition model is a Strauss process when the inhibition is constant with a fixed radius (r) around each event. The amount of inhibition ranges between zero (100% chance of a nearby event) to complete (0% chance of a nearby event). In the case of no inhibition the process is equivalent to a homogeneous Poisson process.

If you assume the inhibition process is constant across the domain with a fixed interaction radius (r), then you can fit a Strauss model to the data. You use the `ppm()` function from the {spatstat} package and include the point pattern data as the first argument. You set the trend term to a constant (implying a stationary process) with the argument `trend ~ 1` and the interaction radius to 10 units with the argument `interaction = Strauss(r = 10)`. Finally you use a border correction out to a distance of 10 units from the window with the `rbord =` argument.

Save the output in the object called `model.in` (inhibition model).

```{r}
model.in <- ppm(SP, 
                trend = ~ 1, 
                interaction = Strauss(r = 10), 
                rbord = 10)
```

The value for `r` in the `Strauss()` function is based on our visual inspection of the plot of `Kest()`. A value is chosen that represents the distance at which there is the largest departure from a CSR model. 

You inspect the model parameters by typing the object name.

```{r}
model.in
```

The first-order term (`beta`) has a value of .0757. This is the intensity of the 'proposal' events. The value of beta exceeds the average intensity by a factor of ten. 

Recall the intensity of the events is obtained as

```{r}
intensity(SP)
```

The interaction parameter (`gamma`) is .275. It is less than one, indicating an inhibition process. The logarithm of gamma, called the interaction coefficient (`Interaction`), is -1.29. Interaction coefficients less than zero imply inhibition.

A table with the coefficients including the standard errors and uncertainty ranges is obtained with the `coef()` method.
```{r}
model.in |>
  summary() |>
  coef()
```

The output includes the `Interaction` coefficient along with it's standard error (`S.E.`) and the associated 95% uncertainty interval. The ratio of the `Interaction` coefficient to its standard error is the `Zval`. A large z-value (in absolute magnitude) translates to a low $p$-value and a rejection of the null hypothesis of no interaction between events.

Output also is the estimated value for the `(Intercept)` term. It is the logarithm of the beta value, so exp(-2.58) = .0757 is the intensity of the proposal events.

You interpret the model output as follows. The process producing the spatial pattern of pine saplings is such that you should see .0757 saplings per unit area [unobserved (latent) rate]. 

But because of event inhibition, where saplings nearby other saplings fail to grow, the number of saplings is reduced to .0074 per unit area. Thus the spatial pattern is suggestive of sibling-sibling interaction. Adults have many offspring, but only some survive due to limited resources.

## Fitting a log-Gaussian Cox model

<https://inlabru-org.github.io/inlabru/articles/web/1d_lgcp.html>

You can also fit a Cox model using the method of stochastic partial differential equations (SPDE). This involves a probability (Bayesian) framework that approximates marginal posterior distributions.

```{r}
library(inlabru)
library(INLA)
library(mgcv)
library(ggplot2)
```

Get the data

```{r}
data(Poisson2_1D)

dim(pts2)
range(pts2)
```

Plot the data and a histogram. Here you choose about 20 bins across the range of values.

```{r}
ggplot(data = pts2) +
  geom_histogram(mapping = aes(x = x), 
                 binwidth = 55 / 20, 
                 boundary = 0, 
                 fill = NA, 
                 color = "black") +
  geom_point(mapping = aes(x = x), 
             y = 0, pch = "|", cex = 4) 
```

Create a 1D mesh of 50 points (`length.out =`) across the range of values (from 0 to 55). The end points of the mesh are not constrained.

```{r}
x <- seq(0, 55, length.out = 50)
mesh1D <- inla.mesh.1d(loc = x, 
                       boundary = "free")
```

Specify the spatial autocorrelation with a Matern cluster process model. The first argument is the mesh onto which the model will be built.

The argument `prior.range =` accepts a vector of length two with the first element the lag distance (range) of the spatial autocorrelation and the second element the probability that the range will be less than that value. If the second value is `NA`, the value of the first element is used as a fixed range.

The argument `prior.sigma =` accepts a vector of length two with the first element is the marginal standard deviation of the intensity and the second element the probability that the standard deviation will be greater than that value.

Here you are non-committal on the range of spatial autocorrelation so you specify a large lag distance (150) with a 75% chance that it will be less than that.

```{r}
Matern <- inla.spde2.pcmatern(mesh = mesh1D, 
                              prior.range = c(150, .75),
                              prior.sigma = c(.1, .75))
```

Next specify the full model.

```{r}
f <- x ~ spde1D(x, model = Matern) + Intercept(1)
```

Next you fit the model to the actual event locations in `pts2`. You use the `lgcp()` function from the {inlabru} package  for log Gaussian Cox process. The `domain =` argument specifies the 1D mesh as a list object.

```{r}
model.lgcp <- lgcp(components = f, 
                   data = pts2, 
                   domain = list(x = mesh1D))
```

You can look at the posterior distributions of the model parameters using the function `spde.posterior()`. It returns x and y values for a plot of the posterior probability density function (PDF) as a data frame, which you plot with the `plot.bru()` function. 

Here is the PDF for the range parameter.

```{r}
spde.posterior(result = model.lgcp, 
               name = "spde1D", 
               what = "range") |>
  plot()
```

Perhaps better viewed on a log scale.

```{r}
spde.posterior(model.lgcp, 
               name = "spde1D", 
               what = "log.range") |>
  plot()
```

Here is the PDF for the Matern correlation part of the model.

```{r}
spde.posterior(model.lgcp, 
               name = "spde1D", 
               what = "matern.correlation") |>
  plot()
```

It shows a maximum correlation of 1 at zero lag distance with a .5 correlation out at a lag of about 20 units.

You can get a feel for sensitivity to priors by specifying different priors and looking at these posterior plots. Always a good idea when fitting models using Bayesian methods.

For example, change the prior range from 150 to 30 and refit, then compare the PDF of the Matern correlation.

You can predict on the 'response' scale [i.e. the intensity function $\lambda$(s)]. First set up a data frame of explanatory values at which to predict (here `grid.df`). Then use the `predict()` method.

```{r}
grid.df <- data.frame(x = seq(0, 55, by = 1)) 
pred.df <- predict(model.lgcp, 
                   data = grid.df, 
                   formula = ~ exp(spde1D + Intercept))
```

The output is a data frame containing the explanatory values and corresponding summary statistics on the posterior predictions.

```{r}
str(pred.df)
```

You pass this data frame to the `plot()` method to produce the following prediction plot.

```{r}
plot(pred.df, color = "red") +
  geom_point(data = pts2, 
             mapping = aes(x = x), 
             y = 0, pch = "|", cex = 2) +
  xlab("x") + ylab("Spatial intensity")
```

How does this compare with the underlying intensity function that generated the data? 

The function `lambda2_1D( ) `in the dataset `Poission2_1D` calculates the true intensity that was used in simulating these data. In order to plot this, you make a data frame with x- and y-coordinates giving the true intensity function, $\lambda$(s). Use a lot of x-values to get a nice smooth plot (150 values).

```{r}
xs <- seq(0, 55, length = 150)
true.lambda <- data.frame(x = xs, 
                          y = lambda2_1D(xs))
```

Plot the fitted and true intensity functions.

```{r}
plot(pred.df, color = "red") +
  geom_point(data = pts2, 
             mapping = aes(x = x), 
             y = 0, pch = "|", cex = 2) +
  geom_line(data = true.lambda, 
            mapping = aes(x, y)) +
  xlab("x") + ylab("Spatial intensity")
```

You can look at the goodness-of-fit of the model using the function `bincount( )`, which plots the 95% credible intervals in a specified set of bins along the x-axis together with the observed count in each bin. 

```{r}
bc <- bincount(
  result = model.lgcp,
  observations = pts2,
  breaks = seq(0, max(pts2), length = 12),
  predictor = x ~ exp(spde1D + Intercept)
)

attributes(bc)$ggp
```

The credible intervals are shown as red rectangles, the mean fitted value as a short horizontal blue line, and the observed data as black points.

Abundance is the integral of the intensity over space. You estimate it by integrating the predicted intensity over x.

Integration is done by adding up the intensity at locations x weighted by a particular weight. The locations x and their weights are constructed using the `ipoints()` function.

Here you create 50 integration points cover the 1D range.

```{r}
ips <- ipoints(c(0, 55), 100, name = "x")

head(ips)
```

Then compute the abundance with the `predict()` method.

```{r}
( Lambda <- predict(model.lgcp, 
                    ips, 
                    ~ sum(weight * exp(spde1D + Intercept))) )
```

* `mean` is the posterior mean abundance
* `sd` is the estimated standard error of the posterior of the abundance
* `q0.025` and `q0.975` are the 95% credible interval bounds
* `q0.5` is the posterior median abundance

The above posterior for abundance takes account only of the variance due to not knowing the parameters of the intensity function. It neglects the variance in the number of events, given the intensity function.

To include this you need to modify the input to the `predict( )` method. You include a data frame that samples from a Poisson density for each value for the abundance (here `N = 50:250`).

```{r}
Nest <- predict(model.lgcp, 
                ips,
                ~ data.frame(N = 50:250,
                             dpois = dpois(50:250,
                             lambda = sum(weight * exp(spde1D + Intercept)))))
```

The result is the same statistics as were calculated for `Lambda`, but for every abundance value between 50 and 250, rather than for the posterior mean abundance alone.

```{r}
head(Nest)
```

You compute the 95% prediction interval and the median with the `inla.qmarginal()` function.

```{r}
inla.qmarginal(c(.025, .5, .975), 
               marginal = list(x = Nest$N, 
                               y = Nest$mean))
```

Now compare `Lambda` to `Nest` by plotting. First calculate the posterior conditional on the mean of `Lambda`.
```{r}
Nest$plugin_estimate <- dpois(Nest$N, 
                              lambda = Lambda$mean)
```

Then plot it and the unconditional posterior.
```{r}
ggplot(data = Nest) +
  geom_line(aes(x = N, 
                y = mean, 
                color = "Posterior")) +
  geom_line(aes(x = N, 
                y = plugin_estimate, 
                color = "Plugin"))
```

Can you explain the difference?

Spatial distribution of gorilla nests using SPDE <https://inlabru-org.github.io/inlabru/index.html>