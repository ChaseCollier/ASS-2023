# Tuesday October 18, 2022 {.unnumbered}

**"Good code is its own best documentation. As you're about to add a comment, ask yourself, 'How can I improve the code so that this comment isn't needed?' Improve the code and then document it to make it even clearer."** - Steve McConnell

Today

- Examples of spatially clustered events
- Determining the statistical significance of event clustering
- Estimating event clustering in multi-type event locations
- More about the Ripley K function

## Examples of spatially clustered events

Bramble canes

The locations of bramble canes are available as a marked `ppp` object in the {spatstat} package. A bramble is a rough (usually wild) tangled prickly shrub with thorny stems.

```{r}
library(spatstat)

data(bramblecanes)
summary(bramblecanes)
```

The marks represent three different ages (as an ordered factor) for the bramble canes. The unit of length is 9 meters.

```{r}
plot(bramblecanes) 
```

Consider the point pattern for all the bramble canes regardless of age and estimate the $K$ function and a corresponding plot. Plot the empirical estimate of $K$ with an 'isotropic' correction at the domain borders (`iso`).

```{r}
K.df <- bramblecanes |>
  Kest() |>
  as.data.frame() |>
  dplyr::mutate(r = r * 9)

library(ggplot2)

ggplot(data = K.df, 
       mapping = aes(x = r, y = iso)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  xlab("Distance (m)") + ylab("K(r)") +
  theme_minimal()
```

The estimate of $K$ on the actual data (black line) is to the left of the theoretical $K$ function under CSR (red line). This means that for any distance there tends to be _more_ events within this distance (larger $K$) than expected under CSR. You conclude that these bramble canes are more clustered than CRS.

The expected number of additional events is multiplied by the total number of events (823) so a value of .1 indicates that at a distance of 1.6 meters you should expect to see about 82 additional events.

Kansas tornado reports

Previously you mapped the intensity of tornadoes across Kansas by considering the genesis locations as point pattern data. Here we return to these data and consider only tornadoes since 1994.

```{r}
library(sf)
library(USAboundaries)
library(maptools)

Torn.sf <- st_read(dsn = "1950-2018-torn-initpoint") |>
  st_transform(crs = 3082) |>
  filter(mag >= 0, yr >= 1994) |>
  mutate(EF = as.factor(mag)) |>
  dplyr::select(EF)

T.ppp <- Torn.sf["EF"] |>
  as_Spatial() |>
  as.ppp()

KS.sf <- us_states(states = "Kansas") |>
  st_transform(crs = st_crs(Torn.sf)$proj4string)

W <- KS.sf |>
  as_Spatial() |>
  as.owin()

T.ppp <- T.ppp[W] |>
  spatstat::rescale(s = 1000, 
                    unitname = "km")

T.ppp |>
  plot()

T.ppp |>
  summary()
```

There are 2181 events with an average intensity of .01 events per square km (1 tornado per 10 square km over the 24-year period 1994--2018).

We compare an estimate of the K function from the set of tornado reports with an estimate of the K function from a model of complete spatial randomness on a plot.
```{r}
K.df <- T.ppp |>
  Kest(correction = "iso") |>
  as.data.frame() |>
  mutate(Kdata = iso * sum(intensity(T.ppp)),
         Kpois = theo * sum(intensity(T.ppp)))

ggplot(data = K.df, 
       mapping = aes(x = r, y = Kdata)) +
  geom_line() +
  geom_line(mapping = aes(y = Kpois), color = "red") +
  geom_vline(xintercept = 60, lty = 'dashed') +
  geom_hline(yintercept = 129, lty = 'dashed') +
  geom_hline(yintercept = 115, lty = 'dashed') +
  xlab("Distance (km)") + ylab("K(r), Expected number of additional tornadoes\n within a distance r of any tornado") +
  theme_minimal()
```

Interpretation: Consider 60 km along the horizontal axis. If we draw a vertical line there we can see that the line intersects the black curve at a height of about 129. This value indicates that at a distance of 60 km from a random tornado report we find, on average, about 129 other tornado reports. Imagine placing a disc with radius 60 km around centered on each event then averaging the number of events under the disc over all events.

The red line is the curve under the assumption that the tornadoes are CSR across the state. We can see that if this was the case we would expect to see on average about 115 tornadoes within a distance 60 km from any tornado. Since there are MORE tornadoes than expected within a given 60 km radius we say there is evidence for clustering at this scale.

The black line lies above the red line across distances from 0 to greater than 100 km.

How do we interpret the output from the nearest neighbor function applied to the set of Kansas tornadoes? Here we create a data frame from the output of the `Gest()` function and remove distances exceeding 8 km.
```{r}
G.df <- T.ppp |>
  Gest(correction = "km") |>
  as.data.frame() |>
  filter(r < 8)

ggplot(data = G.df, 
       mapping = aes(x = r, y = km)) +
  geom_line() + 
  geom_line(aes(y = theo), color = "red") +
  geom_hline(yintercept = .4, lty = 'dashed') +
  geom_vline(xintercept = c(3.2, 4), lty = 'dashed') +
  xlab("Distance (km)") + ylab("G(r): Cumulative % of tornadoes\n within a distance r of another tornado") +
  theme_minimal()
```

The interpretation is that 40% ($G$ = .4) of all tornado reports have another report within a distance of just about 3.2 km on average. If the reports where homogeneous Poisson then the distance would be 4 km. We conclude they are more clustered. 

Note: With many events the difference between the raw and border-corrected estimates is typically small.

## Determining the statistical significance of event clustering {-}

We see the separation between the black solid line and the red line, but is this separation large relative to the sample size? More to the point, is the above difference between the empirical and theoretical distance functions (e.g., $G$) large enough to conclude there is significant clustering? 

There are two ways to approach inference. 1) Compare the statistic of interest against many cases generated from the null hypothesis and ask: does the statistic fall outside the envelope of the null cases? 2) Get estimates of uncertainty on the statistic of interest and ask: does the uncertainty interval contain the null case? 

In the first approach we use a function that takes a `ppp` object and computes the summary statistic of interest (e.g., Ripley K) for a specified number of samples under the null hypothesis of a homogeneous Poisson process. 

To speed things up we consider a subset of all the tornadoes that have an EF rating of 2 or higher by creating a new `ppp` object that contains only tornadoes rated at least EF2. Note: since the marks is a factor vector we can't use `>=`.
```{r}
ST.ppp <- unmark(T.ppp[T.ppp$marks == 2 | 
                       T.ppp$marks == 3 | 
                       T.ppp$marks == 4 |
                       T.ppp$marks == 5])
plot(ST.ppp)
```

We then use the `envelope()` function from the {spatstat} package on this new `ST.ppp` object and specify the statistic of interest with the `fun = Kest` argument and the number of samples with the `nsim =` argument. We then convert the output from that function to a data frame. It takes a few seconds to complete all 99 samples.
```{r}
Kenv.df <- envelope(ST.ppp, 
                    fun = Kest, 
                    nsim = 99) |>
  as.data.frame()

head(Kenv.df)
```

The resulting data frame has estimates of Ripley K as a function of distance (column labeled `obs`). It also has the estimates of K under the null hypothesis of CSR (`theo`) and the lowest (`lo`) and highest (`hi`) values of K across the 99 samples.

We take this data frame and make a plot using the `geom_ribbon()` layer to include a gray ribbon around the model of CSR.
```{r}
ggplot(data = Kenv.df, 
       mapping = aes(x = r, y = obs)) +
  geom_ribbon(mapping = aes(ymin = lo, ymax = hi), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  xlab("Distance (km)") + ylab("K(r)") +
  theme_minimal()
```

The K function computed on the data is the black line and the K function under CSR is the red line. The uncertainty ribbon (gray band) connects the point-wise minimum and maximum values of K computed from the 99 generated point pattern samples.

We can confidently conclude that these tornado reports are _more_ clustered than one would expect by chance.

If the specific intention is to test a null hypothesis of CSR, then a single statistic measuring the departure of K computed on the observations from the theoretical K may be appropriate. 

One such statistic is the maximum absolute deviation (MAD) and is implemented with the `mad.test()` function from the {spatstat} package. The function performs a hypothesis test for goodness-of-fit of the observations to the theoretical model. The larger the value of the statistic, the less likely it is that the data were generated according to this specification.
```{r}
mad.test(ST.ppp, 
         fun = Kest, 
         nsim = 99)
```

The maximum absolute deviation is 7449 which is very large so the $p$-value is small and we reject the null hypothesis of CSR for these data. This is consistent with the graph. Note: Since there are 99 simulations the lowest $p$-value is .01.

Another test statistic is related to the sum of the squared deviations between the estimated and theoretical functions. It is implemented with the `dclf.test()` function.
```{r}
dclf.test(ST.ppp, 
          fun = Kest, 
          nsim = 99)
```

In both cases the $p$-value on the test statistic against the one-sided alternative is less than .01 (Note, the reported $p$-value is two-sided) indicating conclusive evidence of clustering.

Here we repeat this type of inference about clustering in point pattern data using the Swedish pine saplings (`swedishpines`).
```{r}
Kenv.df <- envelope(SP, 
                    fun = Kest, 
                    nsim = 99) |>
  as.data.frame()

ggplot(data = Kenv.df, 
       mapping = aes(x = r * .1, y = obs * intensity(SP))) +
  geom_ribbon(aes(ymin = lo * intensity(SP),
                  ymax = hi * intensity(SP)), 
              fill = "gray70") +
  geom_line() + geom_line(aes(y = theo * intensity(SP)), 
                          color = "red") +
  xlab("Distance (m)") + 
  ylab("K(r), Expected number of additional saplings\n within a distance r of a sapling") +
  theme_minimal()
```

At short distances (closer than about 1 m) we see that the black line is below the red line and outside the gray ribbon which we interpret to mean that there are fewer pine saplings near other pine saplings than would be expected by chance.

But at larger distances we see that the black line is close to the red line and inside the gray ribbon which we interpret to mean that, at this larger spatial scale, the distribution of pine saplings is indistinguishable from CSR.

Based on the fact that much of the black line is within the gray envelope indicates that a formal test against the null hypothesis of CSR will likely fail to reject.
```{r}
mad.test(SP, 
         fun = Kest, 
         nsim = 99)
dclf.test(SP, 
          fun = Kest, 
          nsim = 99)
```

Both return a $p$-value that is greater than .15 so we fail to reject the null hypothesis of CSR.

In the second approach to inference we use the procedure of re-sampling. Note the distinction: Re-sampling refers to generating additional samples from the data while sampling, as we saw above, refers to generating additional samples from some theoretical model.

The bootstrap procedure is a re-sampling strategy whereby new samples are generated from the data by randomly choosing events within the domain. An event that is chosen for the 'bootstrap' sample gets the chance to be chosen again (called 'with replacement'). The number of events in each bootstrap sample must equal the number of events in the data.

Consider 15 numbers from 1 to 15. Then pick randomly from that set of numbers with replacement until the sample size is 15 to create a bootstrap sample.
```{r}
( x <- 1:15 )
sample(x, replace = TRUE)
```

Some numbers get picked more than once and some not at all.

The average of the original 15 `x` values is 8 but the average over the set of numbers in the bootstrap sample will not necessarily be 8. However, the distribution of the averages over many bootstrap samples will be centered close to this average.
```{r}
mx <- NULL
for(i in 1:99){
  mx[i] <- mean(sample(x, replace = TRUE))
}

mx.df <- as.data.frame(mx)
  ggplot(data = mx.df,
         mapping = aes(mx)) +
    geom_density() +
    geom_vline(xintercept = mean(x),
               color = "red")
```

The `lohboot()` function estimates the uncertainty on the computed spatial statistic using a bootstrap procedure. It works by computing a local version of the function (e.g., `localK()`) on the set of re-sampled events.
```{r}
Kboot.df <- ST.ppp |>
  lohboot(fun = Kest) |>
  as.data.frame()

ggplot(data = Kboot.df, 
       mapping = aes(x = r, y = iso)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  xlab("Distance (km)") + ylab("K(r)") +
  theme_minimal()
```

Now the uncertainty band is plotted about the black line (K function computed on the observations) rather than about the null model (red line). We see that the 95% uncertainty band does to include the CSR model. We confidently conclude that the tornadoes in Kansas are more clustered than chance.

Again for the Swedish pine saplings.
```{r}
Kboot.df <- as.data.frame(lohboot(SP, 
                          fun = Kest))

ggplot(Kboot.df, aes(x = r, y = iso)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo), color = "blue", lty = 'dashed') +
  xlab("Distance (km)") + ylab("K(r)") +
  theme_minimal()
```

## Estimating event clustering in multi-type event locations {-}

Often the interest focuses on whether the occurrence of one event type influences (or is influenced by) another event type. For example, does the occurrence of one species of tree influence the occurrence of another species?

Analogues to the G and K functions are available for 'multi-type' point patterns where the marks are factors.

A common statistic for examining 'cross correlation' of event type occurrences is the K cross function $K_{ij}(r)$, which estimates the expected number of events of type $j$ within a distance $r$ of type $i$.

Consider the data called `lansing` from the {spatstat} package that contains the locations of 2,251 trees of various species in a wooded lot in Lansing, MI as a `ppp` object.

```{r}
data(lansing)
summary(lansing)
```

The data are a multi-type planar point pattern with the marks indicating tree species. There are 135 black oaks, 703 hickories, etc. The spatial unit is 924 feet.

Compute and plot the cross $K$ function for Maple and Hickory trees.

```{r}
Kc.df <- lansing |>
  Kcross(i = "maple",
         j = "hickory") |>
  as.data.frame()
 
ggplot(data = Kc.df, 
       mapping = aes(x = r, y = iso)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  geom_vline(xintercept = .2, lty = 'dashed') +
  geom_hline(yintercept = .093, lty = 'dashed') +
  geom_hline(yintercept = .125, lty = 'dashed') +
  xlab("Distance") + ylab("Kc(r)") +
  theme_minimal()
```

The vertical axis is the number of hickory trees within a radius r of a maple tree divided by the average intensity of the hickories. So at a distance of .2 (.2 x 924 ft = 180 ft) from a random maple there is an average of roughly 65 hickories (.093 x 703 hickories). If hickory and maple trees are CSR we would expect about 88 maples (.125 * 703) within that distance.

The presence of a hickory tree reduces the likelihood that a maple tree will be nearby.

Do the same for your EF1 and EF3 tornadoes.
```{r}
plot(Kcross(T.ppp, 
            i = "1", 
            j = "3"))
abline(v = 70)
abline(h = 19000)
abline(h = 15000)

Kc.df <- T.ppp |>
  Kcross(i = "1", 
         j = "3") |>
  as.data.frame()
ggplot(data = Kc.df, 
       mapping = aes(x = r, y = iso)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  geom_vline(xintercept = 70, lty = 'dashed') +
  geom_hline(yintercept = 19000, lty = 'dashed') +
  geom_hline(yintercept = 15000, lty = 'dashed') +
  xlab("Distance") + ylab("Kc(r)") +
  theme_minimal()
```

The vertical axis is the number of EF3 tornadoes within a radius r of an EF1 tornado divided by the average intensity of the EF3 tornadoes. At a distance of 70 km from a random EF1 tornado there are on average 19000 x .000277 = 5.3 EF3 tornadoes. If EF1 and EF3 tornadoes are CSR then we would expect, on average, somewhat fewer EF3 tornadoes in the vicinity of EF1 tornadoes (15000 x .000277 = 4.2).

We can see this more clearly by using the `envelope()` function with the `fun = Kross`. We first use the `subset()` method with `drop = TRUE` to make a new `ppp` object with only those two groups.
```{r}
T.ppp13 <- subset(T.ppp,
                  marks == "1" |
                  marks == "3",
                  drop = TRUE)

Kcenv.df <- T.ppp13 |>
  envelope(fun = Kcross,
           nsim = 99) |>
  as.data.frame()

ggplot(data = Kcenv.df, 
       mapping = aes(x = r, y = obs)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo), color = "red", lty = 'dashed') +
  xlab("Distance (km)") + ylab("Kc(r)") +
  theme_minimal()
```

And we can formally test as before using the `mad.test()` function.
```{r}
mad.test(T.ppp13, fun = Kcross, nsim = 99)
dclf.test(T.ppp13, fun = Kcross, nsim = 99)
```

Both tests lead us to conclude EF3 tornadoes are more likely near EF1 tornadoes than would be expected if they were independently CSR.

## More about the Ripley K function {-}

Last week we used the Ripley K function to detect deviations from CSR.

Compute Ripley $K$ and look at the classes of the resulting object.
```{r}
K <- Kest(T.ppp)
class(K)
```

It has two classes `fv` and `data.frame`. It is a data frame but with additional attribute information. We focus on the data frame portion. 
```{r}
K.df <- as.data.frame(K)
head(K.df)
```

In particular we want the values of `r` and `iso`. The value of `iso` times average intensity is the number of tornadoes within a distance `r`.

We add this information to the data frame.
```{r}
library(dplyr)

K.df <- K.df |>
  mutate(nT = summary(T.ppp)$intensity * iso)
```

Suppose we are interested in the average number of tornadoes at a distance of exactly 50 km. We use the `approx()` function to interpolate the value of `nT` at a distance of 50 km.
```{r}
approx(x = K.df$r, 
       y = K.df$nT,
       xout = 50)$y
```

The variance stabilized Ripley $K$ function called the $L$ function is often used instead of $K$.  The sample version of the $L$ function is defined as
$$
\hat{L}(r) = \Big( \hat{K}(r)/\pi\Big)^{1/2}.
$$

For data that is CSR, the $L$ function has expected value $r$ and its variance is approximately constant in $r$. A common plot is a graph of $r - \hat{L}(r)$ against $r$, which approximately follows the horizontal zero-axis with constant dispersion if the data follow a homogeneous Poisson process.
