---
title: "GIS5122: Applied Spatial Statistics"
subtitle: "Fall 2022"
author: "James B. Elsner"
date: "Date compiled: `r Sys.Date()`"
bibliography: ["References.bib"]
biblio-style: apalike
link-citations: yes
github-repo: jelsner/ASS-2022
site: "bookdown::bookdown_site"
documentclass: book
editor_options: 
  chunk_output_type: console
---

# {-}

tidycensus::get_acs(geography = "county", variables = "B01003_001") will get you the latest 2016-2020 ACS estimates

https://walker-data.com/census-r/wrangling-census-data-with-tidyverse-tools.html

<!--chapter:end:index.Rmd-->

# Syllabus {.unnumbered}

Course Title: GIS5122: Applied Spatial Statistics

## Contact information {.unnumbered}

-   Instructor Name: Professor James B. Elsner
-   Instructor Location: Bellamy Building, Room 323a
-   Lesson Hours: TR 8:00-9:15 p.m.
-   Student Hours: TR 9:15-10:30 a.m., 2-3 p.m.

Email: [jelsner\@fsu.edu](mailto:jelsner@fsu.edu){.email}

Links to my professional stuff (if you are curious)

-   [Website](http://myweb.fsu.edu/jelsner/_site/)
-   [GitHub](https://github.com/jelsner/)
-   [Twitter](https://twitter.com/JBElsner)

## Course description and expected learning outcomes {.unnumbered}

This course is for students who want to learn how to analyze, map, and model spatial and geographical data using the R programming language. It assumes that students know basic statistics through multiple linear regression. And that students have some prior experience with using R. Students without any knowledge of R should look various online tutorials (see below).

In this course you will get a survey of the methods used to describe, analyze, and model *spatial* data. Focus will be on applications. Emphasis is given to how spatial **statistical** methods are related through the concept of spatial autocorrelation.

Expected learning outcomes

1.  Learn how and when to apply statistical methods and models to spatial data,
2.  learn various packages in R for analyzing and modeling spatial data, and
3.  learn how to interpret the results of a spatial data model.

The course offers a programming approach to exposing you to spatial statistics. I want to demystify the process and give you confidence that you can analyze and fit spatial models. I believe some investment in honing programming skills will pay dividends for you later on.

But in taking this approach I don't want to give you the false impression that statisticians have the answers. A *working* knowledge of the model fitting process needs to be combined with a good understanding of the context in which you are working.

## Materials and class meetings {.unnumbered}

-   Access to the internet and a computer

-   Lesson and assignment files on GitHub

-   No textbook is required

-   Many excellent online resources are available. Here are some of my favorites

    -   Data Science <https://r4ds.had.co.nz/>
    -   Tidyverse <https://dominicroye.github.io/en/2020/a-very-short-introduction-to-tidyverse/>
    -   Statistics <https://tinystats.github.io/teacups-giraffes-and-statistics/index.html>
    -   Census data <https://walker-data.com/census-r/wrangling-census-data-with-tidyverse-tools.html>

Class meetings

During each lesson I will work through and explain the R code and notes contained within an `xx-Lesson.Rmd` file. The notes in the lesson files are comprehensive, so you can work through them on your own if you are unable to make it to class.

Notes are written using the markdown language. Markdown is a way to write content for the Web. An R markdown file has the suffix `.Rmd` (R markdown file). The file is opened using the RStudio application.

## Grades and ethics {.unnumbered}

You are responsible for:

1.  Reading and running the code in the lesson R markdown files (`.Rmd`) files. You can do this during the remote lessons as I talk and run my code or outside of class on your own
2.  Completing and returning the lab assignments on time

Grades are determined by how well you do on the assignments using the following standard:

-   A: Outstanding: few, in any, errors/omissions
-   B: Good: only minor errors/omissions
-   C: Satisfactory: minor omissions, at least one major error/omission
-   D: Poor: several major errors/omissions
-   F: Fail: many major errors/omissions

I'll use the +/- grading system.

Grades will be posted as they are recorded on [FSU Canvas](https://canvas.fsu.edu)

Academic honor code

<https://fda.fsu.edu/academic-resources/academic-integrity-and-grievances/academic-honor-policy>

Americans With Disabilities Act

Students with disabilities needing academic accommodation should: (1) register with and provide documentation to the Student Disability Resource Center; (2) bring a letter indicating the need for accommodation and what type. This should be done during the first week of classes.

Diversity and inclusiveness

It is my intent to present notes and data that are respectful of diversity: gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, and culture.

## Outline of topics and schedule {.unnumbered}

1.  Working with data and making graphs (\~ 4 lessons)
2.  Working with spatial data and making maps (\~ 5 lessons)
3.  Quantifying spatial autocorrelation and spatial regression (\~ 5 lessons)
4.  Analyzing and modeling point pattern data (\~ 6 lessons)
5.  Estimating variograms and interpolating spatially (\~ 6 lessons)
6.  Other topics (\~ 2 lessons)

| No. of Weeks | Dates                   | Topic              |
|------|-------------------------|--------------------|
| 2    | August 23, 25           | Syllabus and setup |
| 2    | August 30, September 1  | Data frames        |
| 3    | September 6, 8          |                    |
| 4    | September 13, 15        |                    |
| 5    | September 20, 22        |                    |
| 6    | September 27, 29        |                    |
| 7    | October 4, 6            |                    |
| 8    | October 11, 13          |                    |
| 9    | October 18, 20          |                    |
| 10   | October 25, 27          |                    |
| 11   | November 1, 3           |                    |
| 12   | November 8, 10          |                    |
| 13   | November 15, 17         |                    |
| 14   | November 29, December 1 |                    |

28 dates 23 lesson days + 5 lab days

| Lab        | Date            | Lessons   |
|------------|-----------------|-----------|
| 1          | Tuesday September 6 |       |
| 2          | Thursday September 22 |     |
| 3          | Thursday October 13    |
| 4          | Tuesday November 8    |
| 5          | Thursday December 1       |

## Reference materials {.unnumbered}

1.  Bivand, R. S., E. J. Pebesma, and V. G. Gomez-Rubio, 2013: Applied Spatial Data Analysis with R, 2nd Edition, Springer. A source for much of the material in the lesson notes.
2.  Lovelace, R. Nowosad, J. and Muenchow, J. Geocomputation with R. <https://geocompr.robinlovelace.net/> A source for some of the material in the lesson notes.
3.  Healy, K., 2018: Data Visualization: A practical introduction, <https://socviz.co/>. This book teaches you how to really look at your data. A source for some of the early material in the lesson notes.
4.  Waller, L. A., and C. A. Gotway, 2004: Applied Spatial Statistics for Public Health Data, John Wiley & Sons, Inc. (Available as an e-book in the FSU library). Good overall reference material for analyzing and modeling spatial data.
5.  Analyzing US Census Data: Methods, Maps, and Models in R <https://walker-data.com/census-r/index.html>

-   Cheat Sheets: <https://rstudio.com/resources/cheatsheets/>
-   R Cookbook: How to do specific things: <https://rc2e.com/>
-   R for Geospatial Processing: <https://bakaniko.github.io/FOSS4G2019_Geoprocessing_with_R_workshop/>
-   Spatial Data Science: <https://keen-swartz-3146c4.netlify.com/>

Maps/graphs

-   Inset maps: <https://geocompr.github.io/post/2019/ggplot2-inset-maps/>
-   {cartography} package in R: <https://riatelab.github.io/cartography/docs/articles/cartography.html>
-   geovisualization with {mapdeck}: <https://spatial.blog.ryerson.ca/2019/11/21/geovis-mapdeck-package-in-r/>
-   3D elevation with {rayshader}: <https://www.rayshader.com/>
-   3D elevation to 3D printer: <https://blog.hoxo-m.com/entry/2019/12/19/080000>
-   Accelerate your plots with {ggforce}: <https://rviews.rstudio.com/2019/09/19/intro-to-ggforce/>
-   Summary statistics and ggplot: <https://ggplot2tutor.com/summary_statistics/summary_statistics/>

Space-time statistics

-   Space-time Bayesian modeling package: <https://cran.r-project.org/web/packages/spTimer/spTimer.pdf>
-   Working with space-time rasters: <https://github.com/surfcao/geog5330/blob/master/week12/raster.Rmd>

Bayesian models

-   Bayesian Linear Mixed Models: Random intercepts, slopes and missing data: <https://willhipson.netlify.com/post/bayesian_mlm/bayesian_mlm/>
-   Doing Bayesian Data Analysis in {brms} and the {tidyverse}: <https://bookdown.org/ajkurz/DBDA_recoded/>
-   Spatial models with INLA: <https://becarioprecario.bitbucket.io/inla-gitbook/index.html>
-   Geospatial Health Data: Modeling and Visualization with {RINLA} and {shiny}: <https://paula-moraga.github.io/book-geospatial/>
-   Bayesian workflow: <https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html#1_questioning_authority>

Spatial data

-   Progress in the R ecosystem for representing and handling spatial data <https://link.springer.com/article/10.1007/s10109-020-00336-0>
-   Google earthengine: <https://earthengine.google.com/>
-   Burden of roof: Revisiting housing costs with {tidycensus}: <https://austinwehrwein.com/data-visualization/housing/>
-   The Care and Feeding of Spatial Data: <https://docs.google.com/presentation/d/1BHlrSZWmw9tRWfYFVsRLNhAoX6KzhOhsnezTqL-R0sU/edit#slide=id.g6aeb55b281_0_550>
-   Accessing remotely sensed imagery: <https://twitter.com/mouthofmorrison/status/1212840820019208192/photo/1>
-   Spatial data sets from Brazil: <https://github.com/ipeaGIT/geobr>

Machine learning

-   Supervised machine learning case studies: <https://supervised-ml-course.netlify.com/>
-   Machine learning for spatial prediction: <https://www.youtube.com/watch?v=2pdRk4cj1P0&feature=youtu.be>
-   Machine learning on spatial data: <https://geocompr.robinlovelace.net/spatial-cv.html>

Spatial networks

-   Spatial Networks in R with {sf} and {tidygraph}: <https://www.r-spatial.org/r/2019/09/26/spatial-networks.html>
-   Travel times/distances: <https://github.com/rCarto/osrm>
-   Making network graphs in R - {ggraph} and {tidygraph} introduction <https://youtu.be/geYZ83Aidq4>

Transport planning/routing

<https://docs.ropensci.org/stplanr/index.html> <https://www.urbandemographics.org/post/r5r-fast-multimodal-transport-routing-in-r/>

Time series forecasting

<https://weecology.github.io/MATSS/>

Movement

<https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/1365-2656.13116>

Bookdown

-   Introduction: <https://bookdown.org/yihui/bookdown/introduction.html>
-   Learning more: <https://ropensci.org/blog/2020/04/07/bookdown-learnings/>

Climate data

<https://cran.r-project.org/web/packages/climate/vignettes/getstarted.html> <https://www.ncdc.noaa.gov/teleconnections/enso/indicators/soi/data.csv> USGS water data: <https://waterdata.usgs.gov/blog/dataretrieval/>

Reference books

-   Anselin, L., 2005: Spatial Regression Analysis in R, Spatial Analysis Laboratory, Center for Spatially Integrated Social Science.
-   Baddeley, A., and R. Turner, 2005: spatstat: An R Package for Analyzing Spatial Point Patterns, Journal of Statistical Software, v12.
-   Blangiardo, M., and M. Cameletti, 2015: Spatial and Spatio-temporal Bayesian Models with R-INLA, John Wiley & Sons, Inc., New York. An introduction to Bayesian models for spatial data.
-   Cressie, N. A. C., 1993: Statistics for Spatial Data, Wiley Series in Probability and Mathematical Statistics, John Wiley & Sons, Inc., New York. A mathematical treatment of spatial data analysis.
-   Cressie, N. A. C., and C. K. Wikle, 2011: Statistics for Spatio-Temporal Data, Wiley Series in Probability and Mathematical Statistics, John Wiley & Sons, Inc., New York. A mathematical treatment of space-time statistics with an emphasis on Bayesian models.
-   Diggle, P. J., 2003: Statistical Analysis of Spatial Point Patterns, Second Edition, Arnold Publishers. An introduction to the concepts and methods of statistical analysis of spatial point patterns.
-   Fotherhingham, A. S., C. Brunsdon, and M. Charlton, 2000: Quantitative Geography: Perspectives on Spatial Data Analysis, SAGE Publications, London. A survey of spatial data analysis from the perspective of modern geography.
-   Haining, R., 2003: Spatial Data Analysis: Theory and Practice, Cambridge University Press. A confluence of geographic information science and applied spatial statistics.
-   Illian, J., A. Penttinen, H. Stoyan, and D. Stoyan, 2008: Statistical Analysis and Modeling of Spatial Point Patterns, Wiley Series in Statistics in Practice, John Wiley & Sons, Inc., New York. A mathematical treatment of spatial point processes.
-   Ripley, B. D., 1981: Spatial Statistics, Wiley, New York. A reference book on spatial data analysis with emphasis on point pattern analysis.
-   Wickham, H., 2009: ggplot2: Elegant Graphics for Data Analysis, Springer UseR! Series, Springer, New York. An introduction to the ggplot package for graphics.

Recent research examples

-   [More hots](https://eartharxiv.org/q4y8z/)
-   [Stronger tornadoes](https://eartharxiv.org/wpkt9/)

## Reproducible research {.unnumbered}

A scientific paper has at least two goals: announce a new result and convince readers that the result is correct. Scientific papers should describe the results *and* provide a clear protocol to allow repetition and extension.

Analysis and modeling tools should integrate text with code to make it easier to provide a clear protocol of what was done.

-   Such tools make doing research efficient. Changes are made with little effort.
-   Such tools allow others to build on what you've done. Research achieves more faster.
-   Collaboration is easier.
-   Code sharing leads to greater research impact. Research impact leads to promotion & tenure.

Free and open source software for geospatial data has progressed at an astonishing rate. High performance spatial libraries are now widely available.

However, much of it is still not easy to script. Open source Geographic Information Systems (GIS) like QGIS (see <https://qgis.org>) have greatly reduced the 'barrier to entry' but emphasis on the graphical user interface (GUI) makes reproducible research difficult.

Instead here we will focus on a command line interface (CLI) to help you create reproducible work flows.

You might be interested in this article: [Practical reproducibility in geography and geosciences](https://myweb.fsu.edu/jelsner/NustPebesma2020.pdf)

<!--chapter:end:00-Syllabus.Rmd-->

# Tuesday, August 23, 2022 {.unnumbered}

**"Any fool can write code that a computer can understand. Good programmers write code that humans can understand."** --- Martin Fowler

Today

- What this course is about
- Details about lessons, assignments, and grading
- How to get the most out of this course 

-   Is Milwaukee snowier than Madison?
-   Is global warming making hurricanes stronger?
-   Are tornadoes more likely to form over smooth terrain?

-   Understand what this course is about, how it is structured, and what I expect from you
-   Getting set to work with R and RStudio

## Install R and RStudio on your computer {.unnumbered}

First get R

-   Go to <http://www.r-project.org>
-   Select the CRAN (Comprehensive R Archive Network). Scroll to a mirror site
-   Choose the appropriate file for your computer
-   Follow the instructions to install R

Then get RStudio

-   Go to on <http://rstudio.org>
-   Download RStudio Desktop
-   Install and open RStudio

Finally (not required for success in this class), learn git with R

<https://happygitwithr.com/install-git.html>

## Download course materials {.unnumbered}

-   Navigate to \<[[https://github.com/jelsner/ASS-2022\\\\](https://github.com/jelsner/ASS-2022\){.uri}]([https://github.com/jelsner/ASS-2022\\](https://github.com/jelsner/ASS-2022)%7B.uri%7D){.uri}\>
-   Click on the bright green Code button
-   Download ZIP
-   Unzip the file on your computer
-   Open the `ASS-2022.Rproj` file

## Read the syllabus {.unnumbered}

-   Open the `00-Syllabus.Rmd` file under the `Files` tab

## About RStudio {.unnumbered}

-   Written in HTML (like your Web browser)

-   Top menus

    -   File \> New File \> R Markdown
    -   Tools \> Global Options \> Appearance

-   Upper left panel is the markdown file. This is where you put your text and code

    -   Run code chunks from this panel
    -   Output from the operations can be placed in this panel or in the Console (see the gear icon above)
    -   All the text, code, and output can be rendered to an HTML file or a PDF or Word document (see the Knit button above)

-   Upper right panel shows what is in your current environment and the history of the commands you issued

    -   This is also where you can connect to github

-   Lower left panel is the Console

    -   I think of this as a sandbox where you try out small bits of code. If it works and is relevant to what you want to do you move it to the markdown file
    -   This is also where output from running code will be placed
    -   Not a place for plain text

-   Lower right panel shows your project files, the plots that get made, and all the packages associated with the project

    -   The File tab shows the files in the project. The most important one is the .Rmd.
    -   The Plot tab currently shows a blank sheet
    -   The Packages tab shows all the packages that have been downloaded from CRAN and are associated with this project

## Lab assignments {.unnumbered}

You will do all assignments inside a Rmd file.

1.  Get the assignment `Rmd` file from github and rename it to `yourLastName_yourFirstName.Rmd`
2.  Open the `Rmd` file with RStudio
3.  Replace 'Your Name' with your name in the preamble (YAML)
4.  Answer the questions by typing appropriate code between the code-chunk delimiters
5.  Select the Knit button to generate an HTML file
6.  Fix any errors
7.  Email your completed assignment `Rmd` file to [jelsner\@fsu.edu](mailto:jelsner@fsu.edu){.email}

## Getting started with R {.unnumbered}

Applied statistics is the analysis and modeling of data. Use the `c()` function to input small bits of data into R. The function combines (concatenates) items in a list together.

For example, consider a set of hypothetical annual land falling hurricane counts over a ten-year period.

2 3 0 3 1 0 0 1 2 1

You save these 10 integer values in your working directory by typing them into the console as follows. The console is the lower left window.

```{r}
counts <- c(2, 3, 0, 3, 1, 0, 0, 1, 2, 1)
counts
```

You assign the values to an object called `counts`. The assignment operator is an equal sign (`<-` or `=`). Values do not print. They are assigned to an object name. They are printed by typing the object name as we did on the second line. When printed the values are prefaced with a `[1]`. This indicates that the object is a vector and the first entry in the vector has a value of 2 (The number immediately to the right of `[1]`).

Use the arrow keys to retrieve previous commands. Each command is stored in the history file. The up-arrow key moves backwards through the history file. The left and right arrow keys move the cursor along the line.

Then you apply functions to data stored in an object.

```{r}
sum(counts)
length(counts)
sum(counts)/length(counts)
mean(counts)
```

The function `sum()` totals the number of hurricanes over all ten years, `length()` gives the number of elements in the vector. There is one element (integer value) for each year, so the function returns a value of 10.

Other functions include `sort()`, `min()`, `max()`, `range()`, `diff()`, and `cumsum()`. Try these functions on the landfall counts. What does the `range()` function do? What does the function `diff()` do?

```{r}
diff(counts)
```

The hurricane count data stored in the object `counts` is a vector. This means that R keeps track of the order that the data were entered. There is a first element, a second element, and so on. This is good for several reasons.

The vector of counts has a natural order; year 1, year 2, etc. You don't want to mix these. You would like to be able to make changes to the data item by item instead of entering the values again. Also, vectors are math objects so that math operations can be performed on them.

For example, suppose `counts` contain the annual landfall count from the first decade of a longer record. You want to keep track of counts over other decades. This is done here as follows.

```{r}
d1 <- counts
d2 <- c(0, 5, 4, 2, 3, 0, 3, 3, 2, 1)
```

Most functions operate on each element of the data vector at the same time.

```{r}
d1 + d2
```

The first year of the first decade is added from the first year of the second decade and so on.

What happens if you apply the `c()` function to these two vectors? Try it.

```{r}
c(d1, d2)
```

If you are interested in each year's count as a difference from the decade mean, you type

```{r}
d1 - mean(d1)
```

In this case a single number (the mean of the first decade) is subtracted from a vector. The result is from subtracting the number from each entry in the data vector.

This is an example of data recycling. R repeats values from one vector so that the vector lengths match. Here the mean is repeated 10 times.

## Are you completely new to R? {.unnumbered}

The {swirl} package contains functions to get you started with the basics of R. To install the package use the `install.packages()` function with the name of the package in quotes. The function downloads the package from the Comprehensive R Archive Network (CRAN). You update packages using `update.packages()` function.

To make the functions work in your current session use the `library()` function with the name of the package (without quotes). This needs to be done for every session, but only once per session.

```{r, eval=FALSE}
install.packages("swirl")
library(swirl)
```

Type:

```{r, eval=FALSE}
swirl()
```

Choose the lesson: R Programming. Work through lessons 1:8

Getting help: <https://www.r-project.org/help.html>

<!--chapter:end:01-Lesson.Rmd-->

# Thursday, August 25, 2022 {-}

**"The trouble with programmers is that you can never tell what a programmer is doing until it’s too late."** --- Seymour Cray

Today

- Expectations
- Data science workflow with R markdown
- An introduction to using R
- Data frames

## Expectations {-}

Lesson Hours: Mon/Wed 9:05 a.m. - 9:55 a.m., Lab Hours: Fri 9:05 a.m. - 9:55 a.m., Student Hours: Mon/Wed 9:55 a.m. - 10:30 a.m. The best way to contact me is through email: <jelsner@fsu.edu>.

This course is a survey of methods to describe, analyze, and model _spatial_ data using R. Focus is on applications. I emphasize how spatial statistical methods are related through the concept of spatial autocorrelation.

During each lesson I will work through and explain the R code within an `xx-Lesson.Rmd` file. The notes in the files are comprehensive, so you can work through them on your own. The notes are written using the markdown language.

Grades are determined by how well you do on the weekly assignments.

There are online sites dedicated to all aspects of the R programming language. A list of some of the ones related to spatial analysis and modeling are in the syllabus.

You should now be set up with R and RStudio. If not I will help you after class. I will spend the first several lessons teaching you how to work with R. For some of you this material might be a review. 

On the other hand, if this is entirely new don't get discouraged. This class does not involve writing complex code.

Today I review how to work with small bits of data using functions from the {base} packages. The {base} packages are included in your installation. They form the scaffolding for working with the code, but much of what you will do in this class involve functions from other packages.

The one exception is that I introduce functions from the {readr} package today that simplify getting data into R. These functions are similar to the corresponding functions in the {base} package.

## Data science workflow with R markdown {-}

A scientific paper is _advertisement_ for a claim about the world. The _proof_ is the procedure that was used to obtain the result that under girds the claim. The computer code is the exact procedure. 

Computer code is the recipe for what was done. It is the most efficient way to communicate precisely the steps involved. Communication to others and to your future self.

When you use a spreadsheet, it's hard to explain to someone precisely what you did. Click here, then right click here, then choose menu X, etc. The words you use to describe these types of procedures are not standard.

If you've ever made a map using GIS you know how hard it is to make another (even similar one) with a new set of data. Running code with new data is simple. 

Code is an efficient way to communicate because all important information is given as plain text without ambiguity. Being able to code is a key skill for most technical jobs.

The person most likely to reproduce our work a few months later is us. This is especially true for graphs and figures. These often have a finished quality to them as a result of tweaking and adjustments to the details. This makes it hard to reproduce later. The goal is to do as much of this tweaking as possible with the code we write, rather than in a way that is invisible (retrospectively). Contrast editing an image in Adobe Illustrator.

In data science we toggle between:

(1) Writing code: Code to get our data into R, code to look at tables and summary statistics, code to make graphs, code to compute spatial statistics, code to model and plot our results.

(2) Looking at output: Our code is a set of instructions that produces the output we want: a table, a model, or a figure. It is helpful to be able to see that output.

(3) Taking notes: We also write text about what we are doing, why we are doing it, and what our results mean.

To do be efficient we write our code and our comments _together_ in the same file. This is where R markdown comes in (files that end with `.Rmd`). An R markdown file is a plain text document where text (such as notes or discussion) is interspersed with pieces, or chunks, of R code. When we `Knit` this file the code is executed (from the top to the bottom of the file) and the results supplement or replace the code with output. 

The resulting file is converted into a HTML, PDF, or Word document. The text in the markdown file they has simple format instructions. For example, the following symbols are used for emphasis  _italics_, **bold**, and `code font`. When we create a _new_ markdown document in R Studio, it contains a sample example.

Lesson notes for this class are written in text using markdown formatting as needed. Text is interspersed with code. The format for code chunks is
```{r}
# lines of code here
```


Three back-ticks (on a U.S. keyboard, the character under the escape key) followed by a pair of curly braces containing the name of the language we are using. The back-ticks-and-braces part signal that code is about to begin. We write our code as needed, and then end the chunk with a new line containing three more back-ticks. We can use the _Insert_ button above to save time. 

In the markdown file, the lines between the first and second set of back ticks is grayed and a few small icons are noted in the upper-right corner of the grayed area. The green triangle is used to execute the code and either post the results in the console below or in the line below.

When we keep our notes in this way, we are able to see everything together, the code, the output it produces, and our commentary or clarification on it. Also we can turn it into a good-looking document with one click. This is how we will do everything in this course.

For example, select the _Knit_ button above.

Finally, note the _Outline_ button in the upper right corner of the markdown file. We can organize and navigate through the markdown file section by section based on the pound symbol (`#`).

## An introduction to using R {-}

Applied spatial statistics is the analysis and modeling of data that was collected across space. To begin you need to know about data objects.

The `c()` function is used to create a simple data object (vector object). The function combines (concatenates) individual values into a vector. The length of the vector is the number of data values.

Consider a set of annual land falling hurricane counts over a ten-year period. In the first year there were two hurricanes, the next year there were three, and so on.

2  3  0  3  1  0  0  1  2  1

You save these ten values by assigning them to an object that you call `counts`. The assignment operator is an equal sign (`<-` or `=`). 
```{r}
counts <- c(2, 3, 0, 3, 1, 0, 0, 1, 2, 1)
```

By clicking on the _Environment_ tab in the upper-right panel you see that the object `counts` with numerical values (`num`) 2 3, etc below word _Values_. The elements of the vector object are indexed between 1 and 10 (`1:10`).

You print the values to the console by typing the name of the data object.
```{r}
counts
```

When printed the values are prefaced with a `[1]`. This indicates that the object is a vector and the first element in the vector has a value of 2 (The number immediately to the right of `[1]`).

Note: You can assign and print by wrapping the entire line of code in parentheses.
```{r}
( counts <- c(2, 3, 0, 3, 1, 0, 0, 1, 2, 1) )
```

You can use the arrow keys on your keyboard to retrieve previous commands. Each command is stored in the history file (click on the _History_ tab in the upper-right panel). The up-arrow key moves backwards through the history file. The left and right arrow keys move the cursor along the line.

You apply functions to data objects. A function has a name and parentheses. Inside the parentheses are the function arguments. Many functions have only a single argument, the data object.
```{r}
sum(counts)
length(counts)
sum(counts)/length(counts)
mean(counts)
```

The function `sum()` totals the hurricane counts over all years, `length()` returns the number of elements in the vector. Other functions include `sort()`, `min()`, `max()`, `range()`, `diff()`, and `cumsum()`.

The object `counts` that you create is a vector in the sense that the elements are ordered. There is a first element, a second element, and so on. This is good for several reasons.

The hurricane counts have a chronological order: year 1, year 2, etc and you want that ordered reflected in the data object. Also, you would like to be able to make changes to the data values by element. Also, vectors are math objects so that math operations can be performed on them in a natural way.

For example, math tells us that a scalar multiplied by a vector is a vector where each element of the product has been multiplied by the scalar. The asterisk `*` is used for multiplication.
```{r}
10 * counts
```

Further, suppose `counts` contain the annual landfall count from the first decade of a longer record. You want to keep track of counts over other decades.
```{r}
d1 <- counts
d2 <- c(0, 5, 4, 2, 3, 0, 3, 3, 2, 1)
```

Most functions operate on each element of the data vector all at once.
```{r}
d1 + d2
```

The first year of the first decade is added to the first year of the second decade and so on.

What happens if you apply the `c()` function to these two vectors?
```{r}
c(d1, d2)
```

You get a vector with elements from both `d1` and `d2` in the order of first the first decade counts and then the second decade counts.

If you are interested in each year's count as a difference from the average number over the decade you type
```{r}
d1 - mean(d1)
```

In this case a single number (the average of the first decade) is subtracted from each element of the vector.

Suppose you are interested in the inter annual variability in the set of landfall counts. The variance is computed as
$$
\hbox{var}(x) = \frac{(x_1 - \bar x)^2 + (x_2 - \bar x)^2 + \cdots + (x_n - \bar x)^2}{n-1} = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar x)^2
$$


Although the `var()` function computes this, here you see how to do this using simple functions. The key is to find the squared differences and then sum.
```{r}
x <- d1
xbar <- mean(x)
x - xbar
(x - xbar)^2
sum((x - xbar)^2)
n <- length(x)
n
sum((x - xbar)^2)/(n - 1)
var(x)
```

Elements in a vector object must all have the same type. This type can be numeric, as in counts, character strings, as in
```{r}
simpsons <- c('Homer', 'Marge', 'Bart', 'Lisa', 'Maggie')
simpsons
```

Character strings are made with matching quotes, either double, `"`, or single, `'`. If you mix types the values will be coerced into a common type, which is usually a character string. Arithmetic operations do not work on character strings.

Returning to the land falling hurricane counts. Now suppose the National Hurricane Center (NHC) reanalyzes a storm, and that the 6th year of the 2nd decade is a 1 rather than a 0 for the number of landfalls. In this case you change the sixth element to have the value 1.
```{r}
d2[6] <- 1
```

You assign to the 6th year of the decade a value of one. The square brackets `[]` are used to reference elements of the data vector.

It is important to keep this straight: Parentheses `()` are used by functions and square brackets `[]` are used by data objects.
```{r}
d2
d2[2]
d2[-4]
d2[c(1, 3, 5, 7, 9)]
```

The first line prints all the elements of the vector `df2`. The second prints only the 2nd value of the vector. The third prints all but the 4th value. The fourth prints the values with odd element numbers.

To create structured data, for example the integers 1 through 99 you can use the `:` operator.
```{r, eval=FALSE}
1:99
rev(1:99)
99:1
```

The `seq()` function is more general. You specify the sequence interval with the `by =` or `length =` arguments.
```{r}
seq(from = 1, to = 9, by = 2)
seq(from = 1, to = 10, by = 2)
seq(from = 1, to = 9, length = 5)
```

The `rep()` function is used to create repetitive sequences. The first argument is a value or vector that we want repeated and the second argument is the number of times you want it repeated.
```{r}
rep(1, times = 10)
rep(simpsons, times = 2)
```

In the second example the vector `simpsons` containing the Simpson characters is repeated twice.

To repeat each element of the vector use the `each =` argument.
```{r}
rep(simpsons, each = 2)
```

More complicated patterns can be repeated by specifying pairs of equal length vectors. In this case, each element of the first vector is repeated the corresponding number of times specified by the element in the second vector.
```{r}
rep(c("long", "short"),  times = c(2, 3))
```

To find the maximum number of landfalls during the first decade you type
```{r}
max(d1)
```

What years had the maximum?
```{r}
d1 == 3
```

Notice the double equals signs (`==`).  This is a logical operator that tests each value in `d1` to see if it is equal to 3. The 2nd and 4th values are equal to 3 so `TRUE`s are returned. 

Think of this as asking R a question. Is the value equal to 3?  R answers all at once with a vector of `TRUE`'s and `FALSE`'s.

What years had fewer than 2 hurricanes?
```{r}
d1 < 2
```

Now the question is how do you get the vector element corresponding to the `TRUE` values?  That is, which years have 3 landfalls?
```{r}
which(d1 == 3)
```

The function `which.max()` can be used to get the first maximum.
```{r}
which.max(d1)
```

You might also want to know the total number of landfalls in each decade and the number of years in a decade without a landfall. Or how about the ratio of the mean number of landfalls over the two decades.
```{r}
sum(d1)
sum(d2)
sum(d1 == 0)
sum(d2 == 0)
mean(d2)/mean(d1)
```

So there are 85% more landfalls during the second decade. Is this difference statistically significant?

To remove an object from the environment use the `rm()` function.
```{r}
rm(d1, d2)
```

## Data frames {-}

Spatial data frames will be used throughout this course. A spatial data frame is a data frame plus information about the spatial geometry. Let's start with data frames.

A data frame stores data in a tabular format like a spreadsheet. It is a list of vectors each with the same length. It has column names (and sometimes row names).

For example, you create a data frame object `df` containing three vectors `n`, `s`, `b` each with three elements using the `data.frame()` function.
```{r}
n <- c(2, 3, 5) 
s <- c("aa", "bb", "cc") 
b <- c(TRUE, FALSE, TRUE) 

df <- data.frame(n, s, b)
```

To see that the object is indeed a data frame you use the `class()` function with the name of the object inside the parentheses. 
```{r}
class(df)
```

The object `df` is of class `data.frame`. Note that the object name shows up in our _Environment_ under _Data_ and it includes a little blue arrow indicating that you can view it by clicking on the row.

The data frame shows up as a table (like a spreadsheet) in the `View()` mode (see the command in the console below). Caution: This is not advised for large data frames. 

The top line of the table is called the header. Each line below the header contains a row of data, which begins with the name (or number) of the row followed by the data values. 

Each data element is in a cell. To retrieve a data value from a cell, you enter its row and column coordinates in that order in the single square bracket `[]` operator and separated by a column.

Here is the cell value from the first row, second column of `df`.
```{r}
df[1, 2]
```

You can print the column names (located in the top row in the `View()` mode)  with the `names()` function.
```{r}
names(df)
```

The list of names is a vector of length three containing the elements "n", "s", and "b" in that order. 

You access individual columns of a data frame as vectors by appending the dollar sign (`$`) to the object name. For example, to print the values of the column labeled `s` type
```{r}
df$s
```

Many of the packages we will use this semester include example data frames. The data frame called `mtcars`, for instance, contains information extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).
```{r}
class(mtcars)

names(mtcars)
```

The number of data rows and data columns in the data frame are printed using the `nrow()` and `ncol()` functions.
```{r}
nrow(mtcars)
ncol(mtcars)
```

Further details of built-in data frames like `mtcars` is available in the documentation accessed with the `help()` (or `?`) function.
```{r, eval=FALSE}
help(mtcars)
```

If you type the name of the data frame in the console all the data are printed. 
```{r}
mtcars
```

Instead, to get a glimpse of our data we used the functions `head()`, which prints the first six rows, or `str()`, which lists all the columns by data type. 
```{r}
head(mtcars)

str(mtcars)
```

<!--chapter:end:02-Lesson.Rmd-->

# Tuesday August 30, 2022 {-}

**"When I'm explaining some of the tidy verse principles and philosophy in R statistics, I often break down a home baked chunk of code and illustrate that 'it says what it does and it does what it says.'** --- Diane Beldame

Today

- Working with data frames

## Working with data frames {-}

Consider the data frame `studentdata` from the {LearnBayes} package. To access this data frame, you first install the package with the `install.packages()` function. You put the name of the package {LearnBayes} in quotes (single or double). Then to make the functions from the package available to your current session use the `library()` function with the name of the package (unquoted) inside the parentheses.
```{r}
if(!require(LearnBayes)) install.packages(pkgs = "LearnBayes", repos = "http://cran.us.r-project.org")

library(LearnBayes)
```

Note: The argument `repos =` in the `install.packages()` function directs where the package can be obtained on CRAN (comprehensive R archive network). The CRAN repository is set automatically when using RStudio and you can install packages by clicking on _Packages_ > _Install_ in the lower-right panel.

For interactive use you need to specify the repository and when you use the `Knit` button you don't want to install packages that already exist on your computer so you add the conditional `if()` function that says "only install the package IF it is not (`!`) available".

Make a copy of the data frame by assigning it to an object with the name `df` and print the first six rows using the `head()` function.
```{r}
df <- studentdata
head(df)
```

Data frames are like spreadsheets with rows and columns. The rows are the observations (here each row is a student in an intro stats class at Bowling Green State University) and the columns are the variables. Here the variables are answers to questions like what is your height, choose a number between 1 and 10, what time did you go to bed last night, etc. 

The names of the columns are printed using the `names()` function.
```{r}
names(df)
```

All columns are of the same length, but not all students answered all questions so some of the data frame cells contain the missing-value indicator `NA`.

Data values in a data frame are stored in rows and columns and are accessed with bracket notation [row, column] where row is the row number and column is the column number like a matrix. 

For example here you specify the data value in the 10th row and 2nd column (`Height` column) of the `df` data frame.
```{r}
df[10, 2]
```

By specifying only the row index and leaving the column index blank you get all values in that row which corresponds to all the responses given by the 10th student.
```{r}
df[10, ]
```

Drink preference was one of the questions. Responses across all students are available in the column labeled `Drink` as a vector of character values. You list all the different drink preferences by typing
```{r}
df$Drink
```

Some students left that response blank and therefore the response is coded with the missing-value indicator.

The variable type depends on the question asked. For example, answers given to the question of student height result in a numeric variable, answers given to the question about drink preference result in a character (or factor) variable.

For integer, character, and factor variables we summarize the set of responses with the `table()` function.
```{r}
table(df$Drink)
```

There are 113 students who prefer milk, 178 prefer soda, and 355 prefer water. 

We use the `plot()` method to make a draft plot of this table.
```{r}
plot(x = df$Drink)
```

Notice that the sum of the responses is `r sum(table(df$Drink))`, which is less than the total number of students (`r nrow(df)`).

Students who left that question blank are ignored in the `table()` function. To include the missing values you add the argument `useNA = "ifany"` to the `table()` function.
```{r}
table(df$Drink,
      useNA = "ifany")
```

Note: When you want code executed directly within the text you separate the code using single back ticks. This is useful when you write reports that need periodic updates when new data becomes available. Instead if you hard code the values in the text then you need to search the document for these values during each update.

Suppose you are interested in examining how long students reported sleeping. This was not asked directly. You compute it from the `ToSleep` and `WakeUp` times columns. You assign the result of the difference to a column we call `SleepHrs`.
```{r}
df$SleepHrs <- df$WakeUp - df$ToSleep
head(df)
```

Now you have a new numeric variable in the data frame called `SleepHrs`.

You can't table numeric variables, but the `summary()` method prints a set of summary statistics for the set of values.
```{r}
summary(df$SleepHrs)
```

The average number of hours slept is 7.4 with a maximum of 12.5 and a minimum of 2.5. There are four students that did not answer either when they went to sleep or when they woke up questions. 

You use the `hist()` function to construct a histogram of sleep hours.
```{r}
hist(x = df$SleepHrs)
```

The histogram function divides the number of sleep hours into one-hour bins and counts the number of students whose reported sleep hours falls into each bin. For example based on when they said they went to sleep and when the said they woke up, about 100 students slept between five and six hours the night before the survey.

Since the gender of each student is reported, you can make comparisons between those who identify as male and those who identify as female. For instance, do men sleep more than women? You can answer this question graphically with box plots using the `plot()` method. You specify the character variable on the horizontal axis (x) to be gender with the `x =` argument and the numeric variable on the vertical axis (y) with the `y =` argument.
```{r}
plot(x = df$Gender, 
     y = df$SleepHrs)
```

The plot reveals little difference in the amount of sleep.

Repeat for hair cut prices.
```{r}
plot(x = df$Gender, 
     y = df$Haircut)
```

Big difference.

Finally, is the amount of sleep for a student related to when they go to bed? If you place numeric variables on the x and y axes then you get a scatter plot.
```{r}
plot(x = df$ToSleep,
     y = df$SleepHrs)
```

The `ToSleep` variable is centered on midnight so that -2 means a student went to sleep at 10p.

You describe the decreasing relationship with a line through the points. The least-squares line is fit using the `lm()` function and the line is drawn on the existing plot with the `abline()` function applied to the linear regression object `model`.
```{r}
model <- lm(SleepHrs ~ ToSleep, 
            data = df)

plot(x = df$ToSleep,
     y = df$SleepHrs)
abline(model)
```

Tornadoes

Most of the time you will start by getting your data stored in a file into R. Secondary source data should be imported directly from repositories on the Web. When there is no API (application programming interface) to the repository, you need to first download the data.

For example, consider the regularly updated reports of tornadoes in the United States. The data repository is the Storm Prediction Center (SPC) https://www.spc.noaa.gov/wcm/index.html#data.

Here you are interested in the file called `1950-2020_actual_tornadoes.csv`. First you download the file from the site with the `download.file()` function specifying the location (`url =`) and a name you want the file to be called on your computer (`destfile =`).
```{r}
download.file(url = "http://www.spc.noaa.gov/wcm/data/1950-2019_actual_tornadoes.csv",
              destfile = here::here("data", "Tornadoes.csv"))
```

A file called `Tornadoes.csv` should now be located in the directory `data`. Click on the _Files_ tab in the lower-right panel, then select the `data` folder.

Next you read (import) the file as a data frame using the `readr::read_csv()` function from the {tidyverse} group of packages. 
```{r}
Torn.df <- readr::read_csv(file = here::here("data", "Tornadoes.csv"))
```

You preview the data frame using the `head()` function.
```{r}
head(Torn.df)
```

Each row is a unique tornado report. Observations for each report include the day and time, the state (`st`), the maximum EF rating (`mag`), the number of injuries (`inj`), the number of fatalities (`fat`), estimated property losses (`loss`), estimated crop losses (`closs`), start and end locations in decimal degrees longitude and latitude, length of the damage path in miles (`len`), width of the damage in yards (`wid`).

The total number of tornado reports in the data set is returned using the `nrow()` function.
```{r}
nrow(Torn.df)
```

To create a subset of the data frame that contains only tornadoes in years (`yr`) since 2001, you include the logical operator `yr >= 2001` inside the subset operator. The logical operator is placed in front of the comma since you want all _rows_ where the result of the operator returns a value `TRUE`.
```{r}
Torn2.df <- Torn.df[Torn.df$yr >= 2001, ]
```

You see that there are fewer rows (tornado reports) in this new data frame assigned the object name `Torn2.df`.

You subset again, keeping only tornadoes with EF ratings (`mag` variable) greater than zero. Here you _recycle_ the name `Torn2.df`.
```{r}
Torn2.df <- Torn2.df[Torn2.df$mag > 0, ]
```

Now you compute the correlation between EF rating (`mag`) and path length (`len`) with the `cor()` function. The first argument is the vector of EF ratings and the second argument is the vector of path lengths.
```{r}
cor(Torn2.df$mag, Torn2.df$len)
```

Path length is recorded in miles and path width in yards and the EF damage rating variable `mag` is numeric. To convert path length to kilometers, path width to meters, and the EF rating to a factor and then adding these changes as new columns, type
```{r}
Torn2.df$Length <- Torn2.df$len * 1609.34
Torn2.df$Width <- Torn2.df$wid * .9144
Torn2.df$EF <- factor(Torn2.df$mag)
```

Create side-by-side box plots of path length (in kilometers) by EF rating.
```{r}
plot(x = Torn2.df$EF, 
     y = Torn2.df$Length/1000)
```

Hurricane data

Here you import the data directly from the Web by specifying the URL as a character string using the `file =` argument.
```{r}
loc <- "http://myweb.fsu.edu/jelsner/temp/data/US.txt"
USHur.df <- readr::read_table(file = loc)
```

The `dim()` function returns the size of the data frame defined as the number of rows and the number of columns.
```{r}
dim(USHur.df)
```

There are 166 rows and 6 columns in the data frame. Each row is a year and the columns include `Year`, number of hurricanes (`All`), number of major hurricanes (`MUS`), number of Gulf coast hurricanes (`G`), number of Florida hurricanes (`FL`), and number of East coast hurricanes (`E`) in that order. 

To get a glimpse of the data values you list the first six lines of the data frame using the `head()` function.
```{r}
head(USHur.df)
```

The distribution of Florida hurricane counts by year is obtained using the `table()` function and specifying the `FL` column with `df$FL`.
```{r}
table(USHur.df$FL)
```

There are 93 years without a FL hurricane, 43 years with exactly one hurricane, 24 years with two hurricanes, and so on.

Rainfall data

The data are monthly statewide average rainfall (in inches) for Florida starting in 1895 from http://www.esrl.noaa.gov/psd/data/timeseries/. Note: I put values into a text editor and then uploaded the file to the Web at location http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt.

To import the data you use the `readr::read_table()` function and assign the object the name `FLp.df`.  You type the name of the object to see that it is a tabled data frame (tibble) with 117 rows and 13 columns.
```{r}
loc <- "http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt"
FLp.df <- readr::read_table(file = loc)
FLp.df
```

The first column is the year and the next 12 columns are the months. 

What was the statewide rainfall during June of 1900?
```{r}
FLp.df$Year == 1900

FLp.df$Jun[FLp.df$Year == 1900]
```

What year had the wettest March?
```{r}
FLp.df$Mar

max(FLp.df$Mar)

which.max(FLp.df$Mar)

FLp.df$Year[which.max(FLp.df$Mar)]
```

What month during 1965 was the wettest? How wet was it?
```{r}
FLp.df$Year == 1965

FLp.df[FLp.df$Year == 1965, ]

which.max(FLp.df[FLp.df$Year == 1965, 2:12])

which.max(FLp.df[FLp.df$Year == 1965, 2:12])

max(FLp.df[FLp.df$Year == 1965, 2:12])
```

Using functions from the {dplyr} package

The functions in the {dplyr} package simplify working with data frames. The functions work only on data frames. 

The function names are English language _verbs_ so they are easy to remember. The verbs help you to translate your thoughts into code.

We consider the verbs one at a time using the `airquality` data frame. The data frame contains air quality measurements taken in New York City between May and September 1973. (`?airquality`). 
```{r}
dim(airquality)
head(airquality)
```

The columns include `Ozone` (ozone concentration in ppb), `Solar.R` (solar radiation in langleys), `Wind` (wind speed in mph), `Temp` (air temperature in degrees F), `Month`, and `Day`.

We get summary statistics on the values in each column with the `summary()` method.
```{r}
summary(airquality)
```

Note that columns that have missing values are tabulated. For example, there are 37 missing ozone measurements and 7 missing radiation measurements.

Importantly for making your code more human readable you can apply the `summary()` function on the `airquality` data frame using the pipe operator (`|>`).
```{r}
airquality |> summary()
```

You read the pipe as THEN. "take the airquality data frame THEN summarize the columns".

The pipe operator allows you to string together functions that when read by a human makes it easy to understand what is being done.

Hypothetically, suppose the object of interest is called `me` and there was a function called `wake_up()`. I could apply this function called `wake_up()` in two ways.
```{r, eval=FALSE}
wake_up(me)  # way number one

me |> wake_up()  # way number two
```

The second way involves a bit more typing but it is easier to read (the subject comes before the predicate) and thus easier to understand. This becomes clear when stringing together functions. 

Continuing with the hypothetical example, what happens to the result of `me` after the function `wake_up()` has been applied? I `get_out_of_bed()` and then `get_dressed()`. 

Again, you can apply these functions in two ways.
```{r, eval=FALSE}
get_dressed(get_out_of_bed(wake_up(me)))

me |>
  wake_up() |>
  get_out_of_bed() |>
  get_dressed()
```

The order of the functions usually matters to the outcome. 

Note that I format the code to make it easy to read. Each line is gets only one verb and each line ends with the pipe (except the last one).

Continuing...
```{r, eval=FALSE}
me |>
  wake_up() |>
  get_out_of_bed() |>
  get_dressed() |>
  make_coffee() |>
  drink_coffee() |>
  leave_house()
```

Which is much better in terms of 'readability' then `leave_house(drink_coffee(make_coffee(get_dressed(get_out_of_bed(wake_up(me))))))`.

Tibbles are data frames that make life a little easier. R is an old language, and some things that were useful 10 or 20 years ago now get in your way. To make a data frame a tibble (tabular data frame) use the `as_tibble()` function.
```{r}
class(airquality)
airquality <- dplyr::as_tibble(airquality)
class(airquality)
```

Click on `airquality` in the environment. It is a data frame. We will use the terms 'tibble' and 'data frame' interchangeably in this class.

Now you are ready to look at some of the commonly used verbs and to see how to apply them to a data frame.

The function `select()` chooses variables by name. For example, choose the month (`Month`), day (`Day`), and temperature (`Temp`) columns.
```{r}
airquality |>
  dplyr::select(Month, Day, Temp)
```

The result is a data frame containing only the three columns with column names listed in the `select()` function.

Suppose you want a new data frame with only the temperature and ozone concentrations. You include an assignment operator (`<-`) and an object name (here `df`).
```{r}
df <- airquality |>
        dplyr::select(Temp, Ozone)
df
```

The verbs take only data frames as input and return only data frames.

The function `filter()` chooses observations based on specific values. Suppose we want only the observations where the temperature is at or above 80 F.
```{r}
airquality |>
  dplyr::filter(Temp >= 80)
```

The result is a data frame with the same 6 columns but now only 73 observations. Each of the observations has a temperature of at least 80 F.

Suppose you want a new data frame keeping only observations when temperature is at least 80 F and when winds are less than 5 mph.
```{r}
df <- airquality |> 
  dplyr::filter(Temp >= 80 & Wind < 5)
df
```

The function `arrange()` orders the rows by values given in a particular column.
```{r}
airquality |>
  dplyr::arrange(Solar.R)
```

The ordering is done from the lowest value of radiation to highest value. Here you see the first 10 rows. Note `Month` and `Day` are no longer chronological.

Repeat but order by the value of air temperature.
```{r}
airquality |>
  dplyr::arrange(Temp)
```

Importantly you can string the functions together. For example select the variables radiation, wind, and temperature then filter by temperatures above 90 F and arrange by temperature.
```{r}
airquality |>
  dplyr::select(Solar.R, Wind, Temp) |>
  dplyr::filter(Temp > 90) |>
  dplyr::arrange(Temp)
```

The result is a data frame with three columns and 14 rows arranged by increasing temperatures above 90 F. 

The `mutate()` function adds new columns to the data frame. For example, create a new column called `TempC` as the temperature in degrees Celsius. Also create a column called `WindMS` as the wind speed in meters per second.
```{r}
airquality |>
  dplyr::mutate(TempC = (Temp - 32) * 5/9,
                WindMS = Wind * .44704) 
```

The resulting data frame has 8 columns (two new ones) labeled `TempC` and `WindMS`.

On days when the temperature is below 60 F add a column giving the apparent temperature based on the cooling effect of the wind (wind chill) and then arrange from coldest to warmest apparent temperature.
```{r}
airquality |>
  dplyr::filter(Temp < 60) |>
  dplyr::mutate(TempAp = 35.74 + .6215 * Temp - 35.75 * Wind^.16 + .4275 * Temp * Wind^.16) |>
  dplyr::arrange(TempAp)
```

The `summarize()` function reduces the data frame based on a function that computes a statistic. For examples, to compute the average wind speed during July or the average temperature during June type
```{r}
airquality |>
  dplyr::filter(Month == 7) |>
  dplyr::summarize(Wavg = mean(Wind))

airquality |>
  dplyr::filter(Month == 6) |>
  dplyr::summarize(Tavg = mean(Temp))
```

We've seen functions that compute statistics including `sum()`, `sd()`, `min()`, `max()`, `var()`, `range()`, `median()`. Others include:

Summary function  | Description
-----------------:|:-----------
`dplyr::n()`             | Length of the column
`dplyr::first()`         | First value of the column
`dplyr::last()`          | Last value of the column
`dplyr::n_distinct()`    | Number of distinct values

Find the maximum and median wind speed and maximum ozone concentration values during the month of May. Also determine the number of observations during May.
```{r}
airquality |>
  dplyr::filter(Month == 5) |>
  dplyr::summarize(Wmax = max(Wind),
                   Wmed = median(Wind),
                   OzoneMax = max(Ozone),
                   NumDays = dplyr::n())
```

The result gives an `NA` for the maximum value of ozone (`OzoneMax`) because there is at least one missing value in the `Ozone` column. You fix this with the `na.rm = TRUE` argument in the function `max()`.
```{r}
airquality |>
  dplyr::filter(Month == 5) |>
  dplyr::summarize(Wmax = max(Wind),
                   Wmed = median(Wind),
                   OzoneMax = max(Ozone, na.rm = TRUE),
                   NumDays = dplyr::n())
```

If you want to summarize separately for each month you use the `group_by()` function. You split the data frame by some variable (e.g., `Month`), apply a function to the individual data frames, and then combine the output.

Find the highest ozone concentration by month. Include the number of observations (days) in the month.
```{r}
airquality |>
  dplyr::group_by(Month) |>
  dplyr::summarize(OzoneMax =  max(Ozone, na.rm = TRUE),
                  NumDays = dplyr::n())
```

Find the average ozone concentration when temperatures are above and below 70 F. Include the number of observations (days) in the two groups.
```{r}
airquality |>
  dplyr::group_by(Temp >= 70) |>
  dplyr::summarize(OzoneAvg =  mean(Ozone, na.rm = TRUE),
                   NumDays = dplyr::n())
```

On average ozone concentration is higher on warm days (Temp >= 70 F) days. Said another way; mean ozone concentration statistically depends on temperature.

The mean is a model for the data. The statistical dependency of the mean implies that a model for ozone concentration will be improved by including temperature as an explanatory variable.

In summary, the important verbs are

Verb          | Description
-------------:|:-----------
`select()`    | selects columns; pick variables by their names
`filter()`    | filters rows; pick observations by their values
`arrange()`   | re-orders the rows
`mutate()`    | creates new columns; create new variables with functions of existing variables
`summarize()` | summarizes values; collapse many values down to a single summary
`group_by()`  | allows operations to be grouped

The six functions form the basis of a grammar for data. You can only alter a data frame by reordering the rows (`arrange()`), picking observations and variables of interest (`filter()` and `select()`), adding new variables that are functions of existing variables (`mutate()`), collapsing many values to a summary (`summarise()`), and conditioning on variables (`group_by()`).

The syntax of the functions are all the same:

* The first argument is a data frame. This argument is implicit when using the `|>` operator.
* The subsequent arguments describe what to do with the data frame. You refer to columns in the data frame directly (without using `$`).
* The result is a new data frame

These properties make it easy to chain together many simple lines of code to do complex data manipulations and summaries all while making it easy to read by humans.

<!--chapter:end:03-Lesson.Rmd-->

# Thursday September 1, 2022 {-}

**"Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread."** --- Hadley Wickham

Today

- Making graphs

Working with data frames is part of the iterative cycle of data science, along with visualizing, and modeling. The iterative cycle of data science:

1. Generate questions about our data.
2. Look for answers by visualizing and modeling the data after the data are in suitably arranged data frames.
3. Use what we learn to refine our questions and/or ask new ones.

Questions are tools to guide your investigation. When you ask a question, the question focuses your attention on a specific part of the data set and helps you decide what to do.

For additional practice working with data frames using functions from the {tidyverse} set of packages.

* See http://r4ds.had.co.nz/index.html
* Cheat sheets http://rstudio.com/cheatsheets

Before moving on, let's consider another data frame. The data frame contains observations on Palmer penguins and is available from https://education.rstudio.com/blog/2020/07/palmerpenguins-cran/. 

You import the data frame using the `read_csv()` function.
```{r}
loc <- "https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv"
penguins <- readr::read_csv(file = loc)
penguins
```

The observations are 344 individual penguins each described by species (Adelie, Chinstrap, Gentoo), where it was found (island name), length of bill (mm), depth of bill (mm), body mass (g), male or female, and year.

Each penguin belongs to one of three species. To see how many of the 344 penguins are in each species you use the `table()` function.
```{r}
table(penguins$species)
```

There are 152 Adelie, 68 Chinstrap, and 124 Gentoo penguins.

To create a data frame that includes only the female penguins you type
```{r}
( df <- penguins |> 
          dplyr::filter(sex == "female") )
```
  
To create a data frame that includes only penguins that are not of species Adalie you type
```{r}
( df <- penguins |> 
          dplyr::filter(species != "Adelie") )
```

To create a data frame containing only penguins that weigh more than 6000 grams you type
```{r}
( df <- penguins |> 
          dplyr::filter(body_mass_g > 6000) )
```

To create a data frame with female penguins that have flippers longer than 220 mm we type
```{r}
( df <- penguins |> 
          dplyr::filter(flipper_length_mm > 220 & 
                        sex == "female") )
```

To create a data frame containing rows where the bill length value is NOT missing.
```{r}
( df <- penguins |> 
          dplyr::filter(!is.na(bill_length_mm)) )
```

Note that this filtering will keep rows with other column values missing values but there will be no penguins where the `bill_length` value is `NA`.

Finally, to compute the average bill length for each species.
```{r}
penguins |>
  dplyr::group_by(species) |>
  dplyr::summarize(AvgBL = mean(bill_length_mm, na.rm = TRUE))
```

## Making graphs {-}

The {ggplot2} package is a popular graphics tool among data scientists (e.g., New York Times and 538). Functionality is built on principles of good data visualization.

1.  Mapping data to aesthetics
2.  Layering
3.  Building plots step by step

You make the functions available to your current working directory by typing
```{r}
library(ggplot2)
```

Map data to aesthetics

Consider the following numeric vectors (`foo`, `bar` and `zaz`). Create a data frame `df` using the `data.frame()` function.
```{r}
foo <- c(-122.419416,-121.886329,-71.05888,-74.005941,-118.243685,-117.161084,-0.127758,-77.036871,
         116.407395,-122.332071,-87.629798,-79.383184,-97.743061,121.473701,72.877656,2.352222,
         77.594563,-75.165222,-112.074037,37.6173)

bar <- c(37.77493,37.338208,42.360083,40.712784,34.052234,32.715738,51.507351,38.907192,39.904211,
         47.60621,41.878114,43.653226,30.267153,31.230416,19.075984,48.856614,12.971599,39.952584,
         33.448377,55.755826)

zaz <- c(6471,4175,3144,2106,1450,1410,842,835,758,727,688,628,626,510,497,449,419,413,325,318)

df <- data.frame(foo, bar, zaz)

head(df)
```

To make a scatter plot you use the `ggplot()` function. Note that the package name is {ggplot2} but the function is `ggplot()` (without the 2).

Inside the `ggplot()` function you specify the data frame with the `data =` argument. You also specify what columns from the data frame are to be mapped to what 'aesthetics' with the `aes()` function using the `mapping =` argument. The `aes()` function is nested inside the `ggplot()` function or inside a layer function.

For a scatter plot the aesthetics must include the x and y coordinates at a minimum, and for this example they are in the columns labeled `foo` and `bar` respectively.

Then to render the scatter plot you include the function `geom_point()` as a layer with the `+` symbol. Numeric values are specified using the arguments `x =` and `y =` in the `aes()` function and are rendered as points on a plot.
```{r}
ggplot(data = df, 
       mapping = aes(x = foo, y = bar)) +
  geom_point()
```

You map data values to aesthetic attributes. The *points* in the scatter plot are geometric objects that get drawn. In {ggplot2} lingo, the points are *geoms*. More specifically, the points are point *geoms* that are denoted syntactically with the function `geom_point()`.

All geometric objects have aesthetic attributes (aesthetics):

-   x-position
-   y-position
-   color
-   size
-   transparency

You create a mapping between variables in your data frame and the aesthetic attributes of geometric objects. In the scatter plot you mapped `foo` to the x-position aesthetic and `bar` to the y-position aesthetic. This may seem trivial `foo` is the x-axis and `bar` is on the y-axis. You certainly can do that in Excel.

Here there is a deeper structure. Theoretically, geometric objects (i.e., the things you draw in a plot, like points) don't just have attributes like position. They have a color, size, etc.

For example here you map a new variable to the size aesthetic.
```{r}
ggplot(data = df, 
       mapping = aes(x = foo, y = bar)) +
  geom_point(mapping = aes(size = zaz))
```

You changed the scatter plot to a bubble chart by mapping a new variable to the size aesthetic. Any visualization can be deconstructed into *geom* specifications and a mapping from data to the aesthetic attributes of the geometric objects.

Build plots in layers

The principle of layering is important. To create good visualizations you often need to:

-   Plot multiple datasets, or
-   Plot a dataset with additional contextual information contained in a second dataset, or
-   Plot summaries or statistical transformations over the raw data

Let's modify the bubble chart by getting additional data and plotting it as a new layer below the bubbles. First get the data from the {maps} package using the `map_data()` function and specifying the name of the map (here `"World"`) and assigning it to a data frame with the name `df2`.
```{r}
df2 <- map_data(map = "world") |>
  dplyr::glimpse()
```

Plot the new data as a new layer underneath the bubbles.
```{r}
ggplot(data = df, 
       aes(x = foo, y = bar)) +
  geom_polygon(data = df2, 
               mapping = aes(x = long, y = lat, group = group)) +
  geom_point(mapping = aes(size = zaz), color = "red")
```

This is the same bubble chart but now with a new layer added. You changed the bubble chart into a new visualization called a "dot distribution map," which is more insightful and visually interesting.

The bubble chart is a modified scatter plot and the dot distribution map is a modified bubble chart.

You used two of the data visualization principles (mapping & layering) to build this plot:

-   To create the scatter plot, you mapped `foo` to the x-aesthetic and mapped `bar` to the y-aesthetic
-   To create the bubble chart, you mapped a `zaz` to the size-aesthetic
-   To create the dot distribution map, you added a layer of polygon data under the bubbles.

Iteration (step by step)

The third principle is about process. The graphing process begins with mapping and layering but ends with iteration when you add layers that modify scales, legends, colors, etc. The syntax of `ggplot` *layerability* enables and rewards iteration.

Instead of plotting the result of the above code for making a bubble chart, assign the result to an object called `p1`. Coping/paste the code from above then include the assignment operator `p1 <-`.
```{r}
p1 <- ggplot(data = df, 
             mapping = aes(x = foo, y = bar)) +
        geom_polygon(data = df2, 
                     mapping = aes(x = long, y = lat, group = group)) +
        geom_point(aes(size = zaz), color = "red")
```

Now modify the axes labels saving the new plot to an object called `p2`.
```{r}
( p2 <- p1 + xlab("Longitude") + ylab("Latitude") )
```

Next modify the scale label.
```{r}
p2 + scale_size_continuous(name = "Venture Capital Investment\n(USD, Millions)\n")
```

Of course you can do this all together with
```{r}
p1 + xlab("Longitude") + 
     ylab("Latitude") +
     scale_size_continuous(name = "Venture Capital Investment\n(USD, Millions)\n")
```

The `facet_wrap()` function is a layer to iterate (repeat) the entire plot conditional on another variable. It is like the `dplyr::group_by()` function in the data grammar.

US tornadoes

Consider the tornado records in the file `Tornadoes.csv`. Import the data using the `readr::read_csv()` function then create new columns called `Year`, `Month` and `EF` using the `dplyr::mutate()` function.
```{r}
( Torn.df <- readr::read_csv(file = here::here("data", "Tornadoes.csv")) |>
  dplyr::mutate(Year = yr,
                Month = as.integer(mo),
                EF = mag) )
```

Next create a data frame (`df`) containing the number of tornadoes by year for the state of Kansas.
```{r}
( df <- Torn.df |>
  dplyr::filter(st == "KS") |>
  dplyr::group_by(Year) |>
  dplyr::summarize(nT = dplyr::n()) )
```

Then use the functions from the {ggplot2} package to plot the number of tornadoes by year using lines to connect the values in order of the variable on the x-axis. 
```{r}
ggplot(data = df,
       mapping = aes(x = Year, y = nT)) +
  geom_line()
```

Note: In the early production stage of research, I like to break the code into steps as above: (1) Import the data, (2) manipulate the data, and (3) plot the data. It is easier to document but it also introduces the potential for mistakes because of the intermediary objects in the environment (e.g., `Torn.df`, `df`). 

Below you bring together the above code to create the time series of Kansas tornado frequency without producing intermediary objects.
```{r, eval=FALSE}
readr::read_csv(file = here::here("data", "Tornadoes.csv")) |>
  dplyr::mutate(Year = yr,
                Month = as.integer(mo),
                EF = mag) |>
  dplyr::filter(st == "KS") |>
  dplyr::group_by(Year) |>
  dplyr::summarize(nT = dplyr::n()) |>
ggplot(mapping = aes(x = Year, y = nT)) +
  geom_line() +
  geom_point()
```

Recall that the `group_by()` function allows you to repeat an operation depending on the value (or level) of some variable. For example to count the number of tornadoes by EF damage rating since 2007 and ignoring missing ratings
```{r}
Torn.df |>
  dplyr::filter(Year >= 2007, EF != -9) |>
  dplyr::group_by(EF) |>
  dplyr::summarize(Count = dplyr::n()) 
```

The result is a table listing the number of tornadoes grouped by EF rating.

Instead of printing the table, you create a bar chart using the `geom_col()` function.
```{r}
Torn.df |>
  dplyr::filter(Year >= 2007, EF != -9) |>
  dplyr::group_by(EF) |>
  dplyr::summarize(Count = dplyr::n()) |>
ggplot(mapping = aes(x = EF, y = Count)) +
  geom_col()
```

The `geom_bar()` function counts the number of cases at each x position so you don't need the `group_by()` and `summarize()` functions.
```{r}
Torn.df |>
  dplyr::filter(Year >= 2007, EF != -9) |>
ggplot(mapping = aes(x = EF)) +
  geom_bar()
```

Improve the bar chart and to make it ready for publication.
```{r}
Torn.df |>
  dplyr::filter(Year >= 2007, EF != -9) |>
  dplyr::group_by(EF) |>
  dplyr::summarize(Count = dplyr::n()) |>
ggplot(mapping = aes(x = factor(EF), y = Count, fill = Count)) +
  geom_bar(stat = "identity") +
  xlab("EF Rating") + 
  ylab("Number of Tornadoes") +
  scale_fill_continuous(low = 'green', high = 'orange') +
  geom_text(aes(label = Count), vjust = -.5, size = 3) +
  theme_minimal() +
  theme(legend.position = 'none') 
```

You create a set of plots with the `facet_wrap()` function. Here you create a set of bar charts showing the frequency of tornadoes by EF rating for each year in the data set since 2004. 

You add the function after the `geom_bar()` layer and use the formula syntax (`~ Year`) inside the parentheses. You interpret the syntax as "plot bar charts conditioned on the variable year."
```{r}
Torn.df |>
  dplyr::filter(Year >= 2004, EF != -9) |>
ggplot(mapping = aes(x = factor(EF))) +
  geom_bar() +
  facet_wrap(~ Year)
```

Hot days in Tallahassee and Las Vegas

The data are [daily weather observations](http://www.ncdc.noaa.gov/cdo-web/datasets) from the Tallahassee International Airport.

Import the data.
```{r}
loc <- "http://myweb.fsu.edu/jelsner/temp/data/TLH_Daily1940-2021.csv"
TLH.df <- readr::read_csv(file = loc)
```

The variables of interest are the daily high (and low) temperature in the column labeled `TMAX` (`TMIN`). The values are in degrees F.

Rename the columns then select only the date and temperature columns.
```{r}
TLH.df <- TLH.df |>
  dplyr::rename(TmaxF = TMAX,
                TminF = TMIN,
                Date = DATE) |>
  dplyr::select(Date, TmaxF, TminF) |>
  dplyr::glimpse()
```

Q: Based on these data, is it getting hotter in Tallahassee? Let's compute the annual average high temperature and create a time series graph.

You use the `year()` function from the {lubridate} package to get a column called `Year`. Since the data only has values through mid May 2022 you keep all observations with the `Year` column value less than 2021. You then use the `group_by()` function to group by `Year`, and the `summarize()` function to get the average daily maximum temperature for each year.

```{r}
df <- TLH.df |>
  dplyr::mutate(Year = lubridate::year(Date)) |>
  dplyr::filter(Year < 2022) |>
  dplyr::group_by(Year) |>
  dplyr::summarize(AvgT = mean(TmaxF, na.rm = TRUE)) |>
dplyr::glimpse()
```

You now have a data frame with two columns: `Year` and `AvgT` (annual average daily high temperature in degrees F). If a day is missing a value it is skipped when computing the average because of the `na.rm = TRUE` argument in the `mean()` function.

Next you use functions from the {ggplot2} package to make a time series graph. You specify the x aesthetic as `Year` and the y aesthetic as the `AvgT` and include point and line layers.
```{r}
ggplot(data = df, 
       mapping = aes(x = Year, y = AvgT)) +
  geom_point(size = 3) +
  geom_line() +
  ylab("Average Annual Temperature in Tallahassee, FL (F)")
```

You can go directly to the graph without saving the resulting data frame. That is, you pipe `|>` the resulting data frame after applying the {dplyr} verbs to the `ggplot()` function. The object in the first argument of the `ggplot()` function is the result (data frame) from the code above. Here you also add a smooth curve through the set of averages with the `geom_smooth()` layer.
```{r}
TLH.df |>
  dplyr::mutate(Year = lubridate::year(Date)) |>
  dplyr::filter(Year < 2022) |>
  dplyr::group_by(Year) |>
  dplyr::summarize(AvgT = mean(TmaxF, na.rm = TRUE)) |>
ggplot(mapping = aes(x = Year, y = AvgT)) +
  geom_point(size = 3) +
  geom_line() +
  ylab("Average Annual Temperature in Tallahassee, FL (F)") +
  geom_smooth() +
  theme_minimal()
```

Q: Is the frequency of extremely hot days increasing over time? Let's consider a daily high temperature of 100 F and above as extremely hot.

Here you count the number of days at or above 100F using the `summarize()` function together with the `sum()` function on the logical operator `>=`. If a day is missing a temperature, you remove it with the `na.rm = TRUE` argument in the `sum()` function.
```{r}
TLH.df |>
  dplyr::mutate(Year = lubridate::year(Date)) |>
  dplyr::filter(Year < 2022) |>
  dplyr::group_by(Year) |>
  dplyr::summarize(N100 = sum(TmaxF >= 100, na.rm = TRUE)) |>
ggplot(mapping = aes(x = Year, y = N100, fill = N100)) + 
  geom_bar(stat = 'identity') + 
  scale_fill_continuous(low = 'orange', high = 'red') +
  geom_text(aes(label = N100), vjust = 1.5, size = 3) +
  scale_x_continuous(breaks = seq(1940, 2020, 10)) +
  labs(title = expression(paste("Number of days in Tallahassee, Florida at or above 100", {}^o, " F")),
       subtitle = "Last official 100+ day was September 18, 2019",
       x = "", y = "") +
#  ylab(expression(paste("Number of days in Tallahassee, FL at or above 100", {}^o, " F"))) +
  theme_minimal() +
  theme(axis.text.x  = element_text(size = 11), legend.position = "none")
```

What does the histogram of daily high temperatures look like?
```{r}
( gTLH <- ggplot(data = TLH.df, 
               mapping = aes(x = TmaxF)) + 
  geom_histogram(binwidth = 1, aes(fill = ..count..)) +
  scale_fill_continuous(low = 'green', high = 'blue') +
  scale_x_continuous() +
  scale_y_continuous() +
  ylab("Number of Days") + 
  xlab(expression(paste("Daily High Temperature in Tallahassee, FL (", {}^o, " F)"))) +
  theme_minimal() +
  theme(legend.position = "none") )
```

Q: The most common high temperatures are in the low 90s, but there are relatively few 100+ days. Why?

Compare the histogram of daily high temperatures in Tallahassee with a histogram of daily high temperatures in Las Vegas, Nevada. Here we repeat the code above but for the data frame `LVG.df`. We then use the operators from the {patchwork} package to plot them side by side.
```{r}
LVG.df <- readr::read_csv(file = "http://myweb.fsu.edu/jelsner/temp/data/LV_DailySummary.csv",
                          na = "-9999")

LVG.df <- LVG.df |>
  dplyr::mutate(TmaxF = round(9/5 * TMAX/10 + 32),
                TminF = round(9/5 * TMIN/10 + 32),
                Date = as.Date(as.character(DATE), 
                               format = "%Y%m%d")) |>
  dplyr::select(Date, TmaxF, TminF)

gLVG <- ggplot(data = LVG.df, 
               mapping = aes(x = TmaxF)) + 
  geom_histogram(binwidth = 1, aes(fill = ..count..)) +
  scale_fill_continuous(low = 'green', high = 'blue') +
  scale_x_continuous() +
  scale_y_continuous() +
  ylab("Number of Days") + 
  xlab(expression(paste("Daily High Temperature in Las Vegas, NV (", {}^o, " F)"))) +
  theme_minimal() +
  theme(legend.position = "none")

#install.packages("patchwork")
library(patchwork)

gTLH / gLVG
```

US population and area by state

The object `us_states` from the {spData} package is a data frame from the U.S. Census Bureau. The variables include the state `GEOID` and `NAME`, the `REGION` (`South`, `West`, etc), `AREA` (in square km), and total population in 2010 (`total_pop_10`) and in 2015 (`total_pop_15`).
```{r}
us_states <- spData::us_states
class(us_states)
head(us_states)
```

The object `us_states` has two classes: simple feature and data frame. It is a data frame that has spatial information stored in the column labeled `geometry`. More about this next lesson.

Note also that the variable `AREA` is numeric with units (km\^2). Thus in order to perform some operations you need to specify units or convert the column using `as.numeric()`. For example, if you want to filter by area keeping only states with an area greater than 300,000 square km you could do the following
```{r, eval = FALSE}
us_states |> 
  dplyr::mutate(Area = as.numeric(AREA)) |>
  dplyr::filter(Area > 300000)
```

For now, suppose you want to plot area versus population for each state including state names on the plot. We note large differences between the minimum and maximum values for both variables.
```{r}
us_states |>
  dplyr::summarize(rA = range(AREA),
                   rP = range(total_pop_15))
```

Let's start with a simple scatter plot using logarithmic scales. The variable `AREA` has units so you convert it to a numeric with the `as.numeric()` function.
```{r}
ggplot(data = us_states,
       mapping = aes(x = as.numeric(AREA),
                     y = total_pop_15)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10()
```

Next you use the {scales} package so the tic labels can be expressed in whole numbers with commas.
```{r}
ggplot(data = us_states,
       mapping = aes(x = as.numeric(AREA),
                     y = total_pop_15)) +
  geom_point() +
  scale_x_log10(labels = scales::comma) +
  scale_y_log10(labels = scales::comma)
```

Next you add text labels. You can do this with `geom_text()` or `geom_label()`
```{r}
ggplot(data = us_states,
       mapping = aes(x = as.numeric(AREA),
                     y = total_pop_15)) +
  geom_point() +
  geom_text(aes(label = NAME)) +
  scale_x_log10(labels = scales::comma) +
  scale_y_log10(labels = scales::comma)
```

The labels are centered on top of the points. To fix this you use functions from the {grepel} package.
```{r}
ggplot(data = us_states,
       mapping = aes(x = as.numeric(AREA),
                     y = total_pop_15)) +
  geom_point() +
  ggrepel::geom_text_repel(aes(label = NAME)) +
  scale_x_log10(labels = scales::comma) +
  scale_y_log10(labels = scales::comma)
```

Finally, since the data object is a simple feature data frame you can make a map.
```{r}
ggplot() + 
  geom_sf(data = spData::us_states, 
          mapping = aes(fill = total_pop_15)) +
  scale_fill_continuous(labels = scales::comma) +
  theme_void()
```

More resources and additional examples

-   ggplot extensions <https://exts.ggplot2.tidyverse.org/>
-   Cheat sheets: <https://rstudio.com/resources/cheatsheets/>
-   More examples: <https://geocompr.robinlovelace.net/> {spData} package.

<!--chapter:end:04-Lesson.Rmd-->

# Thursday September 8, 2022 {-}

**"An awful lot of time I spend "coding" is actually spent copying and pasting (And much of the rest is spent googling)."** – Meghan Duffy

Today

- Working with spatial data
- Geo-computation on simple features

## Working with spatial data {-}

The vector model for data (vector data) represents things in the world using points, lines and polygons. These objects have discrete, well-defined borders and a high level of precision. Of course, precision does not imply accurate.

The raster model for data (raster data) represents continuous fields (like elevation and rainfall) using a grid of cells (raster). A raster aggregates fields to a given resolution, meaning that they are consistent over space and scale-able. The smallest features within the field are blurred or lost.

The choice of which data model to use depends on the application: Vector data tends to dominate the social sciences because human settlements and boundaries have discrete borders. Raster data (e.g., remotely sensed imagery) tends to dominate the environmental sciences because environmental conditions are typically continuous. Geographers, ecologists, demographers use vector and raster data. 

Here we use functions from the {sf} package to work with vector data and functions in the {terra} and {raster} packages to work with raster data sets. We will also look at functions from the new {stars} package that work with both vector and raster data models.

R's spatial ecosystem continues to evolve. Most changes build on what has already been done. Occasionally there is a significant change that builds from scratch. The introduction of the {sf} package in about 2018 (Edzer Pebesma) is a significant change.

Simple features

Simple features is a standard from the Open ('open source') Geospatial Consortium (OGC) to represent geographic information. It condenses geographic forms into a single geometry class.

The standard is used in spatial databases (e.g., PostGIS), commercial GIS (e.g., ESRI) and forms the vector data basis for libraries such as [GDAL](http://www.gdal.org/). A subset of simple features forms the [GeoJSON](http://geojson.org/) standard. The {sf} package supports these classes and includes plotting and other methods.

Functions in the {sf} package work with all common vector geometry types: points, lines, polygons and their respective 'multi' versions (which group together features of the same type into a single feature). These functions also support geometry collections, which contain multiple geometry types in a single object. The `raster` data classes are not supported.

The {sf} package incorporates the three main packages of the spatial R ecosystem: {sp} for the class system, {rgdal} for reading and writing data, and {rgeos} for spatial operations done with GEOS.

Simple features are data frames with a special column for storing the spatial information. The spatial column is called `geometry` (or `geom`). The geometry column is referenced like a regular column.

The difference is that the geometry column is a 'list column' of class `sfc` (simple feature column). And the `sfc` is a set of objects of class `sfg` (simple feature geometries).

![Simple Feature Anatomy](figures/sf_anatomy.png)

-   green box is a simple feature: a single record, or `data.frame` row, consisting of attributes and geometry
-   blue box is a single simple feature geometry (an object of class `sfg`)
-   red box a simple feature list-column (an object of class `sfc`, which is a column in the `data.frame`)
-   the geometries are given in well-known text (WKT) format

Geometries are the building blocks of simple features. Well-known text (WKT) is the way simple feature geometries are coded. Well-known binaries (WKB) are hexadecimal strings readable by computers. GIS and spatial databases use WKB to transfer and store geometry objects. WKT is a human-readable text description of simple features. The two formats are exchangeable.

See: https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry

In WKT format a point is a coordinate in 2D, 3D or 4D space (see `vignette("sf1")` for more information) such as:

-   `POINT (5 2)`

The first number is the x coordinate and the second number is the y coordinate.

A line string is a sequence of points with a straight line connecting the points, for example:

-   `LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)`

Each pair of x and y coordinates is separated by a comma.

A polygon is a sequence of points that form a closed, non-intersecting ring. Closed means that the first and the last point of a polygon have the same coordinates. A polygon has one exterior boundary (outer ring) but it can have interior boundaries (inner rings). An inner ring is called a 'hole'.

-   Polygon without a hole - `POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5))`

Here there are two parentheses to start and two to end the string of coordinates.

-   Polygon with one hole - `POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5), (2 4, 3 4, 3 3, 2 3, 2 4))`

Here the first set of coordinates defines the outer edge of the polygon and the next set of coordinates defines the hole. The outer edge vertexes are connected in a counterclockwise direction. The inner edge vertexes (defining the hole in the polygon) are connected in a clockwise direction.

Simple features allow multiple geometries (hence the term 'geometry collection') using 'multi' version of each geometry type:

-   Multi-point - `MULTIPOINT (5 2, 1 3, 3 4, 3 2)`
-   Multi-string - `MULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4))`
-   Multi-polygon - `MULTIPOLYGON (((1 5, 2 2, 4 1, 4 4, 1 5), (0 2, 1 2, 1 3, 0 3, 0 2)))`

The difference (syntax wise) between a polygon with a hole and a multi-polygon is that the vertexes of each polygon are connected in a counterclockwise direction.

A collection of these is made:

-   Geometry collection - `GEOMETRYCOLLECTION (MULTIPOINT (5 2, 1 3, 3 4, 3 2), LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)))`

Simple feature geometry (`sfg`)

The `sfg` class represents the simple feature geometry types: point, line string, polygon (and their 'multi' equivalents, such as multi points) or geometry collection.

Usually you don't need to create geometries. Geometries are typically part of the spatial data we import. However, there are a set of functions to create simple feature geometry objects (`sfg`) from scratch, if needed. The names of these functions are simple and consistent, as they all start with the `st_` prefix and end with the name of the geometry type in lowercase letters:

-   A point - `st_point()`
-   A linestring - `st_linestring()`
-   A polygon - `st_polygon()`
-   A multipoint - `st_multipoint()`
-   A multilinestring - `st_multilinestring()`
-   A multipolygon - `st_multipolygon()`
-   A geometry collection - `st_geometrycollection()`

An `sfg` object can be created from three data types:

-   A numeric vector - a single point
-   A matrix - a set of points, where each row contains a point - a multi-point or line string
-   A list - any other set, e.g. a multi-line string or geometry collection

To create point objects, you use the `st_point()` function from the {sf} package applied to a numeric vector.
```{r}
sf::st_point(c(5, 2)) # XY point
sf::st_point(c(5, 2, 3)) # XYZ point
```

To create multi-point objects, you use matrices constructed from the `rbind()` function.
```{r}
mp.matrix <- rbind(c(5, 2), c(1, 3), c(3, 4), c(3, 2))
mp.matrix
sf::st_multipoint(mp.matrix)

ls.matrix <- rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2))
sf::st_linestring(ls.matrix)
sf::st_linestring(ls.matrix)

plot(sf::st_multipoint(mp.matrix))
plot(sf::st_linestring(ls.matrix))
```

To create a polygon, you use lists.
```{r}
poly.list <- list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)))
sf::st_polygon(poly.list)

poly.border <- rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))
poly.hole <- rbind(c(2, 4), c(3, 4), c(3, 3), c(2, 3), c(2, 4))
poly.with.hole.list <- list(poly.border, poly.hole)
sf::st_polygon(poly.with.hole.list)

plot(sf::st_polygon(poly.list))
plot(sf::st_polygon(poly.with.hole.list))
```

Simple feature geometry column

One `sfg` object contains a single simple feature geometry. A simple feature geometry column (`sfc`) is a list of `sfg` objects together with information about the coordinate reference system.

For example, to combine two simple features into one object with two features, you use the `st_sfc()` function. This is important since `sfg` represents the geometry column in `sf` data frames.
```{r}
point1 <- sf::st_point(c(5, 2))
point2 <- sf::st_point(c(1, 3))
sf::st_sfc(point1, point2)
```

In most cases, a `sfc` object contains objects of the same geometry type. Thus, when you convert `sfg` objects of type `polygon` into a simple feature geometry column, you end up with an `sfc` object of type `polygon`. A geometry column of multiple line strings would result in an `sfc` object of type `multilinestring`.

An example with polygons.
```{r}
poly.list1 <- list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)))
polygon1 <- sf::st_polygon(poly.list1)
poly.list2 <- list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2)))
polygon2 <- sf::st_polygon(poly.list2)
sf::st_sfc(polygon1, polygon2)

plot(sf::st_sfc(polygon1, polygon2))
```

An example with line strings.
```{r}
mls.list1 <- list(rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)), 
                  rbind(c(1, 2), c(2, 4)))
mls1 <- sf::st_multilinestring((mls.list1))
mls.list2 <- list(rbind(c(2, 9), c(7, 9), c(5, 6), c(4, 7), c(2, 7)), 
                  rbind(c(1, 7), c(3, 8)))
mls2 <- sf::st_multilinestring((mls.list2))
sf::st_sfc(mls1, mls2)

plot(sf::st_sfc(mls1, mls2))
```

An example with a geometry collection.
```{r}
sf::st_sfc(point1, mls1)
```

A `sfc` object also stores information on the coordinate reference systems (CRS). To specify a certain CRS, you use the `epsg` or `proj4string` attributes. The default value of `epsg` and `proj4string` is `NA` (Not Available).
```{r}
sf::st_sfc(point1, point2)
```

All geometries in an `sfc` object must have the same CRS. We add coordinate reference system as a `crs =` argument in `st_sfc()`. The argument accepts an integer with the epsg code (for example, 4326).
```{r}
( sfc1 <- sf::st_sfc(point1, point2, 
                     crs = 4326) )
```

The epsg code is translated to a well-known text (WKT) representation of the CRS.
```{r}
sf::st_crs(sfc1)
```

Here the WKT describes a two-dimensional geographic coordinate reference system (GEOCRS) with a latitude axis first, then a longitude axis. The coordinate system is related to Earth by the WGS84 geodetic datum.

See: https://en.wikipedia.org/wiki/Well-known_text_representation_of_coordinate_reference_systems

Simple feature data frames

Features (geometries) typically come with attributes. The attributes might represent the name of the geometry, measured values, groups to which the geometry belongs, etc.

The simple feature class, `sf`, is a combination of an attribute table (`data.frame`) and a simple feature geometry column (`sfc`). Simple features are created using the `st_sf()` function.

Objects of class `sf` behave like regular data frames.
```{r}
methods(class = "sf")
```

Simple features have two classes, `sf` and `data.frame`. This is central to the concept of simple features: most of the time a `sf` can be treated as, and behaves like, a `data.frame`. Simple features are, in essence, data frames but with a column containing the geometric information.

I refer to simple feature objects redundantly as 'simple feature data frames' to distinguish them from S4 class spatial data frames.

Many of these functions were developed for data frames including `rbind()` (for binding rows of data together) and `$` (for creating new columns). The key feature of `sf` objects is that they store spatial and non-spatial data in the same way, as columns in a `data.frame`.

The geometry column of {sf} objects is typically called `geometry` but any name can be used.

Thus `sf` objects take advantage of R's data analysis capabilities to be used on geographic data. It's worth reviewing how to discover basic properties of vector data objects.

For example, we get information about the size and breadth of the `world` simple feature data frame from the {spData} package using `dim()`, `nrow()`, etc.
```{r}
library(spData)

dim(world)
nrow(world)
ncol(world)
```

The data contains ten non-geographic columns (and one geometry column) with 177 rows each one representing a country.

Extracting the attribute data from an `sf` object is the same as dropping its geometry.
```{r}
world.df <- world |>
  sf::st_drop_geometry()
class(world.df)
```

Example: Temperatures at FSU and at the airport

Suppose you measure a temperature of 25C at FSU and 22C at the airport at 9a on September 6, 2022. Thus, you have specific points in space (the coordinates), the name of the locations (FSU, Airport), temperature values and the date of the measurement. Other attributes might include a urbanity category (campus or city), or a remark if the measurement was made with an automatic station.

Start by creating to `sfg` (simple feature geometry) point objects.
```{r}
FSU.point <- sf::st_point(c(-84.29849, 30.44188))
TLH.point <- sf::st_point(c(-84.34505, 30.39541))
```

Then combine the point objects into a `sfc` (simple feature column) object.
```{r}
our.geometry <- sf::st_sfc(FSU.point, TLH.point, 
                           crs = 4326)
```

Then create a data frame of attributes.
```{r}
our.attributes <- data.frame(name = c("FSU", "Airport"),
                             temperature = c(10, 5),
                             date = c(as.Date("2021-01-27"), as.Date("2021-01-27")),
                             category = c("campus", "airport"),
                             automatic = c(TRUE, FALSE))
```

Finally create a simple feature data frame.
```{r}
sfdf <- sf::st_sf(our.attributes, 
                  geometry = our.geometry)
```

The example illustrates the components of `sf` objects. First, you use coordinates to define the geometry of the simple feature geometry (`sfg`). Second, you can combine the geometries into a simple feature geometry column (`sfc`) which also stores the CRS. Third, you store the attribute information on the geometries in a `data.frame`. Fourth, you use the `st_sf()` function to combine the attribute table and the `sfc` object into a `sf` object.
```{r}
sfdf
class(sfdf)
```

Given a simple feature data frame you create a non-spatial data frame with a geometry list-column but that is not of class `sf` using the `as.data.frame()` function.
```{r}
df <- as.data.frame(sfdf)
class(df)
```

In this case the `geometry` column is

-   no longer a `sfc`.
-   no longer has a plot method, and
-   lacks all dedicated methods listed above for class `sf`

In order to avoid any confusion it might be better to use the `st_drop_geometry()` column instead.

Example: US states 

The object `us_states` from the {spData} package is a simple feature data frame from the U.S. Census Bureau. The variables include the name, region, area, and population.

Simple feature data frames can be treated as regular data frames. But the geometry is "sticky". For example when we create a new data frame containing only the population information the geometry column is included in the new data frame.
```{r}
df1 <- us_states |> 
  dplyr::select(starts_with("total")) 
head(df1)
```

The resulting data frame has the two population columns but also a column labeled `geometry`.

When we use the `summarize()` function, a union of the geometries across rows is made.
```{r}
df2 <- us_states |> 
  dplyr::filter(REGION == "Midwest") |> 
  dplyr::summarize(TotalPop2010 = sum(total_pop_10),
                   TotalPop2015 = sum(total_pop_15))
head(df2)
```

Why use the {sf} package when {sp} is already tried and tested?

-   Fast reading and writing of data
-   Enhanced plotting performance
-   `sf` objects are treated as data frames in most operations
-   `sf` functions are combined using `|>` operator and they work well with the {tidyverse} packages
-   `sf` function names are consistent and intuitive (all begin with `st_`)

These advantages led to the development of spatial packages (including {tmap}, {mapview} and {tidycensus}) that now support simple feature objects.

It is easy to convert between the two classes. Consider the `world` S3 spatial data frame from the {spData} package. You convert it to a S4 spatial data frame with the `as()` method.
```{r}
world.sp <- world |>
  as(Class = "Spatial")
```

The method coerces simple features to `Spatial*` and `Spatial*DataFrame` objects.

You convert a S4 spatial data frame into a simple feature data frame with the `st_as_sf()` function.
```{r}
world.sf <- world.sp |>
  sf::st_as_sf()
```

You can create basic maps from simple feature data frames with the base `plot()` method (`plot.sf()`). The function creates a multi-panel one sub-plot for each variable.

## Geo-computation on simple features {-}

Geo-computation on simple features is done with routines in the geometry engine-open source (GEOS) library that functions in the {sf} package make use of.

As an example, consider the file `police.zip` on my website that contains shapefiles in a folder called `police`. The variables include police expenditures (POLICE), crime (CRIME), income (INC), unemployment (UNEMP) and other socio-economic variables for counties in Mississippi.

Input the data using the `st_read()` function from the {sf} package and then assign a geographic coordinate reference system (CRS) to it using the EPSG number 4326.
```{r}
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/police.zip",
              destfile = here::here("data", "police.zip"))
unzip(here::here("data", "police.zip"),
      exdir = here::here("data"))

sfdf <- sf::st_read(dsn = here::here("data", "police"), 
                    layer = "police") 
sf::st_crs(sfdf) <- 4326
```

The geometries are polygons and there are 82 of them, one for each county.

You transform the geographic coordinate system of the polygons to a specific projected CRS as suggested by the function `suggest_crs()` from the {crsuggest} package.
```{r}
crsuggest::suggest_crs(sfdf)
```

The function for transforming the CRS is `st_transform()` from the {sf} package.
```{r}
sfdf <- sfdf |>
    sf::st_transform(crs = 6508)
```

The `st_centroid()` function computes the geographic center of each polygon in the spatial data frame.
```{r}
countyCenters.sf <- sfdf |>
    sf::st_centroid()
```

The warning lets you know that the attributes attached to each polygon might result in misleading information when attached to the new geometry (points). Different geometries can mean different interpretations of the attribute.
```{r}
sf::st_geometry(countyCenters.sf)
```

To get the centroid location for the state, you first join all the counties using the `st_union()` function, then use the `st_centroid()` function.
```{r}
stateCenter.sfc <- sfdf |>
  sf::st_union() |>
  sf::st_centroid()
```

The result is a simple feature geometry column (`sfc`) with a single row where the geometry contains the centroid location.

Which county contains the geographic center of the state? Here you use the geometric binary predicate `st_contains()`.
```{r}
( Contains <- sfdf |>
    sf::st_contains(stateCenter.sfc,
                    sparse = FALSE) )
```

You include the `sparse = FALSE` argument so the result is a matrix containing `TRUE`s and `FALSE`s. Since there are 82 counties and one centroid the matrix has 82 rows and 1 column. All matrix entries are `FALSE` except the one containing the center.

To map the result you first plot the county polygons, then add the county geometry for the center county and fill it red. Note that you use the matrix you called `Contains` to subset this county. Finally you add the location of the state centroid to the plot.
```{r}
library(ggplot2)

ggplot(data = sfdf) +
  geom_sf() +
  geom_sf(data = sfdf[Contains, ], col = "red") +
  geom_sf(data = stateCenter.sfc) +
  theme_void()
```

The function `st_area()` returns a vector of the geographical area (in sq. units) of each of the spatial objects. Here county boundaries as polygons.
```{r}
sfdf |>
  sf::st_area()
```

The vector values have units of square meters (m^2), which are derived from the CRS.

There is an attribute called `AREA` in the data frame but it is better to calculate it from the spatial polygons because then you are sure of the units.

What happens when you apply the area function on the centroid object?
```{r}
countyCenters.sf |>
  sf::st_area()
```

Compute a 10 km buffer around the state and show the result with a plot. First use `st_union()`, then `st_buffer()`, then pipe the output (a simple feature data frame to `ggplot()`).
```{r}
sfdf |>
  sf::st_union() |>
  sf::st_buffer(dist = 10000) |>
ggplot() +
  geom_sf() +
  geom_sf(data = sfdf) +
  theme_void()
```

Length of boundary lines for U.S. states. Transform the CRS to 2163 (US National Atlas Equal Area). Note that the geometry is multi-polygons. Convert the polygons to multi-linestrings, then use `st_length()` to get the total length of the lines.
```{r}
states <- spData::us_states |>
  sf::st_transform(crs = 2163)

sf::st_length(states) # returns zeroes because geometry is polygon

states |>
  sf::st_cast(to = "MULTILINESTRING") |>
  sf::st_length()
```

<!--chapter:end:05-Lesson.Rmd-->

# Tuesday September 13, 2022 {.unnumbered}

**"Hell isn't other people's code. Hell is your own code from 3 years ago."** -- Jeff Atwood

Today

-   Spatial data subsets and joins
-   Interpolating variables using areal weights

## Spatial data subsets and joins {.unnumbered}

Variables (stored as columns) in spatial data structures are referred to as 'attributes'.

With simple feature data frames you can create data subsets using `[`, `subset()` and `$` from the {base} R packages and `select()` and `filter()` from the {dplyr} package.

The `[` operator subsets rows and columns. Indexes specify the elements you wish to extract from an object, e.g. object[i, j], with i and j typically being numbers representing rows and columns. Leaving i or j empty returns all rows or columns, so `world[1:5, ]` returns the first five rows and all columns of the simple feature data frame `world` (from the {spData} package). Examples

```{r}
world <- spData::world
world[c(1, 5, 9), ] # subset rows by row position
world[, 1:3] # subset columns by column position
world[, c("name_long", "lifeExp")] # subset columns by name
```

Here you use logical vectors to create a subset. First create a logical vector `sel_area`.

```{r}
sel_area <- world$area_km2 < 10000
head(sel_area)
summary(sel_area)
```

Then select only cases from the `world` simple feature data frame where the elements of the `sel_area` vector are `TRUE`.

```{r}
small_countries <- world[sel_area, ]
```

This creates a new simple feature data frame, `small_countries`, containing nations whose surface area is smaller than 10,000 square kilometers.

Note: there is no harm in keeping the geometry column because an operation on a {sf} object only changes the geometry when appropriate (e.g. by dissolving borders between adjacent polygons following aggregation). This means that the speed of operations with attribute data in {sf} objects is the same as with columns in a data frames.

The {base} R function `subset()` provides another way to get the same result.

```{r}
small_countries <- subset(world, 
                          area_km2 < 10000)
```

The {dplyr} verbs work on {sf} spatial data frames. The functions include `dplyr::select()` and `dplyr::filter()`.

CAUTION! The {dplyr} and {raster} packages have a `select()` function. When using both packages in the same session, the function in the most recently attached package will be used, 'masking' the other function. This will generate error messages containing text like: unable to find an inherited method for function 'select' for signature "sf". To avoid this error message, and prevent ambiguity, you should always use the long-form function name, prefixed by the package name and two colons `dplyr::select()`.

The `dplyr::select()` function picks the columns by name or position. For example, you can select only two columns, `name_long` and `pop`, with the following command.

```{r}
world1 <- world |>
  dplyr::select(name_long, pop)
names(world1)
```

The result is a simple feature data frame with the geometry column.

With the `select()` function you can subset and rename columns at the same time. Here you select the columns with names `name_long` and `pop` and give the `pop` column a new name (`population`).

```{r}
world |>
  dplyr::select(name_long, 
                population = pop)
```

The `dplyr::pull()` function returns a single vector without the geometry.

```{r}
world |>
  dplyr::pull(pop)
```

The `filter()` function keeps only rows matching given criteria, e.g., only countries with a very high average life expectancy.

```{r}
world |>
  sf::st_drop_geometry() |>
  dplyr::filter(lifeExp > 82)
```

Aggregation summarizes a data frame by a grouping variable. An example of aggregation is to calculate the number of people per continent based on country-level data (one row per country).

This is done with the `dplyr::group_by()` and `dplyr::summarize()` functions.

```{r}
world |>
  dplyr::group_by(continent) |>
  dplyr::summarize(Population = sum(pop, na.rm = TRUE),
                   nCountries = dplyr::n())
```

The two columns in the resulting table are `Population` and `nCountries`. The functions `sum()` and `dplyr::n()` were the aggregating functions.

The result is a simple feature data frame with a single row representing attributes of the world and the geometry as a single multi-polygon through the geometric *union* operator.

You can chain together functions to find the world's three most populous continents and the number of countries they contain.

```{r}
world |> 
  dplyr::select(pop, continent) |> 
  dplyr::group_by(continent) |> 
  dplyr::summarize(Population = sum(pop, na.rm = TRUE), 
                   nCountries = dplyr::n()) |> 
  dplyr::top_n(n = 3, wt = Population) 
```

If you want to create a new column based on existing columns use `dplyr::mutate()`. For example, if you want to calculate population density for each country divide the population column, here `pop`, by an area column, here `area_km2` with unit area in square kilometers.

```{r}
world |> 
  dplyr::mutate(Population_Density = pop / area_km2)

world |>
  dplyr::transmute(Population_Density = pop / area_km2)
```

The `dplyr::transmute()` function performs the same computation but also removes the other columns (except the geometry column).

Subsetting (filtering) your data based on geographic boundaries

The {USAboundaries} package has historical and contemporary boundaries for the United States provided by the U.S. Census Bureau.

Individual states are extracted using the `us_states()` function. CAUTION: this function has the same name as the object `us_states` from the {spData} package.

Here you use the argument `states =` to get only the state of Kansas. You then make a plot of the boundary and check the native coordinate reference system (CRS).

```{r}
KS.sf <- USAboundaries::us_states(states = "Kansas")

library(ggplot2)

ggplot(data = KS.sf) +
  geom_sf()

sf::st_crs(KS.sf)
```

The polygon geometry includes the border and the area inside the border. The CRS is described by the 4326 EPSG code and implemented using well-known text (WKT).

You use a geometric operation to subset spatial data geographically (rather than on some attribute). For example here you subset the tornado tracks as line strings, keeping only those line strings that fall within the Kansas border defined by a polygon geometry.

First import the tornado data. Note here you first ask if the tornado data file is in our list of files with the `if()` conditional and the `list.files()` function. You only download the data file if the file is not (`!`) in the list.
```{r}
if(!"1950-2020-torn-aspath" %in% list.files(here::here("data"))) {
download.file(url = "http://www.spc.noaa.gov/gis/svrgis/zipped/1950-2020-torn-aspath.zip",
              destfile = here::here("data", "1950-2020-torn-aspath.zip"))
unzip(here::here("data", "1950-2020-torn-aspath.zip"), 
      exdir = here::here("data"))
}

Torn.sf <- sf::st_read(dsn = here::here("data", "1950-2020-torn-aspath"), 
                       layer = "1950-2020-torn-aspath") 
```

The geometries are line strings representing the approximate track of each tornado. The CRS has EPSG code of 4326, same as the Kansas polygon.

To keep only the tornado tracks that fall within the border of Kansas you use the `sf::st_intersection()` function. The first argument (`x =`) is the simple feature data frame that you want to subset and the second argument (`y =`) defines the geometry over which the subset occurs.

```{r}
KS_Torn.sf <- sf::st_intersection(x = Torn.sf, 
                                  y = KS.sf)
```

You can use the pipe operator (`|>`) to pass the first argument to the function.

```{r}
KS_Torn.sf <- Torn.sf |>
  sf::st_intersection(y = KS.sf)
```

You make a plot to see if things appear as you expect.

```{r}
ggplot() +
  geom_sf(data = KS.sf) +
  geom_sf(data = KS_Torn.sf)
```

Note that no tornado track lies outside the state border. Line strings that lie outside the border are clipped at the border. However the attribute values represent the entire track.

If you want the entire tornado track for all tornadoes that passed into (or through) the state, then you first use the geometric binary predict function `sf::st_intersects()`. With `sparse = FALSE` a matrix with a single column of `TRUE`s and `FALSE`s is returned. Here you use the piping operator to implicitly specify the `x =` argument as the `Torn.sf` data frame.

```{r}
Intersects <- Torn.sf |>
  sf::st_intersects(y = KS.sf, sparse = FALSE)

head(Intersects)
sum(Intersects)
```

Next you create a new data frame from the original data frame keeping only observations (rows) where `Interects` is TRUE.

```{r}
KS_Torn2.sf <- Torn.sf[Intersects, ]

ggplot() +
  geom_sf(data = KS.sf) +
  geom_sf(data = KS_Torn2.sf)
```

Are tornadoes more common in some parts of Kansas than others? One way to answer this question is to see how far away the tornado centroid is from the center of the state.

Start by computing the centers of the state polygon and the combined set of Kansas tornadoes using the `sf::st_centroid()` function. Note you first use the `sf::st_combine()` function on the tornadoes.

```{r}
geocenterKS <- KS.sf |>
  sf::st_centroid()

centerKStornadoes <- KS_Torn.sf |>
  sf::st_combine() |>
  sf::st_centroid()
```

Then make a map and compute the distance in meters using the `sf::st_distance()` function.

```{r}
ggplot() +
  geom_sf(data = KS.sf) +
  geom_sf(data = geocenterKS, col = "blue") +
  geom_sf(data = centerKStornadoes, col = "red")

geocenterKS |>
  sf::st_distance(centerKStornadoes)
```

Less than 3 km!

More examples: <https://www.jla-data.net/eng/spatial-aggregation/>

Mutating data frames with joins

Combining data from different sources based on a shared variable is a common operation. The {dplyr} package has join functions that follow naming conventions used in database languages (like SQL).

Given two data frames labeled `x` and `y`, the join functions add columns *from y* *to x*, matching rows based on the function name.

-   `inner_join()`: includes all rows in `x` *and* `y`
-   `left_join()`: includes all rows in `x`
-   `full_join()`: includes all rows in `x` *or* `y`

Join functions work the same on data frames and on simple feature data frames. The most common type of attribute join on spatial data takes a simple feature data frame as the first argument and adds columns to it from a data a frame specified as the second argument.

For example, you combine data on coffee production with the `spData::world` simple feature data frame. Coffee production by country is in the data frame called `spData::coffee_data`.

```{r}
dplyr::glimpse(spData::coffee_data)
```

It has 3 columns: `name_long` names major coffee-producing nations and `coffee_production_2016` and `coffee_production_2017` contain estimated values for coffee production in units of 60-kg bags per year.

First select only the name and GDP (per person) from the `spData::world` simple feature data frame.

```{r}
( world.sf <- spData::world |>
    dplyr::select(name_long, gdpPercap) )
```

The `dplyr::left_join()` function takes the data frame named by the argument `x =` and joins it to the data frame named by the argument `y =`.

```{r}
( world_coffee.sf <- dplyr::left_join(x = world.sf, 
                                      y = spData::coffee_data) )
```

Because the two data frames share a common variable name (`name_long`) the join works without using the `by =` argument. The result is a simple feature data frame identical to the `world.sf` object but with two new variables indicating coffee production in 2016 and 2017.

```{r}
names(world_coffee.sf)
```

For a join to work there must be at least one variable name in common.

Since the object listed in the `x =` argument is a simple feature data frame, the join function returns a simple feature data frame with the same number of rows (observations).

Although there are only 47 rows of data in `spData::coffee_data`, all 177 of the country records in `world.sf` are kept intact in `world_coffee.sf`. Rows in the first dataset with no match are assigned `NA` values for the new coffee production variables.

If you want to keep only countries that have a match in the key variable then use `dplyr::inner_join()`. Here you use the piping operator to implicitly specify the `x =` argument as the `world.sf` data frame.

```{r}
world.sf |>
  dplyr::inner_join(spData::coffee_data)
```

You can join in the other direction as well, starting with a regular data frame and adding variables from a simple features object.

More information on attribute data operations such as these is given here: <https://geocompr.robinlovelace.net/attr.html>

## Interpolation using areal weights {.unnumbered}

Areal-weighted interpolation estimates the value of some variable from a set of polygons to an overlapping but incongruent set of target polygons. For example, suppose you want demographic information given at the Census tract level to be estimated within the tornado damage path. Damage paths do not align with census tract boundaries so areal weighted interpolation is needed to get demographic estimates at the tornado level.

The function `sf::st_interpolate_aw()` performs areal-weighted interpolation of polygon data. As an example, consider the number of births by county in North Carolina in over the period 1970 through 1974 (`BIR74`).

The data are available as a shapefile as part of the {sf} package system file. Use the `sf::st_read()` function together with the `system.file()` function to import the data. Then create a map filling by the `BIR74` variable.

```{r}
nc.sf <- sf::st_read(system.file("shape/nc.shp", 
                                  package = "sf"))

ggplot(data = nc.sf) +
  geom_sf(mapping = aes(fill = BIR74))
```

Next construct a 20 by 10 grid of polygons that overlap the state using the `sf::st_make_grid()` function. The function takes the bounding box from the `nc.sf` simple feature data frame and constructs a two-dimension grid using the dimensions specified with the `n =` argument.

```{r}
g.sfc <- sf::st_make_grid(nc.sf, 
                          n = c(20, 10))

ggplot() +
  geom_sf(data = g.sfc, col = "red") +
  geom_sf(data = nc.sf, fill = "transparent")
```

The result is overlapping but incongruent sets of polygons as a `sfc` (simple feature column).

Then you use the `sf::st_interpolate_aw()` function with the first argument a simple feature data frame for which you want to aggregate a particular variable and the argument `to =` to the set of polygons for which you want the variable to be aggregated. The name of the variable must be put in quotes inside the subset operator `[]`. The argument `extensive =` if `FALSE` (default) assumes the variable is spatially intensive (like population density) and the mean is preserved.

```{r}
a1.sf <- sf::st_interpolate_aw(nc.sf["BIR74"], 
                               to = g.sfc,
                               extensive = FALSE)
```

The result is a simple feature data frame with the same polygons geometry as the `sfc` grid and a single variable called (`BIR74`).

```{r}
( p1 <- ggplot() +  
    geom_sf(data = a1.sf, mapping = aes(fill = BIR74)) +
    scale_fill_continuous(limits = c(0, 18000)) +
    labs(title = "Intensive") )
```

Note that the average number of births across the state at the county level matches (roughly) the average number of births across the grid of polygons, but the sums do not match.

```{r}
mean(a1.sf$BIR74) / mean(nc.sf$BIR74)

sum(a1.sf$BIR74) / sum(nc.sf$BIR74)
```

An *intensive* variable is independent of the spatial units (e.g., population density, percentages); a variable that has been normalized in some fashion. An *extensive* variable depends on the spatial unit (e.g., population totals). Assuming a uniform population density, the number of people will depend on the size of the spatial area.

Since the number of births in each county is an extensive variable, you change the `extensive =` argument to `TRUE`.

```{r}
a2.sf <- sf::st_interpolate_aw(nc.sf["BIR74"], 
                               to = g.sfc, 
                               extensive = TRUE)
( p2 <- ggplot(a2.sf) +  
    geom_sf(mapping = aes(fill = BIR74)) +
    scale_fill_continuous(limits = c(0, 18000)) +
    labs(title = "Extensive") )
```

In this case you preserve the total number of births across the domain. You verify this 'mass preservation' property (pycnophylactic property) with a ratio of one.

```{r}
sum(a2.sf$BIR74) / sum(nc.sf$BIR74)
```

Here you create a plot of both interpolations.

```{r}
library(patchwork)

p1 / p2
```

Example: tornado paths and housing units

Here you are interested in the number of houses (housing units) affected by tornadoes occurring in Florida 2014-2020. You begin by creating a polygon geometry for each tornado record.

Import the data, transform the native CRS to 3857 (pseudo-Mercator), and filter on `yr` (year) and `st` (state).

```{r}
FL_Torn.sf <- Torn.sf |>
  sf::st_transform(crs = 3857) |>
  dplyr::filter(yr >= 2014, 
                st == "FL")
```

Next change the geometries from line strings to polygons to represent the tornado path ('footprint'). The path width is given by the variable labeled `wid`. First you create new a new variable with the width in units of meters and then use the `st_buffer()` function with the `dist =` argument set to 1/2 the width.

```{r}
FL_Torn.sf <- FL_Torn.sf |>
  dplyr::mutate(Width = wid * .9144)

FL_TornPath.sf <- FL_Torn.sf |>
  sf::st_buffer(dist = FL_Torn.sf$Width / 2)
```

To see the change from line string track to polygon path plot both together for one of the tornadoes.

```{r}
ggplot() + 
  geom_sf(data = FL_TornPath.sf[10, ]) +
  geom_sf(data = FL_Torn.sf[10, ], col = "red")
```

Now you want the number of houses within the path. The housing units are from the census data. You can access these data with the `tidycensus::get_acs()` function. The {tidycensus} package is an interface to the decennial US Census and American Community Survey APIs and the US Census Bureau's geographic boundary files. Functions return Census and ACS data as simple feature data frames for all Census geographies.

Note: You need to get an API key from U.S. Census. Then

```{r, eval=FALSE}
file.create("CensusAPI") # open then copy/paste your API key
```

To ensure the file is only readable by you, not by any other user on the system use the function `Sys.chmod()` then read the key and install it.

```{r, eval=FALSE}
Sys.chmod("CensusAPI", mode = "0400")
key <- readr::read_file("CensusAPI")
tidycensus::census_api_key(key, install = TRUE, overwrite = TRUE)
readRenviron("~/.Renviron")
```

Make sure the file is listed in the file `.gitignore` so it doesn't get included in your git public repository.

The geometry is the tract level and the variable is the un-weighted sample housing units (B00002_001). Transform the CRS to that of the tornadoes.

```{r}
Census.sf <- tidycensus::get_acs(geography = "tract", 
                                 variables = "B00002_001",
                                 state = "FL",
                                 year = 2015,
                                 geometry = TRUE) |>
  sf::st_transform(crs = sf::st_crs(FL_TornPath.sf))

head(Census.sf)
```

The column labeled `estimate` is the estimate of the number of housing units within the census tract.

Finally you use the `sf::st_interpolate_aw()` function to spatially interpolate the housing units to the tornado path.

```{r}
awi.sf <- sf::st_interpolate_aw(Census.sf["estimate"],
                                to = FL_TornPath.sf, 
                                extensive = TRUE)
head(awi.sf)
range(awi.sf$estimate, 
      na.rm = TRUE)
```

The tornado that hit the most houses occurred just east of downtown Orlando.

```{r}
awi.sf2 <- awi.sf |>
  dplyr::filter(estimate > 175)

tmap::tmap_mode("view")
tmap::tm_shape(awi.sf2) +
  tmap::tm_borders()
```

<!--chapter:end:06-Lesson.Rmd-->

# Thursday September 15, 2022 {.unnumbered}

**"Measuring programming progress by lines of code is like measuring aircraft building progress by weight."** -- Bill Gates

Today

-   S4 spatial data objects
-   Working with raster data

## S4 spatial data objects {.unnumbered}

The {sp} package has methods for working with spatial data as S4 reference classes. A few of the packages we will use this semester for analyzing/modeling spatial data work only with {sp} objects so it is helpful to see how they are structured.

Install and load the package.

```{r}
if(!require(sp)) install.packages(pkgs = "sp", repos = "http://cran.us.r-project.org")

library(sp)
```

Spatial objects from the {sp} package fall into two types:

-   spatial-only information (the geometry). Geometries include `SpatialPoints`, `SpatialLines`, `SpatialPolygons`, etc, and
-   extensions to these types where attribute information is available and stored in a data frame. These include `SpatialPointsDataFrame`, `SpatialLinesDataFrame`, etc.

The typical situation is that you have a simple feature data frame (an S3 spatial object) and you need to convert it to an {sp} spatial data frame before the data can be analyzed or modeled.

Consider again the the tornado tracks that you import as a simple feature data frame.

```{r}
FL_Torn.sf <- sf::st_read(dsn = here::here("data", "1950-2020-torn-aspath"), 
                          layer = "1950-2020-torn-aspath") |>
  dplyr::filter(st == "FL")
class(FL_Torn.sf)
```

The object `FL_Torn.sf` is a simple feature data frame (S3 spatial data object). You convert the simple feature data frame to an S4 spatial data object using the `sf::as_Spatial()` function.

```{r}
FL_Torn.sp <- FL_Torn.sf |>
  sf::as_Spatial()

class(FL_Torn.sp)
```

The file `FL_Torn.sp` is a spatial object of class `SpatialLinesDataFrame`.

Information in S4 spatial objects is stored in slots. Slot names are listed with the `slotNames()` function.

```{r}
FL_Torn.sp |>
  slotNames()
```

The `data` slot contains the data frame (attribute table), the `lines` slot contains the spatial geometries (in this case lines), the `bbox` slot is the boundary box and the `proj4string` slot is the CRS.

The object name followed by the `@` symbol allows access to information in the slot. The `@` symbol is similar to the `$` symbol for regular data frames. For example to see the first three rows of the data frame type

```{r}
FL_Torn.sp@data[1:3, ]
```

You recognize this as information about the first three tornadoes in the record. In fact, the object name together with the slot name `data` has class `data.frame`.

```{r}
class(FL_Torn.sp@data)
```

When using the `$` symbol on S4 spatial objects, you access the columns as you would a data frame. For example, to list the EF rating (column labeled `mag`) of the first 3 tornadoes.

```{r}
FL_Torn.sp$mag[1:3]
```

Selecting, retrieving, or replacing attributes in S4 spatial data frames is done with methods in {base} R package. For example `[]` is used to select rows and/or columns. To select `mag` of the 7th tornado type

```{r}
FL_Torn.sp$mag[7]
```

Other methods include: `plot()`, `summary()`,`dim()` and `names()` (operate on the data slot), `as.data.frame()`, `as.matrix()` and `image()` (for spatial data on a grid), and `length()` (number of features).

You can't use the {dplyr} verbs on S4 data frames. To convert from an S4 spatial data frame to a simple feature data frame use `sf::st_as_sf()`.

The first spatial geometry is given as the first element of the lines list.

```{r}
FL_Torn.sp@lines[1]
```

It is an object of class `Lines`. The line is identified by a matrix indicating the longitude and latitude of the start point in row one and the longitude and latitude of the end point in row two.

The `bbox` slot is an object of class `matrix` and `array` and the `proj4string` slot is of class `CRS`.

The interface to the geometry engine-open source (GEOS) is through the {rgeos} package.

## Working with raster data {.unnumbered}

The *raster data model* divides geographic space into a grid of cells of constant size (resolution) and we use classes from the {raster} package to work with raster data.

A raster is a data structure that divides space into rectangles called 'cells' (or 'pixels'). Each cell has an attribute value.

The {terra} package has functions for creating, reading, manipulating, and writing raster data as S3 reference class objects `SpatRaster` and `SpatVect`.

To see what methods (functions) for class `SpatRaster` are available use the `methods()` function.

```{r}
methods(class = "SpatRaster")
```

The list includes {base} R and {sf} methods.

The `terra::rast()` function creates a raster with a geographic (longitude/latitude) CRS and a 1 by 1 degree grid of cells across the globe.

```{r}
r <- terra::rast()
r
```

Arguments including `xmin`, `nrows`, `ncols`, and `crs` are used to change these default settings.

The object has class `SpatRaster` with geographic coordinates spanning the globe at one-degree resolution in the north-south and the east-west directions.

To create a raster with 36 longitudes -100 and 0 degrees East longitude and 18 latitudes between the equator and 50 degrees N latitude we specify the number of columns, the number of rows and the extent as follows.

```{r}
r <- terra::rast(ncols = 36, nrows = 18, 
                 xmin = -100, xmax = 0, 
                 ymin = 0, ymax = 50)
r
terra::res(r)
```

This results in raster with cell resolution of 2.7 degrees of longitude and 2.7 degrees of latitude.

The structure of the raster can be changed after created. Here you change the resolution to 3 degrees. This induces changes to the number of rows and columns.

```{r}
terra::res(r) <- 3
ncol(r)
nrow(r)
```

The `SpatRaster` object `r` is a template with no values assigned to the cells and by default it will have an extent that spans the globe.

```{r}
r <- terra::rast(ncol = 10, nrow = 10)
terra::ncell(r)
terra::hasValues(r)
```

Here there are 100 cells in a 10 by 10 arrangement with no values in any of the cells.

The `terra::values()` function is used to place values in the cells. The function is specified on the left-hand side of the assignment operator. First you assign to a vector of length `terra::ncell(r)` random numbers from a uniform distribution with the `runif()` function. The default is that the random numbers are between 0 and 1.

```{r}
v <- runif(terra::ncell(r))
head(v)
terra::values(r) <- v
head(r)
```

The cells are arranged in lexicographical order (upper left to lower right) and the cells are populated with values from the vector in this order.

The `terra::plot()` function creates a choropleth map of the values in cells.

```{r}
terra::plot(r)
```

The default CRS is geographic.

```{r}
terra::crs(r)
```

To re-project the raster use the function `terra::project()`.

Here you create a new raster with cell numbers as values using the `terra::setValues()` function to place the numbers in the cells.

```{r}
r <- terra::rast(xmin = -110, xmax = -90, 
                   ymin = 40, ymax = 60, 
                   ncols = 10, nrows = 10)
r <- terra::setValues(r, 1:terra::ncell(r))
terra::plot(r)
```

The values increase starting from top left to bottom right as dictated by the sequence `1:terra::ncell(r)` and the lexicographic order in which the raster grids are filled.

The `terra::rast()` function imports data with functions from the {rgdal} package. Supported formats include `GeoTIFF`, `ESRI`, `ENVI`, and `ERDAS`. Most formats that can import a raster can also be used to export a raster.

Consider the `Meuse` dataset (from the {sp} package), using a file in the native 'raster- file' format.

```{r}
f <- system.file("external/test.grd", 
                 package = "raster")
r <- terra::rast(f)
```

Do the cells contain values? Is the raster stored in memory? Create a plot.

```{r}
terra::hasValues(r)
terra::inMemory(r)
terra::plot(r, main = "Raster layer from file")
```

Note the raster is a set of cells arranged in a rectangular array. Values that are coded as `NA` are not plotted.

`SpatRaster` objects can have more than one raster. These are called layers.

```{r}
r
```

The dimensions are nrow = 115 by ncol = 80 and nlyr = 1.

You can add layers to the object. Here you create three rasters and assign random values to the cells.

```{r}
r1 <- terra::rast(nrow = 10, ncol = 10)
terra::values(r1) <- runif(terra::ncell(r1))
r2 <- terra::rast(nrow = 10, ncol = 10)
terra::values(r2) <- runif(terra::ncell(r2))
r3 <- terra::rast(nrow = 10, ncol = 10)
terra::values(r3) <- runif(terra::ncell(r3))
```

You combine the rasters into a single `SpatRaster` object with the concatenate function `c()`.

```{r}
s <- c(r1, r2, r3)
s
dim(s)
terra::nlyr(s)
terra::plot(s)
```

Each raster is a separate layer.

Here you import a set of raster layers from a file.

```{r}
f <- system.file("external/rlogo.grd", 
                 package = "raster")
b <- terra::rast(f)
b
terra::plot(b)
```

Most {base} R functions (`+`, `*`, `round()`, `ceiling()`, `log()`, etc) work on raster objects. Operations are done on all cells at once.

Here you place the numbers from 1 to 100 sequentially in the cells, then add 100 to these values and take the square root.

```{r}
r <- terra::rast(ncol = 10, nrow = 10)
terra::values(r) <- 1:terra::ncell(r)
s <- r + 100
s <- sqrt(s)
terra::plot(s)
```

Here you replace the cell values with random uniform numbers between 0 and 1. Then round to the nearest integer and add one.

```{r}
r <- terra::rast(ncol = 10, nrow = 10)
terra::values(r) <- runif(terra::ncell(r))
r <- round(r)
r <- r + 1
terra::plot(r)
```

Replace only certain values with the subset function `[]`.

```{r}
r <- terra::rast(xmin = -90, xmax = 90, ymin = -30, ymax = 30)
terra::values(r) <- rnorm(terra::ncell(r))
terra::plot(r)
r[r > 2] <- 0
terra::plot(r)
```

Functions for manipulating a raster

The `terra::crop()` function takes a geographic subset of a larger raster object. A raster is cropped by providing an extent object or other spatial object from which an extent can be extracted (objects from classes deriving from raster and from spatial in the {sp} package).

The `terra::trim()` function crops a raster layer by removing the outer rows and columns that only contain `NA` values. The `terra::extend()` function adds new rows and/or columns with `NA` values.

The `terra::merge()` function combines two or more rasters into a single raster. The input objects must have the same resolution and origin (such that their cells fit into a single larger raster). If this is not the case, first adjust one of the objects with the functions `aggregate()` or `resample()`.

The `terra::aggregate()` and `terra::disagg()` functions change the resolution (cell size) of a raster object.

As a simple example showing some of this functionality here you crop the raster into two pieces and then merge the two pieces into one. The `terra::merge()` function has an argument that allows you to export to a file (here `test.grd`).

```{r}
r1 <- terra::crop(r, terra::ext(-180, 0, 0, 30)) 
r2 <- terra::crop(r, terra::ext(-10, 180, -20, 10))
m <- terra::merge(r1, r2, 
                  filename = here::here('outputs', 'test.grd'), 
                  overwrite = TRUE)
terra::plot(m)
```

The `terra::flip()` function flips the data (reverse order) in the horizontal or vertical direction. The `terra::rotate()` function rotates a raster that have longitudes from 0 to 360 degrees (often used by climatologists) to the standard -180 to 180 degrees system.

You extract values from a raster for a set of locations with the `terra::extract()` function. The locations can be a vector object (points, lines, polygons), a matrix with (x, y) or (longitude, latitude -- in that order!) coordinates, or a vector with cell numbers.

```{r}
r <- terra::rast(ncols = 5, nrows = 5, 
                 xmin = 0, xmax = 5, 
                 ymin = 0, ymax = 5)
terra::values(r) <- 1:25

xy <- rbind(c(.5, .5), c(2.5, 2.5))
p <- terra::vect(xy, crs="+proj=longlat +datum=WGS84")

terra::extract(r, xy)
terra::extract(r, p)
```

To convert the values of a raster layer to points or polygons we use `as.points()` and `as.polygons()`. These functions return a `SpatVector` object for cells that are not missing value.

Vector data is converted to a raster with the `terra::rasterize()` function. Polygon to raster conversion is often done to create a mask (i.e. to set to `NA` a set of cells of a raster object, or to summarize values on a raster by zone. For example a country polygon is converted to a raster that is used to set all the cells outside that country to `NA`. Also polygons representing administrative regions such as states can be converted to a raster to summarize values by region. Point to raster conversion is often done to analyze location data (location of a specific species of tree in a forest).

Example: the number of tornadoes passing through each grid cell

Here you want a latitude/longitude grid (1/2 degree latitude by 1/2 degree longitude) with each cell in the grid containing the number of tornadoes that went through it since 2003.

First import the tornado (initial track point) data as a simple feature data frame.

```{r}
if(!"1950-2020-torn-initpoint" %in% list.files(here::here("data"))) {
download.file(url = "http://www.spc.noaa.gov/gis/svrgis/zipped/1950-2020-torn-initpoint.zip",
              destfile = here::here("data", "1950-2020-torn-initpoint.zip"))
unzip(here::here("data", "1950-2020-torn-initpoint.zip"), 
      exdir = here::here("data"))
}
Torn.sf <- sf::st_read(dsn = here::here("data", "1950-2020-torn-initpoint"), 
                       layer = "1950-2020-torn-initpoint") |>
  dplyr::filter(yr >= 2003)
```

Note the extent of the bounding box and check the native CRS.

```{r}
sf::st_crs(Torn.sf)
```

The CRS is geographic.

Next create a raster (called `frame`) with a domain that covers the area of interest and assign a resolution of one degree in longitude and one degree in latitude. Check the extent of the raster with the `terra::ext()` function.

```{r}
frame <- terra::rast(xmin = -106, xmax = -67, 
                     ymin = 24, ymax = 50)
terra::res(frame) <- .5
terra::ext(frame)
```

Next use the `terra::rasterize()` function to count the number of times each raster cell contains a tornado. The first argument is the spatial data frame and the second is the raster without values. The argument `field =` specifies a column name in the spatial data frame (here just an identifier) and the argument `fun =` specifies what to do. Here you want a count of the unique instances of the field in each cell and this is done with setting `fun = "length"`. Raster cells without tornadoes are given a value of 0 based on the `background =` argument.

```{r}
Torn.v <- terra::vect(Torn.sf)

Torn.r <- terra::rasterize(x = Torn.v, 
                           y = frame, 
                           field = "om", 
                           fun = "length",
                           background = 0)
class(Torn.r)
dim(Torn.r)
```

The result is a raster layer. The number of tornadoes occurring in each cell are the values.

We print out the first 200 values (lexicographical order).

```{r}
terra::values(Torn.r)[1:200]
```

To visualize the raster use the `plot()` method.

```{r}
terra::plot(Torn.r)
```

You can recognize the broad shape of the eastern 2/3rds of the United States. Some cells across the Plains and the South have quite a few tornadoes and very few tornadoes in cells over the Appalachian Mountains.

Clustering

Indeed tornado activity appears in distinct clusters (or groups). A statistic that estimates the amount of cluster is called Moran's I. It is a global measure of clustering with high values indicated by high values nearby to other high values and low values nearby to other low values.

Values of Moran's I range from -1 to +1 where positive values indicate clustering and negative values indicate regularity (e.g., chessboard). It is implemented on a raster with the `raster::Moran()` function.

The function works only with S4 raster objects. So you need to first convert `Torn.r` from a `SpatRaster` to a `RasterLayer`. You do this with `raster()` function after loading the {raster} package.

```{r}
library(raster)

Torn.r2 <- raster(Torn.r)
class(Torn.r2)
str(Torn.r2)
```

The object `Torn.r2` is a `RasterLayer` as an S4 data class. Note the use of slots for storing the information.

You can use the `raster::Moran()` function on the `RasterLayer` object.

```{r}
raster::Moran(Torn.r2)
```

The value of .75 indicates high level of tornado clustering at this scale.

Under the null hypothesis of no spatial autocorrelation the expected value for Moran's I is close to zero [-1/(n-1), where n is the number of cells].

Clusters at a local level can be found using a local indicator of spatial autocorrelation. One such indicator is local Moran's I, which is computed at each cell (using the `MoranLocal()` function) so the result is a raster.

```{r}
Torn_lmi.r <- raster::MoranLocal(Torn.r2)
plot(Torn_lmi.r)
```

This type of plot makes is easy to identify the hot spots of tornadoes over parts of the South and the Central Plains.

To convert the local Moran raster to a S4 spatial data frame with polygon geometries use the `rasterToPolygons()` function.

```{r}
Torn_lmi.sp <- raster::rasterToPolygons(Torn_lmi.r)
class(Torn_lmi.sp)
```

Then convert the `SpatialPolygonsDataFrame` to a simple features data frame and make a plot.

```{r}
Torn_lmi.sf <- sf::st_as_sf(Torn_lmi.sp)

library(ggplot2)
ggplot(data = Torn_lmi.sf) +
  geom_sf(mapping = aes(fill = layer, color = layer))
```

Or using functions from the {tmap} package you map the raster layer directly.

```{r}
tmap::tmap_mode("view")
tmap::tm_shape(Torn_lmi.r) +
  tmap::tm_raster(alpha = .7)
```

Focal (neighborhood) functions

The function `terra::focal()` computes statistics in a neighborhood of cells around a focal cell, putting the result in the focal cell of an output raster. The `terra::distance()` function computes the shortest distance to cells that are not NA. The `terra::direction()` function computes the direction towards (or from) the nearest cell that is not `NA`. The `adjacent()` function determines which cells are adjacent to other cells.

Functions from the {raster} package require data objects to be in the S4 reference class. S4 reference classes allow rich data representations at the expense of flexibility. The S3 reference class objects are more flexible, easier to maintain, and allow for new dialects (e.g., {dplyr}, {ggplot2}). Most packages on CRAN use S3 reference class objects.

Consider a multi-band image taken from a Landsat 7 view of a small part of the Brazilian coast. It is included in the {stars} package and stored as a *GeoTIFF* file labeled `L7_ETMs.tif`. You import the image as a raster stack.

```{r}
if(!require(stars)) install.packages("stars", repos = "http://cran.us.r-project.org")
library(stars)

f <- system.file("tif/L7_ETMs.tif",
                  package = "stars")

library(raster)
L7.rs <- stack(f)

class(L7.rs)
```

The data `L7.rs` is a `RasterStack` object as a S4 reference class.

You list the slot names and extract the extent and CRS using the `@` syntax.

```{r}
L7.rs@extent
L7.rs@crs
```

You extract a single band (layer) from the stack with the `layer =` argument in the `raster()` function. You then plot the raster values with the `plot()` method and compute the spatial autocorrelation with the `raster::Moran()` function.

```{r}
L7.rB3 <- raster(L7.rs, layer = 3)
plot(L7.rB3)
raster::Moran(L7.rB3)
```

You convert the raster to an S3 reference class data frame with the `as.data.frame()` method. Here you do that and then compute the normalized difference vegetation index (NDVI) using columns `L7_ETMs.4` and `L7_ETMs.3` and the `mutate()` function from the {dplyr} package.

NDVI indicates live green vegetation from satellite images. Higher values indicate more green vegetation, negative values indicate water.

```{r}
L7.df <- as.data.frame(L7.rs) |>
  dplyr::mutate(NDVI = (L7_ETMs.4 - L7_ETMs.3)/(L7_ETMs.4 + L7_ETMs.3))
```

More examples and other functions for working with raster data using functions from the {terra} package are illustrated in <https://geocompr.robinlovelace.net/raster-vector.html>. I encourage you to take a look.

<!--chapter:end:07-Lesson.Rmd-->

# Tuesday September 20, 2022 {.unnumbered}

**"Maps invest information with meaning by translating it into visual form."** -- Susan Schulten

Today

-   Working with space-time data
-   Making maps

## Working with space-time data {.unnumbered}

Space-time data arrive in the form of multi-dimensional arrays. Examples include:

-   raster images
-   socio-economic or demographic data
-   environmental variables monitored at fixed stations
-   time series of satellite images with multiple spectral bands
-   spatial simulations
-   climate and weather model output

The {stars} package provides functions and methods for working with space-time data as multi-dimensional S3 reference class arrays.

To see what methods (functions) for class `stars` are available use the `methods()` function.

```{r}
methods(class = "stars")
```

The list includes {base} R and {tidyverse} methods.

The typical data array is that where two dimensions represent spatial raster dimensions and the third dimensions is a band (or time). [Data array](https://raw.githubusercontent.com/r-spatial/stars/master/images/cube1.png)

But arrays can have more dimensions. For example, time, space, spectral band, and sensor type. [Data cube](https://raw.githubusercontent.com/r-spatial/stars/master/images/cube2.png)

You import a set of rasters (raster stack) as a {stars} object using the `stars::read_stars()` function. Consider the multi-band image taken from a Landsat 7 view of a small part of the Brazilian coast. It is included in the {stars} package and stored as a *GeoTIFF* file labeled `L7_ETMs.tif`.

```{r}
f <- system.file("tif/L7_ETMs.tif",
                  package = "stars")
L7.stars <- stars::read_stars(f)
L7.stars

dim(L7.stars)
```

There are three dimensions to this {stars} object, two spatial (`x` and `y`), and the third across six bands (`band`). Values across the six bands and space are summarized as a single attribute with name `L7_ETMs.tif`.

The data are stored in a four dimensional array. The first index is the attribute, the second and third indexes are the spatial coordinates, and the fourth index is the band.

Here you plot bands 3 and 4 by sequencing on the fourth index and using the `plot()` method.

```{r}
plot(L7.stars[,,,3:4])
```

Since the data object is S3 you use functions from the `ggplot2()` package together with the `geom_stars()` layer from the {stars} package to plot all 6 bands with a common color scale bar.

```{r}
library(ggplot2)

ggplot() +
  stars::geom_stars(data = L7.stars) +
  facet_wrap(~ band)
```

You create a new {stars} object by applying a function to the band values. For example here you compute normalized difference vegetation index (NDVI) through a function applied across the `x` and `y` spatial dimensions using the `stars::st_apply()` method after creating the function `NDVI()`.

```{r}
NDVI <- function(z) (z[4] - z[3]) / (z[4] + z[3])

( NDVI.stars <- stars::st_apply(L7.stars, 
                                MARGIN = c("x", "y"), 
                                FUN = NDVI) )
ggplot() +
  stars::geom_stars(data = NDVI.stars) 
```

The stars data frame can also be split, here on the band dimension, to yield a representation as six rasters in the list form.

```{r}
( L7split.stars <- split(L7.stars, 
                         f = "band") )
```

Now the bands are given as columns in the data frame part of the {stars} object and there are only two dimensions (`x` and `y`).

Monthly precipitation across the globe

Here you import a NetCDF (Network Common Data Form) file as a space-time raster. NetCDF is a set of formats that support scientific data as arrays. Here the data are monthly global precipitation anomalies on 2.5 by 2.5 degree lat/lon grid. You read the NetCDF file using three array dimensions, two planar space, and the third is time (monthly starting in 1948).

```{r, eval=FALSE}
if(!"precip.mon.anom.nc" %in% list.files(here::here("data"))) {
  download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/precip.mon.anom.nc",
                destfile = here::here("data", "precip.mon.anom.nc"))
}

( w.stars <- stars::read_stars(here::here("data", "precip.mon.anom.nc")) )
```

There are two spatial dimensions and the third dimension is time in months. There is one attribute which is the rain rate in millimeters per day (mm/d).

Here you plot the first month of the global precipitation anomalies.
```{r, eval=FALSE}
plot(w.stars[,,,1])
```

Raster data do not need to be regular or aligned along the cardinal directions. Functions in the {stars} package supports rotated, sheared, rectilinear and curvi-linear grids. [Grids](https://raw.githubusercontent.com/r-spatial/stars/master/man/figures/README-plot2-1.png)

Functions in the {stars} package also support the vector data model. Vector data cubes arise when you have a single dimension that points to distinct spatial feature geometry, such as polygons (e.g. denoting administrative regions). [Vector data cube polygons](https://raw.githubusercontent.com/r-spatial/stars/master/images/cube3.png)

Or points (e.g., denoting sensor locations). [Vector data cube points](https://raw.githubusercontent.com/r-spatial/stars/master/images/cube4.png)

For more see: <https://github.com/r-spatial/stars/tree/master/vignettes> and <https://awesomeopensource.com/project/r-spatial/stars>

Also you can check out some rough code that I've been working on to take advantage of the {stars} functionality including plotting daily temperatures across the U.S. and creating a vector data cube of COVID19 data in the `stars.Rmd` file on course GitHub site in the folder `Other_Rmds`.

## Mapping using functions from the {ggplot2} package {.unnumbered}

The {ggplot2} package has supports `sf` objects for making maps through the function `geom_sf()`. An initial `ggplot()` function is followed by one or more layers that are added with `+` symbol. The layers begin with `geom_`.

For example, consider the objects `nz` and `nz_height` from the {spData} package, where `nz` is a simple feature data frame from the New Zealand census with information about the area, population, and sex ratio (male/female) in the country's 16 administrative regions.

```{r}
str(spData::nz)
```

The simple feature column (`sfc`) is labeled `geom` and the geometry type is multi-polygon.

And `spData::nz_height` is a simple feature data frame containing the elevation of specific high points (peaks) in New Zealand.

```{r}
str(spData::nz_height)
```

The simple feature column is labeled `geometry` and the geometry type is point.

You make a choropleth map of the median income in the New Zealand regions and add a layer indicating the location of the elevation peaks.

```{r}
ggplot() + 
  geom_sf(data = spData::nz, 
          mapping = aes(fill = Median_income)) +
  geom_sf(data = spData::nz_height) +
  scale_x_continuous(breaks = c(170, 175))
```

The first use of `geom_sf()` takes the geometry column of the simple feature data frame `spData::nz` for mapping the spatial aesthetic. The `mapping =` argument specifies other aesthetics with the `aes()` function. Here `fill =` points to the column `Medium_income` in the simple feature data frame. The second use of `geom_sf()` takes the geometry column of `spData::nz_height` and adds the location of the highest peaks as points.

The `geom_sf()` function automatically plots graticules (lines of latitude and longitude) with labels. The default ranges for the graticules can be overridden using `scale_x_continuous()`, `scale_y_continuous()` or `coord_sf(datum = NA)`.

The advantage of using functions from {ggplot2} for mapping include a large community of users and many add-on packages.

Another example: the county land area by state in the U.S. The data is a simple feature data frame available in the {USAboundariesData} package at `ropensci.org` (not on CRAN).

```{r, eval=FALSE}
install.packages("USAboundariesData", 
                 repos = "http://packages.ropensci.org", 
                 type = "source")
```

Here you extract the county borders in Florida then make a choropleth of the land area.

```{r}
FLcounties.sf <- USAboundaries::us_counties(states = "FL")

ggplot() +
  geom_sf(data = FLcounties.sf,
          mapping = aes(fill = aland))
```

## Mapping using functions from the {tmap} package {.unnumbered}

There are several other packages for making quick, nice maps listed in the syllabus.

I particularly like the {tmap} package because it is agnostic to the type of spatial data object. Simple feature data frames as well as {sp} and {raster} objects can be combined on a single map. This is not the case with the {ggplot2} functions.

```{r}
if(!require(tmap)) install.packages(pkgs = "tmap", repos = "http://cran.us.r-project.org")
```

Functions in the {tmap} use the 'grammar of graphics' philosophy that separates the data frame from the aesthetics (how data are made visible). Functions translate the data into aesthetics. The aesthetics can include the location on a geographic map (defined by the geometry), color, and other visual components.

A {tmap} map starts with the `tm_shape()` function that takes as input a spatial data frame. The function is followed by one or more layers such as `tm_fill()`, `tm_dots()`, `tm_raster()`, etc that defines how a property in the data gets translated to a visual component.

Returning to the New Zealand simple feature data frame (`nz`). To make a map of the region borders you first identify the spatial data frame with the `tm_shape()` function and then add a borders layer with the `tm_borders()` layer.

```{r}
tmap::tm_shape(shp = spData::nz) +
  tmap::tm_borders() 
```

The function `tmap::tm_shape()` and its subsequent drawing layers (here `tmap::tm_borders()`) as a 'group'. The data in the `tmap::tm_shape()` function must be a spatial object of class simple feature, raster, or an S4 class spatial object.

Here you use a fill layer (`tmap::tm_fill()`) instead of the borders layer.

```{r}
tmap::tm_shape(spData::nz) +
  tmap::tm_fill() 
```

The multi-polygons are filled using the same gray color as the borders so they disappear.

In this next example you layer using the fill aesthetic and then add a border aesthetic.

```{r}
tmap::tm_shape(spData::nz) +
  tmap::tm_fill(col = 'green') +
  tmap::tm_borders() 
```

Layers are added with the `+` operator and are functionally equivalent to adding a GIS layer.

You can assign the resulting map to an object. For example here you assign the map of New Zealand to the object `map_nz`.

```{r}
map_nz <- tmap::tm_shape(spData::nz) + 
  tmap::tm_polygons()

class(map_nz)
```

The resulting object is of class `tmap`.

New spatial data are added with `+ tm_shape(new_object)`. In this case `new_object` represents a new spatial data frame to be plotted over the preceding layers. When a new spatial data frame is added in this way, all subsequent aesthetic functions refer to it, until another spatial data frame is added.

For example, let's add an elevation layer to the New Zealand map. The elevation raster (`nz_elev`) spatial data frame is in the {spDataLarge} package on GitHub.

The `install_github()` function from the {devtools} package is used to install packages on GitHub. GitHub is a company that provides hosting for software development version control using Git. Git is a version-control system for tracking changes in code during software development.

```{r}
if(!require(devtools)) install.packages(pkgs = "devtools", repos = "http://cran.us.r-project.org")
library(devtools)

if(!require(spDataLarge)) install_github(repo = "Nowosad/spDataLarge")
library(spDataLarge)
```

Next identify the spatial data for the the new layer by adding `tm_shape(nz_elev)`. Then add the raster layer with the `tm_raster()` function and set the transparency level to 70% (`alpha = .7`).

```{r}
( map_nz1 <- map_nz +
  tmap::tm_shape(spDataLarge::nz_elev) + 
    tmap::tm_raster(alpha = .7) )
```

The new map object `map_nz1` builds on top of the existing map object `map_nz` by adding the raster layer `spDataLarge::nz_elev` representing elevation.

You can create new layers with functions. For instance, a function like `sf::st_union()` operates on the `geometry` column of a simple feature data frame.

As an example, here you create a line string layer as a simple feature object using three geo-computation functions. You start by creating a union over all polygons (regions) with the `sf::st_union()` function applied to the `spData::nz` simple feature object. The result is a multi-polygon defining the coastlines.

Then you buffer this multi-polgyon out to a distance of 22.2 km using the `sf::st_buffer()` function. The result is a single polygon defining the coastal boundary around the entire country.

Finally you change the polygon geometry to a line string geometry with the `sf::st_cast()` function.

The operations are linked together with the pipe operator.

```{r}
( nz_water.sfc <- spData::nz |>
  sf::st_union() |> 
  sf::st_buffer(dist = 22200) |> 
  sf::st_cast(to = "LINESTRING") )
```

Now add the resulting `sfc` as a layer to our map.

```{r}
( map_nz2 <- map_nz1 +
  tmap::tm_shape(nz_water.sfc) + 
    tmap::tm_lines() )
```

Finally, create a layer representing the country elevation high points (stored in the object `spData::nz_height`) onto the `map_nz2` object with `tmap::tm_dots()` function.

```{r}
( map_nz3 <- map_nz2 +
  tmap::tm_shape(spData::nz_height) + 
    tmap::tm_dots() )
```

Map layout, facets, and inserts

Layout functions help create a cartographic map. Elements include the title, the scale bar, margins, aspect ratios, etc. For example, here elements such as a north arrow and a scale bar are added with `tm_compass()` and `tm_scale_bar()`, respectively and the `tm_layout()` function is used to add the title and background color.

```{r}
map_nz + 
  tm_compass(type = "8star", 
             position = c("left", "top")) +
  tm_scale_bar(breaks = c(0, 100, 200), 
               text.size = 1) +
  tm_layout(title = "New Zealand",
            bg.color = "lightblue")
```

Putting two or more maps with the same scale side by side allows for easy comparisons and to see how spatial relationships change with respect to another variable. Creating small multiples of the same map with different variables is called 'faceting'. 

Consider the simple feature data frame `World` from the {tmap} package. Make the data frame accessible to this session with the `data()` function.

```{r}
library(tmap)
data(World)
head(World)
```

The simple feature data frame has socio-economic indicators by country. Each row is a country.

Further, consider the simple feature data frame `urban_agglomerations` from the {spData} package. The data frame is from the United Nations population division with projections up to 2050 for the top 30 largest areas by population at 5 year intervals (in long form).

The geometries are points indicating the location of the largest urban metro areas.

You create a new data frame keeping only the years 1970, 1990, 2010, and 2030 by using the `filter()` function from the {dplyr} package.

```{r}
urb_1970_2030 <- spData::urban_agglomerations |> 
  dplyr::filter(year %in% c(1970, 1990, 2010, 2030))
```

Note that the operator `%in%` acts like a recursive `or`. If year == 1970 or year == 1990, ... For example,

```{r}
1969:2031 

1969:2031 %in% c(1970, 1990, 2010, 2030)
```

Returns a series of TRUEs and FALSEs.

The first map layer is the country polygons from the `World` data frame and the second layer is city locations from the `urb_1970_2030` data frame using the `tmap::tm_symbols()` function. The symbol size is scaled by the variable `population_millions`. Finally you group by the variable `year` with the `tmap::tm_facets()` function to produce a four-panel set of maps.

```{r}
tmap::tm_shape(World) + 
  tmap::tm_polygons() + 
tmap::tm_shape(urb_1970_2030) + 
  tmap::tm_symbols(col = "black", 
                   border.col = "white",
                   size = "population_millions") +
  tmap::tm_facets(by = "year", 
                  nrow = 2, 
                  free.coords = FALSE)
```

The above code chunk demonstrates key features of faceted maps created with functions from the {tmap} package.

-   Shapes that do not have a facet variable are repeated (the countries in `World` in this case).
-   The `by =` argument which varies depending on a variable (`year` in this case).
-   nrow/ncol setting specifying the number of rows (and columns) that facets should be arranged into.
-   The `free.coords =` argument specifies whether each map has its own bounding box.

Small multiples are also generated by assigning more than one value to one of the aesthetic arguments.

For example here you map the happiness index (`HPI`) on one map and gross domestic product per capita (`gdp_cap_est`) on another map. Both variables are in the `World` data frame.

```{r}
tmap::tm_shape(World) +
    tmap::tm_polygons(c("HPI", "gdp_cap_est"), 
                      style = c("pretty", "kmeans"),
                      palette = list("RdYlGn", "Purples"),
                      title = c("Happy Planet Index", "GDP per capita")) 
```

Note that the variable names must be in quotes (e.g., "HPI").

The maps are identical except for the variable being plotted. All arguments of the layer functions can be vectorized, one for each map. Arguments that normally take a vector, such as `palette =`, are placed in a `list()`.

Multiple map objects can also be arranged in a single plot with the `tmap::tmap_arrange()` function. Here you create two separate maps then arrange them.

```{r}
map1 <- tmap::tm_shape(World) +
           tmap::tm_polygons("HPI", 
                             style = "pretty",
                             palette = "RdYlGn",
                             title = "Happy Planet Index") 

map2 <- tmap::tm_shape(World) +
           tmap::tm_polygons("gdp_cap_est", 
                             style = "kmeans",
                             palette = "Purples",
                             title = "GDP per capita") 

tmap_arrange(map1, map2)
```

Example: COVID19 vaccinations by state on Saturday February 6, 2021. Get the data.

```{r}
f <- "https://raw.githubusercontent.com/owid/covid-19-data/e2da3a49250481a8a22f993ee5c3731111ba6958/scripts/scripts/vaccinations/us_states/input/cdc_data_2021-02-06.csv"

df <- readr::read_csv(f)
```

Get a US census mapfrom the {USAboundaries} package. Rename the state name column (`name`) to `LongName`.

```{r}
sf <- USAboundaries::us_states() |>
  dplyr::filter(!name %in% c("District of Columbia", "Puerto Rico", "Hawaii", "Alaska")) |>
  dplyr::rename(LongName = name)
```

Join the COVID data frame with the simple feature data frame from the census. Then make a map showing the doses administered per 100K people.

```{r}
sf <- sf |>
  dplyr::left_join(df, 
                   by = "LongName")

tmap::tm_shape(sf) +
  tmap::tm_fill(col = "Admin_Per_100K", title = "Per 100K" ) +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_layout(legend.outside = TRUE)
```

## Creating an interactive map {.unnumbered}

A nice feature of the {tmap} package is that you can create an interactive map using the same code used to create a static map.

For example, with the mode set to `"view"` in the `tmap::tmap_mode()` function the county boundary map created from the `FLcounties.sf` simple feature data frame using the {tmap} functions is interactive.

```{r}
tmap::tmap_mode("view")

tmap::tm_shape(FLcounties.sf) +
  tmap::tm_borders()
```

Click on the layer symbol and change to OpenStreetMap.

With the interactive mode turned on, all maps produced with {tmap} launch as zoom-able HTML. This feature includes the ability to specify the base map with `tm_basemap()` (or `tmap_options()`) as demonstrated here.

```{r, eval=FALSE}
map_nz + 
  tmap::tm_basemap(server = "OpenTopoMap")
```

You can also create interactive maps with the `tmap_leaflet()` function.

The view mode in {tmap} works with faceted plots. The argument sync in `tm_facets()` is used to produce multiple maps with synchronized zoom and pan settings.

```{r}
world_coffee <- dplyr::left_join(spData::world, 
                                 spData::coffee_data, 
                                 by = "name_long")
tmap::tm_shape(world_coffee) + 
  tmap::tm_polygons(c("coffee_production_2016", 
                "coffee_production_2017")) + 
  tmap::tm_facets(nrow = 1, sync = TRUE)
```

Change the view mode back to plot.

```{r}
tmap_mode("plot")
```

## Adding an inset map {.unnumbered}

An inset map puts the geographic study area into context. Here you create a map of the central part of New Zealand's Southern Alps. The inset map shows where the main map is in relation to the rest of New Zealand.

The first step is to define the area of interest. Here it is done here by creating a new spatial object `nz_region` using the `sf::st_bbox()` function and the `sf::st_as_sfc()` to make it a simple feature column.

```{r}
nz_region <- sf::st_bbox(c(xmin = 1340000, xmax = 1450000,
                         ymin = 5130000, ymax = 5210000),
                         crs = sf::st_crs(spData::nz_height)) |> 
  sf::st_as_sfc()
```

Next create a base map showing New Zealand's Southern Alps area. This is the closeup view of where the most important message is stated. The region is clipped to the simple feature column `nz_region` created above. The layers include a raster of elevations and locations of high points. A scale bar is included.

```{r}
( nz_height_map <- tmap::tm_shape(nz_elev, 
                                  bbox = nz_region) +
  tmap::tm_raster(style = "cont", 
                  palette = "YlGn", 
                  legend.show = TRUE) +
  tmap::tm_shape(spData::nz_height) + 
  tmap::tm_symbols(shape = 2, 
                   col = "red", 
                   size = 1) +
  tmap::tm_scale_bar(position = c("left", "bottom")) )
```

Next create the inset map. It gives a context and helps to locate the area of interest. This map clearly indicates the location of the main map.

```{r}
( nz_map <- tmap::tm_shape(spData::nz) + 
  tmap::tm_polygons() +
  tmap::tm_shape(spData::nz_height) + 
  tmap::tm_symbols(shape = 2, 
                   col = "red", 
                   size = .1) + 
  tmap::tm_shape(nz_region) + 
  tmap::tm_borders(lwd = 3) )
```

Finally combine the two maps. The `viewport()` function from the {grid} package is used to give a center location (x and y) and the size (width and height) of the inset map.

```{r}
library(grid)

nz_height_map
print(nz_map, 
      vp = viewport(.8, .27, width = .5, height = .5))
```

- Additional details and examples on making maps in R are available in the book "Geocomputation with R" by Lovelace, Nowosad, and Muenchow <https://geocompr.robinlovelace.net/adv-map.html>

- Mapping walking (etc) distances. <https://walker-data.com/mapboxapi/>

<!--chapter:end:08-Lesson.Rmd-->

# Tuesday September 27, 2022 {.unnumbered}

**"You haven't mastered a tool until you understand when it should not be used."** -- Kelsey Hightower

Today

-   Defining spatial neighborhoods and spatial weights
-   Computing spatial autocorrelation
-   Spatial lag and its relation to autocorrelation

## Defining spatial neighborhoods and spatial weights {.unnumbered}

Autocorrelation plays a central role in spatial statistics. It measures the degree to which things tend to cluster. Things include attribute values aggregated to polygons (or raster cells) as well as locations. How autocorrelation gets estimated depends on the geometry of the spatial data.

Things tend to cluster because of:

-   Association: whatever causes an attribute to have a certain value in one area causes the same attribute to have a similar value in areas nearby. Crime rates in nearby neighborhoods might tend to cluster due to similar factors.

-   Causality: something within a given area directly influences outcomes within nearby areas. Non-infectious diseases (e.g., lung cancer) have similar rates in neighborhoods close to an oil refinery.

-   Interaction: the movement of people, goods or information creates relationships between areas. COVID spreads through areas through the movement of people.

Spatial statistics quantify, and condition on, autocorrelation but they are silent about physical causes. Understanding the reason for autocorrelation in your data is important for inference because the causal mechanism might be confounded by its. The divorce rate is high in southern states, but so is the number of Waffle Houses. Understanding causation requires domain specific knowledge.

When a variable's values are aggregated (summed or averaged) to regions, autocorrelation is quantified by calculating how similar a value in region $i$ is to the value in region $j$ and weighting this similarity by how 'close' region $i$ is to region $j$. Closer regions are given greater weight.

High similarities with high weight (similar values close together) yield high values of spatial autocorrelation. Low similarities with high weight (dissimilar values close together) yield low values of spatial autocorrelation. Let $\hbox{sim}_{ij}$ denote the similarity between values $Y_i$ and $Y_j$, and let $w_{ij}$ denote a set of weights describing the 'distance' between regions $i$ and $j$, for $i$, $j$ = 1, ..., $N$.

A general spatial autocorrelation index (SAI) is given by $$
\hbox{SAI} = \frac{\sum_{i,j=1}^N w_{ij}\hbox{sim}_{ij}}{\sum_{i,j=1}^N w_{ij}}
$$ which represents the weighted similarity between regions. The set of weights ($w_{ij}$) is called a spatial weights matrix. The spatial weights matrix defines the neighbors for each region and defines the strength of each association.

For cells in a raster under the rook-contiguity criterion, $w_{ij}$ = 1 if cell $i$ and $j$ share a boundary, and 0 if they don't share a boundary. In this case $w_{ij}$ = $w_{ji}$. Also, a cell is not a neighbor of itself so $w_{ii}$ = 0.

Alternatively you can define center locations from a set of polygon regions and let $w_{ij}$ = 1 if the center of region $i$ is near the center of region $j$ and 0 otherwise. Here you need to decide on the number of nearest neighbors.

You can also define neighbors by distance. For example, if $d_{ij}$ is the distance between centers $i$ and $j$, you can let $w_{ij}$ = 1 if $d_{ij}$ \< $\delta$ and 0 otherwise.

Consider crime data at the tract level in the city of Columbus, Ohio. The tract polygons are projected with arbitrary spatial coordinates.

```{r}
if(!"columbus" %in% list.files("data")) {
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/columbus.zip",
              destfile = here::here("data", "columbus.zip"))
unzip(here::here("data", "columbus.zip"),
      exdir = here::here("data"))
}

( CC.sf <- sf::st_read(dsn = here::here("data", "columbus"),
                       layer = "columbus") )
```

The simple feature data frame contains housing values (`HOVAL`), income values (`INC`) and (`CRIME`) in census tracts across the city. Crime (`CRIME`) is residential burglaries and vehicle thefts per 1000 households. Income (`INC`) and housing values (`HOVAL`) are annual values with units of 1000 dollars.

Create a choropleth map of the crime rates (`CRIME`).

```{r}
tmap::tm_shape(CC.sf) +
  tmap::tm_fill(col = "CRIME",
                title = "Burglary & Vehicle Thefts\n/1000 Households")
```

Note that the variable name CRIME must be in quotes.

Alternatively we create a choropleth map of the crime rates using `geom_sf()`. Here the variable name CRIME is without quotes.

```{r}
library(ggplot2)

ggplot(data = CC.sf) + 
  geom_sf(mapping = aes(fill = CRIME)) +
  labs(fill = "Burglary & Vehicle Thefts\n/1000 Households") +
  theme_void()
```

High crime areas tend to be clustered.

Autocorrelation quantifies the amount of clustering. To compute the autocorrelation you first need to define the neighbors for each polygon.

You create a list of neighbors using the spdep::`poly2nb()` function from the {spdep} package. The 'nb' in the function names stands for neighbor list object. The function builds the list from geometries based on contiguity. Neighbors must share at least one geographic location. By default the contiguity is defined as having at least one location in common. This is changed by using the argument `queen = FALSE`. Functions in the {spdep} package support S3 and S4 spatial data objects.

```{r}
if(!require(spdep)) install.packages("spdep", repos = "http://cran.us.r-project.org")

( nbs <- spdep::poly2nb(CC.sf) )
```

Note that this only works for spatial data frames.

The output tells you there are 49 tracts (polygons). Each tract is bordered by at least one other tract. The average number of neighbors is 4.8. The total number of neighbors over all tracts is 236. This represents 9.8% of all possible connections (if every tract is a neighbor of itself and every other tract 49 \* 49).

A graph of the neighbor links is obtained with the `plot()` method. The arguments include the neighbor list object (`nbs`) and the location of the polygon centers, which are extracted from the simple feature data frame using the sf::`st_centroid()`.

```{r}
plot(CC.sf$geometry)
plot(nbs, 
     sf::st_centroid(CC.sf$geometry),
     add = TRUE)
```

The graph is a network showing the contiguity pattern (adjacency neighbor structure). Tracts close to the center of the city have more neighboring tracts and thus more links in the network.

The number of links per tract (node)--link distribution--is obtained with the `summary()` method.

```{r}
summary(nbs)
```

The list of neighboring tracts for the first two tracts.

```{r}
nbs[[1]]
nbs[[2]]
```

The first tract has two neighbors that include tracts 2 and 3. The neighbor numbers are stored as an integer vector within the `nb` object. Tract 2 has three neighbors that include tracts 1, 3, and 4. Tract 5 has 8 neighbors and so on. The function spdep::`card()` tallies the number of neighbors by tract.

```{r}
spdep::card(nbs)
```

Tract 5 has 8 neighbors and so on.

The next step is to include weights to the neighbor list object indicate how close each neighbor is. The function spdep::`nb2listw()` turns the neighbor list object into a spatial weights object. By default the weighting scheme gives each link the same weight equal to the multiplicative inverse of the number of neighbors.

```{r}
wts <- nbs |>
  spdep::nb2listw()

class(wts)
```

This `wts` object is a list with two elements. The first element (`listw`) is the weights matrix and the second element (`nb`) is the neighbor list object.

```{r}
summary(wts)
```

The network statistics are given along with information about the weights. The default weighting scheme assigns a weight to each neighbor equal to the inverse of the number of neighbors (`style = "W"`). For a tract with 5 neighbors each neighbor gets a weight of 1/5. The sum over all weights (`S0`) is the number of tracts.

To see the weights for the first two tracts type

```{r}
wts$weights[1:2]
```

The object `weights` represents the weights matrix as a list. The full matrix has dimensions 49 x 49 but most of the entries are zero.

```{r}
nbs |>
  spdep::nb2mat() |>
  head()
```

To see the neighbors of the first two tracts type

```{r}
wts$neighbours[1:2]
```

Tract 1 has two neighbors (tract 2 & 3) so each are given a weight of 1/2. Tract 2 has three neighbors (tract 1, 3, & 4) so each are given a weight of 1/3.

With the weights matrix saved as an object you are ready to compute a metric of spatial autocorrelation.

Caution: Neighbors defined by contiguity can leave some areas without any. Islands for example. By default the `spdep::nb2listw()` function assumes each area has at least one neighbor. If this is not the case you need to specify how areas without neighbors are handled using the argument `zero.policy = TRUE`. This permits the weights list to be formed with zero-length weights vectors.

For example, consider the districts in the country of Scotland.

```{r}
if(!"scotlip" %in% list.files(here::here("data"))) {
download.file("http://myweb.fsu.edu/jelsner/temp/data/scotlip.zip",
              destfile = here::here("data", "scotlip.zip"))
unzip(here::here("data", "scotlip.zip"),
      exdir = here::here("data"))
}
SL.sf <- sf::st_read(dsn = here::here("data", "scotlip"), 
                     layer = "scotlip")
plot(SL.sf$geometry)
```

Three of the districts are islands. These districts have no bordering districts.

Create a list of neighbors.

```{r}
( nbs2 <- SL.sf |>
    spdep::poly2nb() )
```

Three regions with no links.

Use the `spdep::nb2listw()` function with the argument `zero.policy = TRUE`. Otherwise we get an error saying the empty neighbor sets are found.

```{r}
wts2 <- nbs2 |>
  spdep::nb2listw(zero.policy = TRUE)
head(wts2$weights)
```

## Computing autocorrelation {.unnumbered}

A common autocorrelation statistic is Moran's I. Moran's I follows the basic form of autocorrelation indexes where the similarity between regions $i$ and $j$ is proportional to the product of the deviations from the mean $$
\hbox{sim}_{ij} \propto (Y_i - \bar Y) (Y_j - \bar Y)
$$ where $i$ indexes the region and $j$ indexes the neighbors of $i$. The value of $\hbox{sim}_{ij}$ is large when the $Y$ values in the product are on the same side of their respective means (both above or below) and small when they are on opposite sides of their respective means (one above and one below or vice versa).

The formula for I is $$
\hbox{I} = \frac{N} {W} \frac {\sum_{i,j} w_{ij}(Y_i-\bar Y) (Y_j-\bar Y)} {\sum_{i} (Y_i-\bar Y)^2}
$$ where $N$ is the number regions, $w_{ij}$ is the matrix of weights, and $W$ is the sum over all weights.

Consider the following grid of cells containing attribute values.

```{r}
if(!require(spatstat)) install.packages(pkgs = "spatstat", repos = "http://cran.us.r-project.org")
suppressMessages(library(spatstat))

set.seed(6750)
Y <- ppp(runif(200, 0, 1), 
         runif(200, 0, 1))
plot(quadratcount(Y), main = "")
```

The formula for results in one value of I representing the magnitude of the autocorrelation (amount of clustering) over the entire area.

First consider a single cell in the area ($N$ = 1). Start with the middle cell (row 3, column 3). Let $i$ = 3 in the above formula and let $j$ index the cells touching the center cell in reading order starting with cell (2, 2), then cell (2, 3), etc.

Assume each neighbor is given a weight of 1/8 so $W = \sum_{j=1}^8 w_j = 1$. Then the value of I for the single center cell is I\_{3, 3} = (6 - mean(y)) \* ((8 - mean(y)) + (3 - mean(y)) + (9 - mean(y)) + (12 - mean(y)) + (10 - mean(y)) + (10 - mean(y)) + (9 - mean(y))) / (6 - mean(y))\^2)

```{r}
y <- c(3, 10, 7, 12, 5, 11, 8, 3, 9, 12, 
      6, 12, 6, 10, 3, 8, 10, 10, 9, 7, 
      5, 10, 8, 5, 11)
( yb <- mean(y) )
```

```{r}
Inum_i <- (6 - yb) * 
                 ((8 - yb) + (3 - yb) + (9 - yb) + 
                  (12 - yb) + (10 - yb) + (10 - yb) + 
                  (10 - yb) + (9 - yb))
Iden_i <- (6 - yb)^2
Inum_i/Iden_i
```

The I value of -3.5 indicates that the center cell, which has a value below the average over all 25 cells, is mostly surrounded by cells having values above the average.

Repeat this calculation for every cell and then take the sum.

This is what the function `spdep::moran()` from the {spdep} package does. The first argument is the vector containing the values for which you are interested in determining the magnitude of the spatial autocorrelation and the second argument is the `listw` object.

Further, you need to specify the number of regions and the sum of the weights `S0`. The latter is obtained from the `spdep::Szero()` function applied to the `listw` object.

Returning to the Columbus crime data here let `m` be the number of census tracts and `s` be the sum of the weights. You then apply the `spdep::moran()` function on the variable `CRIME`.

```{r}
m <- length(CC.sf$CRIME)
s <- spdep::Szero(wts)

spdep::moran(CC.sf$CRIME, 
             listw = wts, 
             n = m, 
             S0 = s)
```

The function returns the Moran's I statistic and the kurtosis (K) of the distribution of crime values. Moran's I ranges from -1 to +1.

The value of .5 for the crime rates indicates a high level of spatial autocorrelation. This is expected based on the clustering of crime in the central city.

Positive values of Moran's I indicate clustering and negative values indicate inhibition. Inhibition is a process leading to nearby values having attribute values that are opposite in magnitude as those at each location (like a checkerboard pattern)

Kurtosis is a statistic that indicates how peaked the distribution of the attribute values is. A normal distribution has a kurtosis of 3. If the kurtosis is too large (or small) relative to a normal distribution then any statistical inference we make with Moran's I will be suspect.

[Wikipedia Moran's I](https://en.wikipedia.org/wiki/Moran%27s_I)

Another statistic that indicates the amount of spatial autocorrelation is Geary's C. The equation is 
$$
\hbox{C} = \frac{(N-1) \sum_{i,j} w_{ij} (Y_i-Y_j)^2}{2 W \sum_{i}(Y_i-\bar Y)^2} 
$$ 
where $W$ is the sum over all weights ($w_{ij}$) and $N$ is the number of areas.

The syntax of the `spdep::geary()` function is similar that of `spdep::moran()` except you also specify `n1` to be one minus the number of areas.

```{r}
spdep::geary(CC.sf$CRIME, 
             listw = wts,
             n = m, 
             S0 = s, 
             n1 = m - 1)
```

Values for Geary's C range from 0 to 2 with 1 indicating no autocorrelation. Values less than 1 indicate positive autocorrelation. Both I and C are global measures of autocorrelation, but C is more sensitive to local variations in autocorrelation.

Rule of thumb: If the interpretation of Geary's C is much different than the interpretation of Moran's I then consider computing local measures of autocorrelation.

[Wikipedia Geary's C](https://en.wikipedia.org/wiki/Geary%27s_C)

## Spatial lag and its relation to autocorrelation {.unnumbered}

The interpretation of Moran's I is simplified by the fact that the value of Moran's I is the slope coefficient from a regression of the weighted average of the neighborhood values onto the observed values.

The weighted average of neighborhood values is called the spatial lag.

Let `crime` be the set of crime values in each region as a data vector. You create a spatial lag variable using the `spdep::lag.listw()` function. The first argument is the `listw` object and the second is the vector of crime values.

```{r}
crime <- CC.sf$CRIME
Wcrime <- spdep::lag.listw(wts, 
                           crime)
```

For each value in the vector `crime` there is a corresponding value in the vector `Wcrime` representing the average crime over the neighboring regions.

Recall tract 1 had tract 2 and 3 as its only neighbors. So the following should return a `TRUE`.

```{r}
Wcrime[1] == (crime[2] + crime[3])/2
```

A scatter plot of the neighborhood average crime rates versus the individual polygon crime rates in each shows there is a relationship.

```{r}
data.frame(crime, Wcrime) |>
ggplot(mapping = aes(x = crime, y = Wcrime)) + 
  geom_point() + 
  geom_smooth(method = lm) +
  scale_x_continuous(limits = c(0, 70)) +
  scale_y_continuous(limits = c(0, 70)) +
  xlab("Crime") + 
  ylab("Average Crime in the Neighborhood") +
  theme_minimal()
```

The vertical axis contains the neighborhood average crime rate. The range of neighborhood averages is smaller than the range of individual polygon crime rates.

Tracts with low values of crime tend to be surrounded by tracts with low values of crime on average and tracts with high values of crime tend be surrounded by tracts with high values of crime. The slope is upward (positive).

The magnitude of the slope is the Moran's I value. To check this use the `lm()` function from the base set of packages. The function is used to fit linear regression models.

```{r}
lm(Wcrime ~ crime)
```

The coefficient on the `crime` variable in the linear regression is .5.

The scatter plot is called a 'Moran's scatter plot.'

Let's consider another data set.

```{r}
if(!"sids2" %in% list.files(here::here("data"))) {
download.file("http://myweb.fsu.edu/jelsner/temp/data/sids2.zip",
              destfile = here::here("data", "sids2.zip"))
unzip(here::here("data", "sids2.zip"),
      exdir = here::here("data"))
}

SIDS.sf <- sf::st_read(dsn = here::here("data", "sids2")) |>
  sf::st_set_crs(4326)
head(SIDS.sf)
```

The column `SIDR79` contains the death rate (per 1000 live births) (1979-84) from sudden infant death syndrome. Create a choropleth map of the SIDS rates.

```{r}
tmap::tm_shape(SIDS.sf) +
  tmap::tm_fill("SIDR79", title = "") +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_layout(title = "SIDS Rates 1979-84 [per 1000]",
                  legend.outside = TRUE)
```

Create a neighborhood list (`nb`) and a `listw` object (`wts`) then graph the neighborhood network.

```{r}
nbs <- spdep::poly2nb(SIDS.sf)
wts <- spdep::nb2listw(nbs)

plot(nbs, 
     sf::st_centroid(st_geometry(SIDS.sf)))
```

Next compute Moran's I on the SIDS rates over the period 1974--1979.

```{r}
m <- length(SIDS.sf$SIDR79)
s <- spdep::Szero(wts)

spdep::moran(SIDS.sf$SIDR79, 
             listw = wts, 
             n = m, 
             S0 = s)
```

I is .14 and K is 4.4. A normal distribution has a kurtosis of 3. Values less than about 2 or greater than about 4 indicate that inferences about autocorrelation based on the assumption of normality are suspect.

Weights are specified using the `style =` argument in the `nb2listw()` function. The default "W" is row standardized (sum of the weights over all links equals the number of polygons). "B" is binary (each neighbor gets a weight of one). "S" is a variance stabilizing scheme.

Each style gives a somewhat different value for I.

```{r}
x <- SIDS.sf$SIDR79
spdep::moran.test(x, spdep::nb2listw(nbs, style = "W"))$estimate[1]
spdep::moran.test(x, spdep::nb2listw(nbs, style = "B"))$estimate[1]  # binary
spdep::moran.test(x, spdep::nb2listw(nbs, style = "S"))$estimate[1]  # variance-stabilizing
```

When reporting a Moran's I you need to state what type of weighting was used.

Let `sids` be a vector with elements containing the SIDS rate in each county. You create a spatial lag variable using the `spdep::lag.listw()` function. The first argument is the `listw` object and the second is the vector of rates.

```{r}
sids <- SIDS.sf$SIDR79
Wsids <- spdep::lag.listw(wts, 
                          sids)
```

For each value in the vector `sids` there is a corresponding value in the object `Wsids` representing the neighborhood average SIDS rate.

```{r}
Wsids[1]
j <- wts$neighbours[[1]]
j
sum(SIDS.sf$SIDR79[j])/length(j)
```

The weight for county one is `Wsids[1]` = 2.659. The neighbor indexes for this county are in the vector `wts$neighbours[[1]]` of length 3. Add the SIDS rates from those counties and divide by the number of counties (`length(j)`).

A scatter plot of the neighborhood average SIDS rate versus the actual SIDS rate in each region.

```{r}
data.frame(sids, Wsids) |>
ggplot(aes(x = sids, y = Wsids)) + 
  geom_point() + 
  geom_smooth(method = lm) +
  scale_x_continuous(limits = c(0, 7)) +
  scale_y_continuous(limits = c(0, 7)) +
  xlab("SIDS") + ylab("Spatial Lag of SIDS") +
  theme_minimal()
```

The regression line slopes upward indicating positive spatial autocorrelation. The value of the slope is I. To check this type

```{r}
lm(Wsids ~ sids)
```

<!--chapter:end:09-Lesson.Rmd-->

# Thursday September 29, 2022 {.unnumbered}

**"Be curious. Read widely. Try new things. I think a lot of what people call intelligence boils down to curiosity."** - Aaron Swartz

Today

- Other neighbor definitions
- Assessing the statistical significance of autocorrelation
- Bivariate spatial autocorrelation
- Local indicators of spatial autocorrelation

## Other neighbor definitions {-}

Last time you saw how to compute autocorrelation using areal aggregated data. The procedure involves a weights matrix, which you created using the default neighborhood definition and the weighting scheme with functions from the {spdep} package. 

It was noted that the magnitude of autocorrelation depends on the weighting scheme used. Other neighborhood definitions are possible and they will also influence the magnitude of the autocorrelation.

Let's consider the historical demographic data in Mississippi counties. Import the data as a simple feature data frame and assign the geometry a geographic CRS.
```{r}
if(!"police" %in% list.files(here::here("data"))) {
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/police.zip",
              destfile = here::here("data", "police.zip"))
unzip(here::here("data", "police.zip"),
      exdir = here::here("data"))
}

( PE.sf <- sf::st_read(dsn = here::here("data", "police"), 
                 layer = "police") |>
  sf::st_set_crs(4326) )
```

Variables in the simple feature data frame include police expenditures (`POLICE`), crime (`CRIME`), income (`INC`), unemployment (`UNEMP`) and other socio-economic characteristics across Mississippi at the county level. Police expenditures are per person 1982 (dollars per person). Personal income is per person in 1982 (dollars per person). Crime is the number of serious crimes per 100,000 person in 1981. Unemployment is percent of people looking for work in 1980.

The geometries are polygons that define the county borders.
```{r}
library(ggplot2)

ggplot(data = PE.sf) +
  geom_sf()
```

To estimate autocorrelation for any variable in the data frame, you need to first assign the neighbors and weights for each region. 

The default options in the `spdep::poly2nb()` and `spdep::nb2listw()` result in neighbors defined by 'queen' contiguity (polygon intersections can include a single point) and weights defined by row standardization (the sum of the weights equals the number of regions).
```{r}
nbs <- spdep::poly2nb(PE.sf)
wts <- spdep::nb2listw(nbs)
```

Alternatively you can specify the number of neighbors and then assign neighbors based on proximity (closeness). Here you first extract the coordinates of the polygon centroids as a matrix.
```{r}
coords <- PE.sf |>
  sf::st_centroid() |>
  sf::st_coordinates()
head(coords)
```

Then use the `spdep::knearneigh()` function on the coordinate matrix and specify the number of neighbors with the `k =` argument. Here you set it to six. That is, allow each county to have 6 closest neighbors.

Since the CRS is geographic you need to include the `longlat = TRUE` argument so distances are calculated using great circles.
```{r}
knn <- spdep::knearneigh(coords, 
                         k = 6, 
                        longlat = TRUE)
names(knn)
head(knn$nn)
```

The output is a list of five elements with the first element a matrix with the row dimension the number of counties and the column dimension the number of neighbors. 

Note that by using distance to define neighbors the matrix is not symmetric. For example, county 3 is a neighbor of county 2, but county 2 is not a neighbor of county 3.

Certain spatial models require the neighbor matrix to be symmetric. That is if region X is a neighbor of region Y then region Y must be a neighbor of region X.

You turn this matrix into a neighbor object (class `nb`) with the `spdep::knn2nb()` function. 
```{r}
nbs2 <- spdep::knn2nb(knn)
summary(nbs2)
```

If you include the argument `sym = TRUE` in the `knn2nb()` function then it forces the neighbor matrix to be symmetric.
```{r}
nbs3 <- spdep::knn2nb(knn,
                      sym = TRUE)
summary(nbs3)
```

The result shows that six is now the minimum number of nearest neighbors with some counties having has many as 10 neighbors to guarantee symmetry.

Compare the default adjacency neighborhoods with the nearest-neighbor neighborhoods.
```{r}
plot(sf::st_geometry(PE.sf), border = "grey")
plot(nbs, coords, add = TRUE)

plot(sf::st_geometry(PE.sf), border = "grey")
plot(nbs2, coords, add = TRUE)
```

Toggle between the plots.

A difference between the two neighborhoods is the number of links on counties along the borders. The nearest-neighbor defined neighborhoods have more links. Note: when neighbors are defined by proximity counties can share a border but they still may not be neighbors.

Your choice of neighbors is based on domain specific knowledge. If the process you are interested in can be described by a dispersal mechanism then proximity definition might be the right choice for defining neighbors. If the process can be described by a border diffusion mechanism then contiguity might be the right choice.

Create weight matrices for these alternative neighborhoods using the same `spdep::nb2listw()` function.
```{r}
wts2 <- spdep::nb2listw(nbs2)
wts3 <- spdep::nb2listw(nbs3)
```

You compute Moran's I for the percentage of white people variable (`WHITE`) with the `moran()` function separately for the three different weight matrices.
```{r}
spdep::moran(PE.sf$WHITE,
             listw = wts,
             n = length(nbs),
             S0 = spdep::Szero(wts))

spdep::moran(PE.sf$WHITE,
             listw = wts2,
             n = length(nbs2),
             S0 = spdep::Szero(wts2))

spdep::moran(PE.sf$WHITE,
             listw = wts3,
             n = length(nbs3),
             S0 = spdep::Szero(wts3))
```

Values of Moran's I are constrained between -1 and +1. In this case the neighborhood definition has little or no impact on inferences made about spatial autocorrelation. The kurtosis is between 2 and 4 consistent with a set of values from a normal distribution.

In a similar way you compute the Geary's C statistic.
```{r}
spdep::geary(PE.sf$WHITE, 
             listw = wts,
             n = length(nbs), 
             S0 = spdep::Szero(wts), 
             n1 = length(nbs) - 1)
```

Values of Geary's C range between 0 and 2 with values less than one indicating positive autocorrelation.

If the values of Moran's I and Geary's C result in different interpretations about the amount of clustering then it is a good idea to examine _local_ variations in autocorrelation.

## Assessing the statistical significance of autocorrelation {-}

Attribute values randomly placed across a spatial domain will result in some autocorrelation. Statistical tests provide a way to guard against being fooled by this randomness. For example, claiming a 'hot spot' when none exists. In statistical parlance, is the value of Moran's I significant with respect to the null hypothesis of no autocorrelation? 

One way to answer this question is to draw an uncertainty band on the regression line in a Moran scatter plot. If a horizontal line can be placed entirely within the band then the slope (Moran's I) is not significant against the null hypothesis of no autocorrelation.

More formally the question is answered by comparing the standard deviate ($z$ value) of the I statistic to the appropriate value from a standard normal distribution. This is done using the `spdep::moran.test()` function, where the $z$ value is the difference between I and the expected value of I divided by the square root of the variance of I.

The function takes a variable name or numeric vector and a spatial weights list object in that order. The argument `randomisation = FALSE` means the variance of I is computed under the assumption of normally distributed unemployment (`UNEMP`) rates.
```{r}
( mt <- spdep::moran.test(PE.sf$UNEMP, 
                          listw = wts,
                          randomisation = FALSE) )
```

Moran's I is .218 with a variance of .0045. The $z$ value for I is 3.41 giving a $p$-value of .0003 under the null hypothesis of no autocorrelation. Thus you reject the null hypothesis and conclude there is weak but significant autocorrelation in unemployment rates across Mississippi at the county level.

Outputs from the `spdep::moran.test()` function are in the form of a list.
```{r}
str(mt)
```

The list element called `estimate` is a vector of length three containing Moran's I, the expected value of Moran's I under the assumption of no autocorrelation, and the variance of Moran's I. 

The $z$ value is the difference between I and it's expected value divided by the square root of the variance.
```{r}
( mt$estimate[1] - mt$estimate[2] ) / sqrt(mt$estimate[3])
```

The $p$-value is the area under a standard normal distribution curve to the right (`lower.tail = FALSE`) of 3.4102 (`mt$statistic`).
```{r}
pnorm(mt$statistic, 
      lower.tail = FALSE)

curve(dnorm(x), from = -4, to = 4, lwd = 2)
abline(v = mt$statistic, col = 'red')
```

So about .03% of the area lies to the right of the red line.

Recall the $p$-value summarizes the evidence in support of the null hypothesis. The smaller the $p$-value, the less evidence there is in support of the null hypothesis. 

In this case it is the probability that the county unemployment rates could have been arranged at random across the state if the null hypothesis is true. The small $p$-value tells you that the spatial arrangement of the data is unusual with respect to the null hypothesis.

The interpretation of the $p$-value is stated as evidence AGAINST the null hypothesis. This is because interest lies in the null hypothesis being untenable. A $p$-value less than .01 is said to provide convincing evidence against the null, a $p$-value between .01 and .05 is said to provide moderate evidence against the null, and a $p$-value between .05 and .15 is said to be suggestive, but inconclusive in providing evidence against the null. A $p$-value greater than .15 is said to provide no evidence against the null. 

Note you do not interpret "no evidence" as "no effect (no autocorrelation)".

Under the assumption of normal distributed and uncorrelated data, the expected value for Moran's I is -1/(n-1) where n is the number of regions. 

A check on the distribution of unemployment rates indicates that normality is somewhat suspect. A good way to check the normality assumption is to use the `sm.density()` function from the {sm} package.
```{r}
if(!require(sm)) install.packages("sm", repos = "http://cran.us.r-project.org")

sm::sm.density(PE.sf$UNEMP, 
               model = "Normal",
               xlab = "Unemployment Rates")
```

The unemployment rates are less "peaked" (lower kurtosis) than a normal distribution. In this case it is better to use the default `randomisation = TRUE` argument.

Further, the assumptions underlying Moran's test are sensitive to the form of the graph of neighbor relationships and other factors so results should be checked against a test that involves permutations.

A random sampling approach to inference is made with the `spdep::moran.mc()` function. MC stands for Monte Carlo which refers to the city of Monte Carlo in Monaco famous for its gambling casinos.

The name of the data vector and the weights list object (`listw`) are required as is the number of permutations (`nsim`). Each permutation is a random rearrangement of the unemployment rates across the counties. This removes the spatial autocorrelation but keeps the non-spatial distribution of the unemployment rates. The neighbor topology and weights remain the same.

For each permutation (random shuffle of the data values), I is computed and saved. The $p$-value is obtained as the ratio of the number of permuted I values greater or exceeding the observed I over the number of permutation plus one. In the case where there are 5 permuted I values greater or equal to the observed value based on 99 simulations, the $p$-value is 5/(99 + 1) = .05.

For example, if you want inference on I using 9999 permutations type
```{r}
set.seed(40453)

( mP <- spdep::moran.mc(PE.sf$UNEMP, 
                        listw = wts,
                        nsim = 9999) )
```

Nine of the permutations yield a Moran's I greater than .218, hence the $p$-value as evidence in support of the null hypothesis (the true value for Moran's I is zero) is .0009.

Note: you initiate the random number generator with a seed value (any will do) so that the set of random permutations of the values across the domain will be the same each time you run this code chunk. This is important for reproducibility. The default random number generator seed value is determined from the current time (internal clock) and so no random permutations will be identical. To control the seed use the `set.seed()` function.

The values of I computed for each permutation are saved in the vector `mP$res`.
```{r}
head(mP$res)
tail(mP$res)
```

The last value in the vector is I computed using the data in the correct counties. The $p$-value as evidence in support of the null hypothesis that I is zero is given as
```{r}
sum(mP$res > mP$res[10000])/9999
```

A density graph displays the distribution of permuted I's.
```{r}
df <- data.frame(mp = mP$res[-10000])
ggplot(data = df,
       mapping = aes(mp)) + 
  geom_density() + 
  geom_rug() + 
  geom_vline(xintercept = mP$res[10000], 
             color = "red", size = 2) +
  theme_minimal()
```

The density curve is centered just to the left of zero consistent with the theoretical expectation (mean).

What to do with the knowledge that the unemployment rates have significant autocorrelation? By itself, not much, but it can provide notice that something might be going on in certain regions (hot spot analysis).

The knowledge is useful after other factors are considered. In the language of statistics, knowledge of significant autocorrelation in the model residuals can help you build a better model.

## Bivariate spatial autocorrelation {-}

The idea of spatial autocorrelation can be extended to two variables. It is motivated by the fact that aspatial bi-variate association measures, like Pearson's correlation, do not recognize the spatial arrangement of the regions.

Consider the correlation between police expenditure (`POLICE`) and the amount of crime (`CRIME`) in the police expenditure data set.
```{r}
police <- PE.sf$POLICE
crime <- PE.sf$CRIME

cor.test(police, crime, method = "pearson")
```

You note a significant (direct) correlation ($p$-value < .01) exists between these two variables. 

But you also note some significant spatial autocorrelation in each of the variables separately.
```{r}
spdep::moran.test(police, 
                  listw = wts)
spdep::moran.test(crime, 
                  listw = wts)
```

The Lee statistic integrates the Pearson correlation as an aspatial bi-variate association metric with Moran's I as a uni-variate spatial autocorrelation metric. The formula is
$$
L(x,y) = \frac{n}{\sum_{i=1}^{n}(\sum_{j=1}^{n}w_{ij})^2}
\frac{\sum_{i=1}^{n}(\sum_{j=1}^{n}w_{ij}(x_i-\bar{x})) ((\sum_{j=1}^{n}w_{ij}(y_j-\bar{y}))}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2} \sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}}
$$

The formula is implemented in the `spdep::lee()` function where the first two arguments are the variables of interest and you need to include the weights matrix and the number of regions. The output from this function is a list of two with the first being the value of Lee's statistic (`L`).
```{r}
spdep::lee(crime, police, 
           listw = wts, 
           n = length(nbs))$L
```

Values of L range between -1 and +1 with the value here of .13 indicating relatively weak bi-variate spatial autocorrelation between crime and police expenditures. Statistically you infer that crime in a county has some influence on police expenditure in that county and in the neighboring counties, but not much.

The `crime` and `police` variables are not adequately described by a normal distribution.
```{r}
par(mfrow = c(2, 1))
sm::sm.density(crime, model = "normal")
sm::sm.density(police, model = "normal")
```

Thus you perform a non-parametric test on the bi-variate spatial autocorrelation with the `spdep::lee.mc()` function. The crime and police expenditure values are randomly permuted and values of `L` are computed for each permutation.
```{r}
spdep::lee.mc(crime, police, 
              listw = wts, 
              nsim = 999)
```

Based on a $p$-value that exceeds .05 you conclude that there is no significant bi-variate spatial autocorrelation between crime and police expenditure in these data.

## Local indicators of spatial autocorrelation {-}

The Moran's I statistic was first used in the 1950s. Localization of the statistic was presented by Luc Anselin in 1995 (Anselin, L. 1995. Local indicators of spatial association, Geographical Analysis, 27, 93–115).

Earlier you saw the `raster::MoranLocal()` function from the {raster} package returns a raster of local Moran's I values.

Local I is a deconstruction of global I where geographic proximity is used in two ways. (1) to define and weight neighbors and (2) to determine the spatial scale over which I is computed.

Using queen's contiguity you determine the neighborhood topology and the weights for the police expenditure data from Mississippi. Here you print them in the full matrix form with the `spdep::list2mat()` function.
```{r}
round(spdep::listw2mat(wts)[1:5, 1:10], 2)
```

The matrix shows that the first county has three neighbors 2, 3, and 9 and each get a weight of 1/3. The third county has four neighbors 1, 4, 9 and 10 and each gets a weight of 1/4.

Compute local Moran's I on the percentage of white people using the `spdep::localmoran()` function. Two arguments are needed (1) the attribute variable for which you want to compute local correlation and (2) the weights matrix as a list object.
```{r}
Ii_stats <- spdep::localmoran(PE.sf$WHITE, 
                              listw = wts)
str(Ii_stats)
```

The local I is stored in the first column of a matrix where the rows are the counties. The other columns are the expected value for I, the variance of I, the $z$ value and the $p$-value. For example, the local I statistics from the first county are given by typing
```{r}
head(Ii_stats)
```

Because these local values must average to the global value (when using row standardized weights), they can take on values outside the range between -1 and 1. A `summary()` method on the first column of the `Li`  object gives statistics from the non-spatial distribution of I's.
```{r}
summary(Ii_stats[, 1])
```

You map the values by first attaching the matrix columns of interest to the simple feature data frame. Here you attach `Ii`, `Var`, and `Pi`.
```{r}
PE.sf$Ii <- Ii_stats[, 1]
PE.sf$Vi <- Ii_stats[, 3]
PE.sf$Pi <- Ii_stats[, 5]
```

Then using the {ggplot2} syntax.
```{r}
( g1 <- ggplot(data = PE.sf) +
  geom_sf(aes(fill = Ii)) +
  scale_fill_gradient2(low = "green",
                       high = "blue") )
```

You also map out the variances.
```{r}
ggplot(data = PE.sf) +
  geom_sf(aes(fill = Vi)) +
  scale_fill_gradient()
```

Variances are larger for counties near the boundaries as the sample sizes are smaller.

Compare the map of local autocorrelation with a map of percent white. 
```{r}
( g2 <- ggplot(data = PE.sf) +
  geom_sf(aes(fill = WHITE)) +
  scale_fill_gradient(low = "black",
                      high = "white") )
```

Plot them together.
```{r}
library(patchwork)

g1 + g2
```

Areas where percent white is high over the northeast are areas with the largest spatial correlation. Other areas of high spatial correlation include the Mississippi Valley and in the south. Note the county with the most negative spatial correlation is the county in the northwest with a fairly high percentage of whites neighbored by counties with much lower percentages of whites.

Local values of Lee's bi-variate spatial autocorrelation are available from the `spdep::lee()` function.
```{r}
lee_stat <- spdep::lee(crime, police, 
                       listw = wts, 
                       n = length(nbs))

PE.sf$localL <- lee_stat$localL

tmap::tm_shape(PE.sf) +
  tmap::tm_fill("localL",
                title = "") +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_layout(title = "Local bi-variate spatial autocorrelation",
                  legend.outside = TRUE)
```

Areas in dark green indicate where the correlation between crime and policing is most influenced by neighboring crime and policing.

Population and tornado reports

Is the frequency of tornado reports correlated with the number of people in a region? Might this correlation extend to the number of people in neighboring region?

To answer these questions you quantify the non-spatial correlation and the bi-variate spatial autocorrelation between tornado occurrences and population. To keep this manageable you focus on one state (Iowa).

Start by getting the U.S. Census data with functions from the {tidycensus} package. Downloading U.S. census data using functions from the {tidycensus} package requires you register with the Census Bureau. 

You can get an API key from http://api.census.gov/data/key_signup.html. Then use the `tidycensus::census_api_key()` function and put your key in quotes.
```{r, eval=FALSE}
tidycensus::census_api_key("YOUR API KEY GOES HERE")
```

The `get_decennial()` function grants access to the 1990, 2000, and 2010 decennial US Census data and the `get_acs()` function grants access to the 5-year American Community Survey data. For example, here is how you get county-level population for Iowa.
```{r}
Counties.sf <- tidycensus::get_acs(geography = "county", 
                                   variables = "B02001_001E", 
                                   state = "IA",
                                   geometry = TRUE)
```

The code returns a simple feature data frame with county borders as multi-polygons. The variable `B02001_001E` is the 2015-2019 population estimate in each county within the state.

Next get the tornado data and count the number of tracks by county. A single track can intersect more than one county.
```{r}
Torn.sf <- sf::st_read(dsn = here::here("data", "1950-2020-torn-aspath"), 
                       layer = "1950-2020-torn-aspath") |>
  sf::st_transform(crs = sf::st_crs(Counties.sf)) |>
  dplyr::filter(yr >= 2015)

( TorCounts.df <- Torn.sf |>
  sf::st_intersection(Counties.sf) |>
  sf::st_drop_geometry() |>
  dplyr::group_by(GEOID) |>
  dplyr::summarize(nT = dplyr::n()) )
```

Next join the counts to the simple feature data frame.
```{r}
Counties.sf <- Counties.sf |>
  dplyr::left_join(TorCounts.df,
                   by = "GEOID") |>
  dplyr::mutate(nT = tidyr::replace_na(nT, 0)) |>
  dplyr::mutate(Area = sf::st_area(Counties.sf),
                rate = nT/Area/(2020 - 2015 + 1) * 10^10,
                lpop = log10(estimate))
```

Note that some counties have no tornadoes and the `dplyr::left_join()` returns a value of `NA` for those. You use `dplyr::mutate()` with `tidyr::replace_na()` to turn those counts to a value of 0.

Make a two-panel map displaying the log of the population and the tornado rates.
```{r}
map1 <- tmap::tm_shape(Counties.sf) +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_fill(col = "lpop",
                title = "Log Population",
                palette = "Blues") +
  tmap::tm_layout(legend.outside = "TRUE")

map2 <- tmap::tm_shape(Counties.sf) +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_fill(col = "rate",
                title = "Annual Rate\n[/10,000 sq. km]",
                palette = "Greens") +
  tmap::tm_layout(legend.outside = "TRUE")

tmap::tmap_arrange(map1, map2)
```

There appears some relationship. The non-spatial correlation between the two variables is obtained with the `cor.test()` function.
```{r}
lpop <- Counties.sf$lpop
rate <- as.numeric(Counties.sf$rate)

cor.test(lpop, rate)
```

The bi-variate spatial autocorrelation is assessed using the Lee statistic. A formal non-parametric test under the null hypothesis of no bi-variate spatial autocorrelation is done using a Monte Carlo simulation.
```{r}
nbs <- spdep::poly2nb(Counties.sf)
wts <- spdep::nb2listw(nbs)

lee_stat <- spdep::lee(lpop, rate, 
                       listw = wts, 
                       n = length(nbs))
lee_stat$L

spdep::lee.mc(lpop, rate, listw = wts, nsim = 9999)
```

Finally you map out the local variation in the bi-variate spatial autocorrelation.
```{r}
Counties.sf$localL <- lee_stat$localL

tmap::tm_shape(Counties.sf) +
  tmap::tm_fill("localL",
                title = "Local Bivariate\nSpatial Autocorrelation") +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_layout(legend.outside = TRUE)
```

What might cause this? Compare with Kansas.

Also, compare local Lee with local Moran.
```{r}
Ii_stats <- spdep::localmoran(rate, 
                              listw = wts)
Counties.sf$localI = Ii_stats[, 1]

tmap::tm_shape(Counties.sf) +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_fill(col = "localI",
                title = "Local Autocorrelation",
                palette = "Purples") +
  tmap::tm_layout(legend.outside = "TRUE")
```

<!--chapter:end:10-Lesson.Rmd-->

# Tuesday October 4, 2022 {.unnumbered}

**"The most important single aspect of software development is to be clear about what you are trying to build."** – Bjarne Stroustrup

Today

- Constraining group membership based on spatial autocorrelation
- Estimating spatial autocorrelation in model residuals
- Choosing a spatial regression model

## Constraining group membership based on spatial autocorrelation {.unnumbered}

As a spatial data analyst you likely will face the situation in which there are many variables and you need to group them in a way that minimizes inter-group variation but maximizes between-group variation. If you know the number of groups a priori then a common grouping (or clustering) method is called K-means.

If your data is spatial you will want the additional constraint that the resulting groups be geographically linked. In fact there are many situations that require separating geographies into discrete but contiguous regions (regionalization) such as designing communities, planning areas, amenity zones, logistical units, or even for the purpose of setting up experiments with real world geographic constraints.

There are many situations where the optimal grouping using traditional cluster metrics is sub-optimal in practice because of these geographic constraints.

Unconstrained grouping on data with spatial characteristics may result in contiguous regions because of autocorrelation, but if you want to ensure that all groups are spatially-contiguous you need a method specifically designed for the task. The 'skater' algorithm available in the {spdep} package is well-implemented and well-documented.

The 'skater' algorithm (spatial 'k'luster analysis by tree edge removal) builds a connectivity graph to represent spatial relationships between neighboring areas, where each area is represented by a node and edges represent connections between areas. Edge costs are calculated by evaluating the dissimilarity in attribute space between neighboring areas. The connectivity graph is reduced by pruning edges with higher dissimilarity.

Consider again the crime data at the tract level in the city of Columbus, Ohio. The tract polygons are projected with arbitrary spatial coordinates.
```{r}
( CC.sf <- sf::st_read(dsn = here::here("data", "columbus"),
                       layer = "columbus") )
```

First, create choropleth maps of housing value, income, and crime.
```{r}
tmap::tm_shape(CC.sf) +
  tmap::tm_fill(col = c("HOVAL", "INC", "CRIME"))
```

The maps show distinct regional patterns. Housing values and income are clustered toward the southeast and crime is clustered in the center. But although housing values are also high in the north you don't necessarily want to group that tract with those in the southeast because they are geographically distinct.

To group these patterns under the constraint of spatial contiguity you first scale the attribute values and center them using the `scale()` function. Scaling and centering variables should be done with any clustering approaches.
```{r}
( CCs.df <- CC.sf |> 
    dplyr::mutate(HOVAL = scale(HOVAL),
                  INC = scale(INC),
                  CRIME = scale(CRIME)) |>
    dplyr::select(HOVAL, INC, CRIME) |>
    sf::st_drop_geometry() )
```

Next create adjacency neighbors using queen contiguity.
```{r}
nbs <- spdep::poly2nb(CC.sf, 
                      queen = TRUE)

plot(CC.sf$geometry)
plot(nbs, 
     sf::st_centroid(sf::st_geometry(CC.sf)),
     add = TRUE)
```

Next combine the contiguity graph with your scaled attribute data to calculate edge costs based on the distance between each node. The function `spdep::nbcosts()` provides distance methods for Euclidean, Manhattan, Canberra, binary, Minkowski, and Mahalanobis, and defaults to Euclidean if not specified.
```{r}
costs <- spdep::nbcosts(nbs, 
                        data = CCs.df)
```

Next transform the edge costs into spatial weights using the `spdep::nb2listw()` function before constructing the minimum spanning tree with the weights list.
```{r}
wts <- spdep::nb2listw(nbs,
                       glist = costs,
                       style = "B")
mst <- spdep::mstree(wts)

head(mst)
```

Edges with higher dissimilarity are removed leaving a set of nodes and edges that take the minimum sum of dissimilarities across all edges of the tree (a minimum spanning tree).

The edge connecting node (tract) 33 with node (tract) 35 has a dissimilarity of .56 units. The edge connecting tract 35 with tract 43 has a dissimilarity of .19 units.

Finally, the `spdep::skater()` function partitions the graph by identifying which edges to remove based on dissimilarity while maximizing the between-group variation. The `ncuts =` argument specifies the number of partitions to make, resulting in `ncuts` + 1 groups.
```{r}
clus5 <- spdep::skater(edges = mst[,1:2], 
                       data = CCs.df, 
                       ncuts = 4)
```

Where are these groups located?
```{r}
CC.sf <- CC.sf |>
  dplyr::mutate(Group = clus5$groups)

library(ggplot2)
ggplot() +
  geom_sf(data = CC.sf,
          mapping = aes(fill = factor(Group)))
```

The map shows five distinct regions based on the three variables of income, housing value, and crime. Importantly the regions are contiguous.

As a comparison, here is the result of grouping the same three variables using hierarchical clustering using the method of minimum variance (Ward) and without regard to spatial contiguity.
```{r}
dd <- dist(CCs.df)
hc <- hclust(dd, 
             method = "ward.D")
hcGroup <- cutree(hc, k = 5)
CC.sf <- CC.sf |>
  dplyr::mutate(hcGroup = hcGroup)

ggplot() +
  geom_sf(data = CC.sf,
          mapping = aes(fill = factor(hcGroup)))
```

Here the map shows five regions but the regions are not contiguous.

More information: https://www.tandfonline.com/doi/abs/10.1080/13658810600665111

## Estimating spatial autocorrelation in model residuals {.unnumbered}

A spatial regression model should be considered for your data whenever the residuals resulting from a aspatial regression exhibit spatial autocorrelation. A common way to proceed is to first regress the response variable onto the explanatory variables and check for autocorrelation in the residuals.

If there is significant spatial autocorrelation in the residuals then a spatial regression model should be considered.

Let's stay with the Columbus crime data and fit a linear regression model with `CRIME` as the response variable and `INC` and `HOVAL` as the explanatory variables. At the level of tracts, how well does income and housing value statistically explain the amount of crime?
```{r}
model <- lm(CRIME ~ INC + HOVAL, 
            data = CC.sf)
summary(model)
```

The model statistically explains 55% of the variation in crime as can be seen by the multiple R-squared value. Looking at the coefficients (values under the `Estimate` column), you see that _higher_ incomes are associated with lower values of crime (negative coefficient) and _higher_ housing values are associated with lower crime. For every one unit increase in income, crime values decrease by 1.6 units.

Use the `residuals()` method to extract the vector of residuals from the model.
```{r}
( res <- residuals(model) )
```

There are 49 residuals one for each tract. The residuals are the difference between the observed crime rates and the predicted crime rates (observed - predicted). A residual that has a value greater than 0 indicates that the model _under_ predicts the observed crime rate in that tract and a residual that has a value less than 0 indicates that the model _over_ predicts the observed crime rate.

If you plot the residuals, they should be approximated by a normal distribution. You check this with the `sm::sm.density()` function with the first argument the vector of residuals (`res`) and the argument `model =` set to "Normal".
```{r}
sm::sm.density(res, 
               model = "Normal")
```

The density curve of the residuals (black line) fits completely within the blue ribbon that defines a normal distribution.

Next create a map of the model residuals. Do the residuals show any pattern of clustering? Since the values in the vector of residuals `res` are arranged in the same order as the rows in the simple feature data frame you create a new column in the data frame using the `$` syntax and calling the new column `res`.
```{r}
CC.sf$res <- res

tmap::tm_shape(CC.sf) +
  tmap::tm_fill(col = "res") +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_layout(title = "Linear model residuals")
```

The map shows contiguous tracts with negative residuals across the southwestern and southern part of the city and a group of contiguous tracts with positive residuals toward the center. 

The map indicates some clustering but the clustering appears to be less than with the crime values themselves. That is, after accounting for regional factors related to crime, the autocorrelation is reduced.

To determine the amount of autocorrelation in the residuals use the `spdep::lm.morantest()` function, passing the regression model object and the weights object to it. Note that you once again use the default neighborhood and weighting schemes.
```{r}
nbs <- spdep::poly2nb(CC.sf)
wts <- spdep::nb2listw(nbs)

spdep::lm.morantest(model, 
                    listw = wts)
```

Moran's I on the model residuals is .22. This compares with the value of .5 on the value of crime alone. Part of the autocorrelation in the crime rates is statistically 'absorbed' by the explanatory factors.

But does this output let you know if you need a spatial regression model?

The $p$-value on I of .002, thus you reject the null hypothesis of no spatial autocorrelation in the model residuals and conclude that a spatial regression model would improve the fit. The $z$-value (as the basis for the $p$-value) takes into account the fact that these are residuals from a model so the variance is adjusted accordingly.

Given significant spatial autocorrelation in the model residuals, the next step is to choose the type of spatial regression model.

## Choosing a spatial regression model {.unnumbered}

Ordinary least-squares regression models fit to spatial data can lead to improper inference because observations are not independent. This might lead to poor policy decisions. Thus it's necessary to check the residuals from an ordinary least-squares model for autocorrelation. If the residuals are strongly correlated the model is not specified properly.

You can try to improve the model by adding variables. If that's not possible (no additional data, or no clue as to what variable to include), you can try a spatial regression model. Spatial regression models are widely used in econometrics and epidemiology.

The equation for a regression model in vector notation is
$$
y = X \beta + \varepsilon
$$
where $y$ is a $n$ by 1 vector of response variable values, $X$ is a $n$ by $p+1$ matrix containing the explanatory variables and augmented by a column of ones for the intercept term, $\beta$ is a $p+1$ $\times$ 1 vector of model coefficients and $\varepsilon$ is a $n$ by 1 vector of residuals (iid).

A couple options exist if the elements of the vector $\varepsilon$ are correlated. One is to include a spatial lag term so the model becomes
$$
y = \rho W y + X \beta + \varepsilon
$$
where $Wy$ is the weighted average of the neighborhood response values (spatial lag variable) with $W$ the spatial weights matrix, and $\rho$ is the autoregression coefficient. This is called a spatial autoregressive (SAR) model.

Note: $Wy$ is the spatial lag variable you compute with the `spdep::lag.listw()` function and $\rho$ is Moran's I. Thus the model is also called a spatial lag model (SLM).

Justification for the spatial lag model is domain specific but motivated by a 'diffusion' process. The response variable $y_i$ is influenced by the explanatory variables at location $i$ and by explanatory variables at locations $j$.

$\rho Wy$ is called the _spatial signal_ term and $\beta X$ is called the _trend_ term.

Another option is to include a spatial error term so the model becomes
$$
y = X\beta + \lambda W \epsilon + u
$$
where $\lambda$ is the autoregression coefficient, $W\epsilon$ is the spatial error term representing the weighted average of the neighborhood residuals, and $u$ are the overall residuals assumed to be iid. This is called a spatial error model (SEM).

Here the lag term is computed using the residuals rather the response variable.

Application of the spatial error model is motivated by the omitted variable bias. Suppose the variable $y$ is statistically described by two variables $x$ and $z$ each centered on zero and independent. Then
$$
y = \beta x + \theta z
$$

If $z$ is not observed, then the vector $\theta z$ is nested in the error term $\epsilon$.
$$
y = \beta x + \epsilon
$$

Examples of an unobserved latent variable $z$ include local culture, social capital, neighborhood readiness. Importantly you would expect the latent variable to be spatially correlated (e.g., culture will be similar across neighborhoods), so let
$$
z = \lambda W z + r\\
z = (I - \lambda W)^{-1} r
$$
where $r$ is a vector of random independent residuals (e.g., culture is similar but not identical), $W$ is the spatial weights matrix and $\lambda$ is a scalar spatial correlation parameter. Substituting into the equation above
$$
y = \beta x + \theta z \\
y = \beta x +   \theta (I - \lambda W)^{-1} r\\
y = \beta x + (I - \lambda W)^{-1} \varepsilon
$$
where $\varepsilon = \theta r$.

Another motivation for considering a spatial error model is heterogeneity. Suppose you have multiple observations for each unit. If you want a model that incorporates individual effects you can include a $n \times 1$ vector $a$ of individual intercepts for each unit.
$$
y = a + X\beta
$$
where now $X$ is a $n$ $\times$ $p$ matrix.

In a cross-sectional setting with one observation per unit (typically the case in observational studies), this approach is not possible since you will have more parameters than observations.

Instead you can treat $a$ as a vector of spatial random effects. You assume that the intercepts follows a spatially smoothed process
$$
a = \lambda W a + \epsilon \\
a = (I - \lambda W)^{-1} \epsilon
$$
which leads to the previous model
$$
y = X\beta + (I - \lambda W)^{-1} \epsilon
$$

In the absence of domain-specific knowledge of the process that might be responsible for the autocorrelated residuals, you can run some statistical tests on the linear model.

The tests are performed with the `spdep::lm.LMtests()` function. The `LM` stands for 'Lagrange multiplier' indicating that the technique simultaneously determines the coefficients on the explanatory variables AND the coefficient on the spatial lag variable.

The test type is specified as a character string. The tests should be considered in a sequence starting with the standard versions and moving to the 'robust' versions if the choice remains ambiguous.

To perform LM tests you specify the model object, the weights matrix, and the two model types using the `test =` argument. The model types are specified as character strings `"LMerr"` and `"LMlag"` for the spatial error and lag models, respectively.
```{r}
spdep::lm.LMtests(model, 
                  listw = wts, 
                  test = c("LMerr", "LMlag"))
```

The output shows that both the spatial error and spatial lag models are significant ($p$-value < .15). Ideally one model is significant and the other is not, and you choose the model that is significant.

Since both are significant, you test again. This time you use the robust forms of the statistics with character strings `"RLMerr"` and `"RLMlag"` in the `test =` argument.
```{r}
spdep::lm.LMtests(model, 
                  listw = wts, 
                  test = c("RLMerr", "RLMlag"))
```

Here the error model has a large $p$-value and the lag model has a $p$-value that is less than .15 so you choose the lag model for your spatial regression.

A decision tree (from Luc Anselin) shows the sequence of tests for making a choice about which type of spatial model to use [Decision Tree](http://myweb.fsu.edu/jelsner/temp/SpatialRegressionDecisionTree.png)

If both tests show significance models, then you should fit both models and check which one results in the lowest information criteria (AIC).

Another options is to include both a spatial lag term and a spatial error term into a single model.

<!--chapter:end:11-Lesson.Rmd-->

# Thursday October 6, 2022 {.unnumbered}

**"Feeling a little uncomfortable with your skills is a sign of learning, and continuous learning is what the tech industry thrives on!"** --- Vanessa Hurst

Today

-   Fitting and interpreting a spatially-lagged Y model
-   Fitting and interpreting a spatially-lagged X model
-   Fitting and interpreting spatial Durban models

Ordinary least-squares regression models fit to spatial data can lead to improper inference because observations are not independent. This might lead to poor policy decisions. Thus it's necessary to check the residuals from an aspatial model for autocorrelation. If the residuals are strongly correlated the model is not specified properly.

## Fitting and interpreting a spatially-lagged Y model {-}

Continuing with the Columbus crime data.

```{r}
( CC.sf <- sf::st_read(dsn = here::here("data", "columbus"),
                       layer = "columbus") )
```

Recall interest with these data centers on crime as a response variable and income and housing value as explanatory variables. You set the formula as a string and refit a OLS regression model.

```{r}
f <- CRIME ~ INC + HOVAL

( model.ols <- lm(f, data = CC.sf) )
```

The marginal effect of income on crime is -1.6 and the marginal effect of housing value on crime is -.27.

A nice way to visualize the relative significance of the explanatory variables is to make a plot. Here you use the `broom::tidy()` method and then `ggplot()` as follows.

```{r}
if(!require(broom)) install.packages(pkgs = "broom", repos = "http://cran.us.r-project.org")
library(broom)

( d <- broom::tidy(model.ols, 
                   conf.int = TRUE) )

library(ggplot2)

ggplot(d[-1,], aes(x = estimate,  # we do not plot the intercept term
                   y = term, 
                   xmin = conf.low, 
                   xmax = conf.high, 
                   height = 0)) +
  geom_point(size = 2) +
  geom_vline(xintercept = 0, lty = 4) +
  geom_errorbarh()
```

The maximum likelihood estimate is shown as a point and the confidence interval around the estimate is shown as a horizontal error bar. The default confidence level is 95% (`conf.level = .95`). The effects are statistically significant as the confidence intervals do not intersect the zero line (dashed-dotted).

Then check for spatial autocorrelation in the residuals. This is done by first defining the weights matrix and then applying Moran's I test as follows.

```{r}
nbs <- spdep::poly2nb(CC.sf, 
                      queen = TRUE)
wts <- spdep::nb2listw(nbs)

spdep::lm.morantest(model.ols, 
                    listw = wts)
```

The results show that the model residuals have significant spatial autocorrelation so reporting the marginal effects with an OLS regression model would not be correct.

You then fit a spatially-lagged Y model using the `lagsarlm()` function from the {spatialreg} package. The model is 

$$
y = \rho W y + X \beta + \varepsilon
$$ 
where $Wy$ is the weighted average of the neighborhood response values (spatial lag variable) with $W$ the spatial weights matrix, and $\rho$ is the autoregression coefficient.

The `spatialreg::lagsarlm()` function first determines a value for $\rho$ ( with the internal `optimize()` function) and then the $\beta$'s are obtained using generalized least squares (GLS). The model formula `f` is the same as what you used to fit the OLS regression above. You save the model object as `model.slym`.

```{r}
if(!require(spatialreg)) install.packages(pkgs = "spatialreg", repos = "http://cran.us.r-project.org")

model.slym <- spatialreg::lagsarlm(formula = f, 
                                   data = CC.sf, 
                                   listw = wts)

summary(model.slym)
```

The first batch of output concerns the model residuals and the coefficients on the explanatory variables. The model residuals are the observed crime rates minus the predicted crime rates.

The coefficients on income and housing have the same sign (negative) and they remain statistically significant (-1.05 for income and -.27 for housing value). But you can't interpret these coefficients as the marginal effects.

The next set of output is about the coefficient of spatial autocorrelation ($\rho$). The value is .423 and a likelihood ratio test gives a value of 9.41 which translates to a $p$-value of .002. The null hypothesis is the autocorrelation is zero, so you confidently reject it. This is consistent with the significant Moran's I value that you found in the linear model residuals.

Two other tests are performed on the value of $\rho$ including a z-test (t-test) using the asymptotic standard error and a Wald test. Both tests confirm that the lag term should be included in the model from crime involving income and housing values.

In spatial models that contain a lagged response term, the coefficients are not marginal effects. The spatial lag model allows for 'spillover'. That is a change in an explanatory variable anywhere in the study domain will affect the value of the response variable *everywhere*. Spillover occurs even when the neighborhood weights matrix represents local contiguity. The spillover makes interpreting the coefficients more complicated.

With a spatially-lagged Y model a change in the value of an explanatory variable results in both *direct* and *indirect* effects on the response variable.

For example, the direct effect gives the impact a change in income has on crime averaged over all tracts. It takes into account the effects that occur from a change in the $i$th tract's income on crime across neighboring tracts.

The indirect effect gives the impact of a change in income has on crime averaged over all *other* tracts. The indirect effect represent spillovers. The influences on the dependent variable $y$ in a region rendered by change in $x$ in some *other* region. For example, if all tracts $i \ne j$ (i not equal to j) increase their income, what will be the impact on crime in region $i$?

The total effect (TE) is the sum of the direct and indirect effects. It measures the total cumulative impact on crime arising from one tract $j$ increasing its income over all other tracts (on average). It is given by 

$$
\hbox{TE} = \left(\frac{\beta_k}{1-\rho^2}\right)\left(1 + \rho\right)
$$ 
where $\beta_k$ is the marginal effect of variable $k$ and $\rho$ is the spatial autocorrelation coefficient. With $\rho = 0$ TE is $\beta_k$.

Here $\beta_{INC}$ is -1.0487 and $\rho$ is .4233, so the total effect is

```{r}
( TE_INC <- -1.0487 / (1 - .4233^2) * (1 + .4233) )
```

The direct, indirect, and total effects are shown using the `spatialreg::impacts()` function.

```{r}
spatialreg::impacts(model.slym, 
                    listw = wts)
```

The direct effects are the changes in the response variable of a particular region arising from a one unit increase in an explanatory variable in that region.

The indirect effects are the changes in the response variable of a particular region arising from a one unit increase in an explanatory variable in another region. For example, due to spatial autocorrelation, a one-unit change in the income variable in region 1 affects the crime rate in regions 2 and 3.

The next set of output concerns the overall model fit. It includes the log likelihood value and the AIC (Akaike Information Criterion). The AIC value for the linear model is included. Here it is clear that the spatial lag model is an improvement (smaller AIC) over the aspatial model.

The larger the likelihood, the better the model and two times the difference in log likelihoods from two competing models divided by the number of observations gives a scale for how much improvement.

```{r}
x <- 2 * (logLik(model.slym) - logLik(model.ols))/49
x[1]
```

Improvement table

| Likelihood difference | Qualitative improvement |
|-----------------------|-------------------------|
| 1                     | huge                    |
| .1                    | large                   |
| .01                   | good                    |
| .001                  | okay                    |

The final bit of output is a Lagrange multiplier test for remaining autocorrelation. The null hypothesis is there is no remaining autocorrelation since we have a lag term in the model. The result is a high $p$-value so you are satisfied that the lag term takes care of the autocorrelation.

Compare the spatial lag model to a spatial error model. Here you use the `spatialreg::errorsarlm()` function.

```{r}
model.sem <- spatialreg::errorsarlm(formula = f, 
                                    data = CC.sf, 
                                    listw = wts)
summary(model.sem)
```

You find the coefficient of spatial autocorrelation ($\lambda$) is significant, but the log likelihood value from the model is smaller (-183.7) and the AIC value is larger (377.5) compared with corresponding values from the lag model. This is consistent with the lagrange multiplier (LM) tests indicating the spatial lag model is more appropriate.

Also you can compare the log likelihoods from the two spatial regression models that you fit.

```{r}
x <- 2 * (logLik(model.slym) - logLik(model.sem))/49
x[1]
```

With a value of .04 you conclude that there is good improvement of the lag model over the error model. Again, this is consistent with your decision above to use the lag model.

With the spatial error model the coefficients can be interpreted as marginal effects like with the OLS model.

If there are large differences (e.g., different signs) between the coefficient estimate from SEM and OLS, this suggests that neither model is yielding parameters estimates matching the underlying parameters of the data generating process.

You test whether there is a significant difference in coefficient estimates with the Hausman test under the hypothesis of no difference.

```{r}
spatialreg::Hausman.test(model.sem)
```

The $p$-value gives inconclusive evidence that the coefficients are different and that maybe the SEM is not the right way to proceed with these data.

The `predict()` method implements the `predict.sarlm()` function to calculate predictions from the spatial regression model. The prediction on a spatial lag Y model is decomposed into a "trend" term (explanatory variable effect) and a "signal" term (spatial smoother). The predicted fit is the sum of the trend and the signal terms when using the spatial lag model.

You make predictions with the `predict()` method under the assumption that the mean response is known. You examine the structure of the corresponding predict object.

```{r}
( predictedValues <- predict(model.slym) )
```

The predicted values are in the column labeled `fit`. The predicted values are a sum of the trend term ($X\beta$) and the signal term ($\rho W y$). The signal term is called the spatial smoother.

As a first-order check if things are what you think they are, compare the first five predicted values with the corresponding observed values.

```{r}
predictedValues[1:5]
CC.sf$CRIME[1:5]
```

Some predicted values are lower than the corresponding observed values and some are higher.

The predicted values along with the values for the trend and signal are added to the simple features data frame.

```{r}
CC.sf$fit <- as.numeric(predictedValues)
CC.sf$trend <- attr(predictedValues, "trend")
CC.sf$signal <- attr(predictedValues, "signal")
```

The components of the predictions are mapped and placed on the same plot.

```{r}
library(ggplot2)

( g1 <- ggplot() +
    geom_sf(data = CC.sf, aes(fill = fit)) +
    scale_fill_viridis_c() +
    ggtitle("Predicted Crime") )

( g2 <- ggplot() +
    geom_sf(data = CC.sf, aes(fill = trend)) +
    scale_fill_viridis_c() +
    ggtitle("Trend (Explanatory Variables)") )

( g3 <- ggplot() +
    geom_sf(data = CC.sf, aes(fill = signal)) +
    scale_fill_viridis_c() +
    ggtitle("Signal") )

library(patchwork)
g1 + g2 + g3
```

The trend term and the spatial smoother have similar ranges indicating nearly equal contributions to the predictions. The largest difference between the two terms occurs in the city's east side.

A map of the difference makes this clear.

```{r}
CC.sf <- CC.sf |>
  dplyr::mutate(CovMinusSmooth = trend - signal)

tmap::tm_shape(CC.sf) +
  tmap::tm_fill(col = "CovMinusSmooth")
```

How many tracts have a smaller residual with the lag model versus the OLS model?

```{r}
CC.sf |>
  dplyr::mutate(residualsL = CRIME - fit,
                lagWins = abs(residuals(model.ols)) > abs(residualsL),
                CovMinusSmooth = trend - signal) |>
  sf::st_drop_geometry() |>
  dplyr::summarize(N = sum(lagWins))
```

In 32 out of the 49 tracts the residuals from the spatial model are smaller than the residuals from the OLS model.

## Fitting and interpreting a spatially-lagged X model {-}

Another spatial regression option is to modify the linear model to include spatially-lagged explanatory variables. This is called the spatially-lagged X model. $$
y = X \beta + WX \theta + \varepsilon
$$

In this case the weights matrix is (post) multiplied by the matrix of X variables where $W$ is again the weights matrix and $\theta$ is a vector of coefficients for each lagged explanatory variable.

Here you fit the spatially-lagged X model using the `spatialreg::lmSLX()` function and save the model object as `model.slxm`.

```{r}
( model.slxm <- spatialreg::lmSLX(formula = f, 
                                  data = CC.sf, 
                                  listw = wts) )
```

With this model, beside the direct marginal effects of income and housing value on crime, you also have the spatially-lagged indirect effects.

The total effect of income on crime is the sum of the direct effect and indirect effect. And again, using the `spatialreg::impacts()` function you see this.

```{r}
spatialreg::impacts(model.slxm, listw = wts)
```

You get the impact measures and their standard errors, z-values and $p$-values with the `summary()` method applied to the output of the `impacts()` function.

```{r}
summary(spatialreg::impacts(model.slxm, listw = wts))
```

Results show that income has a significant direct *and* indirect effect on crime rates, but housing values only show a significant direct effect and not a significant indirect effect.

Again you visualize the relative significance of the effects.
```{r}
model.slxm |>
  broom::tidy(conf.int = TRUE) |>
  dplyr::slice(-1) |>
ggplot(aes(x = estimate,
                   y = term, 
                   xmin = conf.low, 
                   xmax = conf.high, 
                   height = 0)) +
  geom_point(size = 2) +
  geom_vline(xintercept = 0, lty = 4) +
  geom_errorbarh()
```

Compare R squared values between the OLS model and the spatially-lagged X model.

```{r}
summary(model.ols)$r.squared
summary(model.slxm)$r.squared
```

The spatially lagged model has an R squared value that is higher than the R squared value from the linear regression.

## Fitting and interpreting spatial Durbin models {-}

A workflow for finding the correct spatial model is to consider both the spatial Durbin error model and the spatial Durbin model.

The spatial Durban error model (SDEM) is a spatial error model with a spatially-lagged X term added. To fit a SDEM use the `spatialreg::errorsarlm()` function but include the argument `etype = "emixed"` to ensure that the spatially lagged X variables are added and the lagged intercept term is dropped when the weights style is row standardized (`"W"`).

```{r}
( model.sdem <- spatialreg::errorsarlm(formula = f, 
                                       data = CC.sf, 
                                       listw = wts,
                                       etype = "emixed") )
```

The spatial Durban model (SDM) is a spatially-lagged Y model with a spatially-lagged X term added to it. To fit a SDM use the `lagsarlm()` function but include the argument `type = "mixed"` to ensure that the spatially lagged X variables are added and the lagged intercept term is dropped when the weights style is row standardized (`"W"`).

```{r}
( model.sdm <- spatialreg::lagsarlm(formula = f, 
                                    data = CC.sf, 
                                    listw = wts,
                                    type = "mixed") )
```

How to do you choose between these two models? Is the relationship between crime and income and housing values a global or local effect? Is there any reason to think that if something happens in one tract it will spillover across the entire city? If crime happens in one tract does it influence crime across the entire city? If so, then it is a global relationship. Or should it be a more local effect? If there is more crime in one tract then maybe that influences crime in the neighboring tract but not tracts farther away. If so, then it is a local relationship.

If you think it is a local relationship, start with the spatial Durbin error model and look at the $p$-values on the direct and indirect effects.

```{r}
summary(spatialreg::impacts(model.sdem, 
                            listw = wts, 
                            R = 500), zstats = TRUE)
```

You see that income has a statistically significant direct and indirect effect on crime. This means that tracts with higher income have lower crime and tracts whose *neighboring tracts* have higher income also have lower crime.

On the other hand, housing values have only a statistically significant direct effect on crime. Tracts with more expensive houses have lower crime but tracts whose neighboring tracts have more expensive houses do not imply lower crime. And the total effect of housing values on crime across the city is not significant. So if housing values go up in tracts citywide, there is no statistical evidence that crime will go down (or up).

Try a likelihood ratio test with the null hypothesis being that you should restrict the model.

```{r}
spatialreg::LR.Sarlm(model.sdem, 
                     model.slxm)
```

The relatively small $p$-value suggests you shouldn't restrict the spatial Durbin model to just the spatially-lagged X model although the evidence is not overwhelming.

More information

-   <https://youtu.be/b3HtV2Mhmvk>
-   <https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2420725>

<!--chapter:end:12-Lesson.Rmd-->

# Tuesday October 11, 2022 {.unnumbered}

**"We build our computer systems the way we build or cities; over time, without plan, on top of ruins."** – Ellen Ullman

Today

- Fitting and interpreting geographic regressions
- Mapping incidence and risk with spatial regression models

## Fitting and interpreting geographic regressions {-}

Another approach to modeling spatial data is to assume that the _relationships_ between the response variable and the explanatory variables are modified by contextual factors that depend on location. In this case you fit a separate regression model at each geographic location. 

The analogy is a local measure of spatial autocorrelation where you estimate the statistic at each location. It is a useful approach for exploratory analysis (e.g., to show where the explanatory variables are most strongly related to the response variable). It is called geographically weighted regression (GWR) or simply geographic regression. GWR is used in epidemiology, particularly for research on infectious diseases and for evaluating health policies and programs.

Since GWR fits a separate regression model using data focused at every spatial location in the dataset, it is not a single model but a procedure for fitting a set of models. This is different from the spatial regression such as the spatially-lagged Y model, which are single models with spatial terms.

Observations across the entire domain contribute to the model fit at a particular location, but the observations are weighted inversely by their distance to the particular location. At short distances, observations are given the largest weights based on a Gaussian function and a bandwidth. The bandwidth is specified as a single parameter or it is determined through a cross-validation procedure. The bandwidth can also be a function of location.

Said another way, linear regression is a model for the conditional mean. The mean of the response variable depends on the explanatory variables. Geographic regressions show how this dependency varies by location. GWR is used as an exploratory technique for determining where local regression coefficients are different from corresponding global values.

Continuing with the Columbus crime data.

```{r}
( CC.sf <- sf::st_read(dsn = here::here("data", "columbus"),
                       layer = "columbus") )
```

Start by fitting a 'global' ordinarly least squares (OLS) linear regression to the crime rates using income and housing values, exactly as you did earlier.

```{r}
f <- CRIME ~ INC + HOVAL
( model.ols <- lm(formula = f,
                  data = CC.sf) )
```

The coefficients on the two explanatory variables indicate that crime decreases in areas of higher income and higher housing values.

You compare this result to results from geographic regressions. The functions are in the {spgwr} package.

```{r}
if(!require(spgwr)) install.packages(pkgs = "spgwr", repos = "http://cran.us.r-project.org")
```

The `sp` part of the package name indicates that the functions were developed to work with S4 spatial objects. 

The functions allow you to use S3 simple features by specifying the locations as a matrix. Here you extract the centroid from each census tract as a matrix.

```{r}
Locations <- sf::st_coordinates(sf::st_centroid(CC.sf))

head(Locations)
```

These are the X and Y coordinate values specifying the centroid for the first six tracts (out of 49).

To determine the optimal bandwidth for the Gaussian kernel (weighting function) you use the `spgwr::gwr.sel()` function. You need to specify the arguments, model formula (`formula =`), the data frame (`data =`), and the coordinates (`coords =`) as part of the function call. The argument `coords =` is the matrix of coordinates of points representing the spatial locations of the observations. It can be omitted if the data is an S4 spatial data frame from the {sp} package.

```{r}
( bw <- spgwr::gwr.sel(formula = f, 
                       data = CC.sf,
                       coords = Locations) )
```

The procedure makes an initial guess at the optimal bandwidth distance and then fits local regression models at each location using weights that decay defined by the kernel (guassian by default) and that bandwidth (distance).

The output shows that the first bandwidth chosen was 2.22 in arbitrary distance units. The resulting prediction skill from fitting 49 regression models with that bandwidth is 7474 units. The resulting CV score is based on cross validation whereby skill is computed at each location when data from that location is not used to fit the regression models.

The procedure continues by increasing the bandwidth distance (to 3.59) and then computing a new CV score from refitting the regression models. Since the new CV score is higher (7480) than the initial CV score (7474), the bandwidth is changed in the other direction (decreasing from 2.22 to 1.37) and the models again are refit. With that bandwidth, the CV score is 7404, which is lower than the initial bandwidth so the bandwidth is decreased again. The procedure continues until no additional improvement in prediction skill occurs. 

The output shows that no additional improvement in skill occurs at a bandwidth distance of .404 units, and this single value is assigned to the object you called `bw`.

Once the bandwidth distance is determined you use the `spgwr::gwr()` function to fit the regressions using that bandwidth. The arguments are the same as before but includes the `bandwidth =` argument where you specify the object `bw`.

```{r}
model.gwr <- gwr(formula = f, 
                 data = CC.sf, 
                 coords = Locations,
                 bandwidth = bw)
```

The model and observed data are assigned to a list object with element names listed using the `names()` function.

```{r}
names(model.gwr)
```

The first element is `SDF` containing the model output as a S4 spatial data frame.

```{r}
class(model.gwr$SDF)
```

See Lesson 7 where S4 spatial data objects were covered.

The structure of the spatial data frame is obtained with the `str()` function and by setting the `max.level` argument to 2. 

```{r}
str(model.gwr$SDF, max.level = 2)
```

Here there are five slots with the first slot labeled `@data` indicating that it is a data frame. The number of rows and columns in the data frame are listed with the `dim()` function.

```{r}
dim(model.gwr$SDF)
```

There are 49 rows and 7 columns. Each row corresponds to a tract and information about the regressions localized to the tract is given in the columns. Column names are listed with the `names()` function.

```{r}
names(model.gwr$SDF)
```

They include the sum of the weights `sum.w` (the larger the sum the more often the tract is included in the local regressions--favoring smaller counties and ones farther from the borders of the spatial domain), the three regression coefficients one for each of the  explanatory variables (`INC` and `HOVAL`) and an intercept term, the residual (`gwr.e`), the predicted value (`pred`) and the local goodness-of-fit (`localR2`).

You create a map displaying where income has the most and least influence on crime by first adding the income coefficient from the data frame (column labeled `INC`) to the simple feature data frame since the order of the rows in the `SDF` matches the order in the simple feature data frame and then using functions from the {ggplot2} package.
```{r}
CC.sf$INCcoef <- model.gwr$SDF$INC

library(ggplot2)

ggplot(CC.sf) +
  geom_sf(aes(fill = INCcoef)) +
  scale_fill_viridis_c()
```

Most tracts have coefficients with values less than zero. Recall the global coefficient is less than zero. But areas in yellow show where the coefficient values are greater than zero indicating a direct relationship between crime and income.

How about the coefficients on housing values?

```{r}
CC.sf$HOVALcoef <- model.gwr$SDF$HOVAL

ggplot(CC.sf) +
  geom_sf(aes(fill = HOVALcoef)) +
  scale_fill_viridis_c()
```

While the global coefficient is negative indicating crime rates tend to be lower in areas with higher housing values, the opposite is the case over much of city especially on the south side.

You put the vector of GWR predictions into the `CC.sf` simple feature data frame giving it the column name `predGWR` and then map the predictions using functions from the {tmap} package.

```{r}
CC.sf$predGWR <- model.gwr$SDF$pred

tmap::tm_shape(CC.sf) +
  tmap::tm_fill("predGWR", title = "Predicted crimes\nper 1000") +
  tmap::tm_layout(legend.outside = TRUE)
```

The geographic regressions capture the spatial pattern of crimes across the city. The spread of predicted values matches the observed spread better than the linear model. The pattern of predicted crime is also a smoother than with a global OLS regression.

Where is the relationship between crime and the two explanatory variables the tightest? This is answered by mapping the R squared coefficient for each of the models.

```{r}
CC.sf$localR2 <- model.gwr$SDF$localR2

ggplot(CC.sf) +
  geom_sf(aes(fill = localR2)) +
  scale_fill_viridis_c()
```

Although crime rates are highest in the center, the relationship between crime and income and housing values is largest in tracts across the eastern part of the city.

This type of nuanced exploratory analysis is made possible with GWR.

Also, when fitting a regression model to data that vary spatially you are assuming an underlying stationary process. This means you believe the explanatory variables 'provoke' the same response (statistically) across the domain. If this is not the case then it shows up in a map of correlated residuals. One way to check the assumption of a stationary process is to use geographic regression.

## Mapping incidence and risk with spatial regression models {-}

Spatial regression models are used in disease mapping where it is common to use a standardized incidence ratio (SIR) defined as the ratio of the observed to the _expected_ number of disease cases. Small areas can give extreme SIRs due to low population sizes or small samples. Extreme values of SIRs can be misleading and unreliable for reporting.

Because of this so-called 'small area problem' it is better to estimate disease risk using a spatial regression model. Spatial regression models incorporate information from neighboring areas and explanatory information. The result is a smoothing (shrinking) of extreme values.

Consider county-level lung cancer cases in Pennsylvania from the {SpatialEpi} package. The county boundaries for the state are in the list object `pennLC` with element name `spatial.polygon`. Change the native spatial polygons S4 object to an S3 simple feature data frame using the `sf::st_as_sf()` function and display a map of the county borders.

```{r}
if(!require(SpatialEpi)) install.packages("SpatialEpi", repos = "http://cran.us.r-project.org")

LC.sp <- SpatialEpi::pennLC$spatial.polygon
LC.sf <- sf::st_as_sf(LC.sp)

ggplot(LC.sf) +
  geom_sf()
```

For each region (county) $i$, $i = 1, \ldots, n$ the SIR is defined as the ratio of observed counts ($Y_i$) to the expected counts ($E_i$).

$$
\hbox{SIR}_i = Y_i/E_i.
$$

The expected count $E_i$ is the total number of cases expected if the population of area $i$ behaves the way the standard population behaves. If you ignore differences in rates for different stratum (e.g., age groups, race, etc) then you compute the expected counts as

$$
E_i = r^{(s)} n^{(i)},
$$
where $r^{(s)}$ is the rate in the standard population (total number of cases divided by the total population across all regions), and $n^{(i)}$ is the population of region $i$.

Then $\hbox{SIR}_i$ indicates whether region $i$ has higher ($\hbox{SIR}_i > 1$), equal ($\hbox{SIR}_i = 1$) or lower ($\hbox{SIR}_i < 1$) risk than expected relative to the standard population.

When applied to mortality data, the ratio is known as the standardized mortality ratio (SMR).

The data frame `SpatialEpi::pennLC$data` contains the number of lung cancer cases and the population of Pennsylvania at county level, stratified on race (white and non-white), gender (female and male) and age (under 40, 40-59, 60-69 and 70+). 

You compute the number of cases for all the strata together in each county by aggregating the rows of the data frame by county and adding up the number of cases.
```{r}
( County.df <- SpatialEpi::pennLC$data |>
  dplyr::group_by(county) |>
  dplyr::summarize(Y = sum(cases)) )
```

You then calculate the expected number of cases in each county using indirect standardization. The expected counts in each county represent the total number of disease cases one would expect if the population in the county behaved the way the population of Pennsylvania behaves. You do this by using the `SpatialEpi::expected()` function. The function has three arguments including `population` (vector of population counts for each strata in each area), `cases` (vector with the number of cases for each strata in each area), and `n.strata` (number of strata).

The vectors `population` and `cases` need to be sorted by area first and then, within each area, the counts for all strata need to be listed in the same order. All strata need to be included in the vectors, including strata with 0 cases. Here you use the `dplyr::arrange()` function.

```{r}
Strata.df <- SpatialEpi::pennLC$data |>
  dplyr::arrange(county, race, gender, age)
head(Strata.df)
```

Then you get the expected counts (E) in each county by calling the `SpatialEpi::expected()` function, where you set population equal to `Strata.df$population` and cases equal to `Strata.df$cases`. There are two races, two genders and four age groups for each county, so number of strata is set to 2 x 2 x 4 = 16.

```{r}
( E <- SpatialEpi::expected(population = Strata.df$population,
                            cases = Strata.df$cases, 
                            n.strata = 16) )
```

Now you add the observed count `Y`, the expected count `E` the computed SIR to the simple feature data frame `LC.sf` and make a map of the standardized incidence ratios (SIR) with blue shades below a value of 1 (midpoint) and red shades above a value of 1.
```{r}
LC.sf <- LC.sf |>
  dplyr::mutate(Y = County.df$Y,
                E = E,
                SIR = Y/E)

ggplot(LC.sf) + 
  geom_sf(aes(fill = SIR)) +
  scale_fill_gradient2(midpoint = 1, 
                       low = "blue", 
                       mid = "white", 
                       high = "red") +
  theme_minimal()
```

In counties with SIR = 1 (white) the number of cancer cases observed is the same as the number of expected cases. In counties with SIR > 1 (red), the number of cancer cases observed is higher than the expected cases. Counties with SIR < 1 (blue) have fewer cancer cases observed than expected.

In regions with few people the expected counts may be very low and the SIR value may be misleading. Therefore, it is preferred to estimate disease risk using models that borrow information from neighboring areas, and incorporate explanatory information. This results in smoothing (shrinkage) of extreme values.

Let the observed counts $Y$ be modeled with a Poisson distribution having a mean $E \theta$, where $E$ are the expected counts and $\theta$ are the relative risks. The logarithm of the relative risk is expressed as the sum of an intercept that models the overall disease risk level, and random effects to account for local variability.

The relative risk quantifies whether an area has a higher ($\theta > 1$) or lower ($\theta < 1$) risk than the average risk in the population. For example if $\theta_i = 2$, then the risk in area $i$ is twice the average risk in the population.

The model is expressed as

$$
Y \sim \hbox{Poisson}(E\theta) \\
\log(\theta) = \alpha + u + v
$$

The parameter $\alpha$ is the overall risk in the region of study, $u$ is the spatially structured random effect representing the dependency in risk across neighboring areas, and $v$ is the uncorrelated random noise modeled as $v \sim N(0, \sigma_v^2)$.

It is common to include explanatory variables to quantify risk factors (e.g., distance to nearest coal plant). Thus the log($\theta$) is expressed as

$$
\log(\theta) = \alpha + X\beta + u + v
$$

where $X$ are the explanatory variables and $\beta$ are the associated coefficients. A coefficient is interpreted such that a one-unit increase in the explanatory variable value changes the relative risk by a factor $\exp(\beta)$, holding the other variables constant.

A popular form for the combined spatially structured random effect and the uncorrelated random effect is the Besag-York-Mollié (BYM) model, which assigns a conditional autoregression distribution to $u$ as

$$
u | u_{j \ne i} \sim N(\bar u_{\delta}, \frac{\sigma_u^2}{n_{\delta}})
$$

where $\bar  u_{\delta_i} = \Sigma_{j \in \delta_i} u_j/n_{\delta_i}$ and where $\delta_i$ is the set of neighbors of area $i$ and $n_{\delta_i}$ is the number of neighbors of area $i$.

In words, the logarithm of the disease incidence rate in area $i$ conditional on the incidence rates in the neighborhood of $i$ is modeled with a normal distribution centered on the neighborhood average ($\bar  u_{\delta_i}$) with a variance scaled by the number of neighbors. This is called the conditional autoregressive (CAR) distribution.

The model is fit using an application of Bayes rule through the method of integrated nested Laplace approximation (INLA), which results in posterior densities for the predicted relative risk. This is done with functions in the {INLA} package. You get the package (it is not on CRAN) as follows.

```{r eval=FALSE}
options(timeout = 120)

install.packages("INLA", repos=c(getOption("repos"), INLA = "https://inla.r-inla-download.org/R/stable"), dep = TRUE)
```

The syntax for the BYM model using functions from the {INLA} package is given as
```{r}
f <- Y ~ 
  f(IDu, model = "besag", graph = g, scale.model = TRUE) +
  f(IDv, model = "iid")
```

The formula includes the response in the left-hand side, and the fixed and random effects on the right-hand side. By default, the formula includes an intercept. 

The random effects are set using `f()` with parameters equal to the name of the index variable, the model, and other options. The BYM formula includes a spatially structured random effect with index variable with name `IDu` and equal to c(1, 2, ..., I), where I is the number of regions (here the number of counties) and model `"besag"` with a CAR distribution and with neighborhood structure given by the graph `g`. The option `scale.model = TRUE` is used to make the precision parameter of models with different CAR priors comparable. 

The formula also includes an uncorrelated random effect with index variable with name `IDv` again equal to c(1, 2, ..., I), and model "iid". This is an independent and identically distributed zero-mean normally distributed random effect. Note that both the `ID` variables are identical but need to be specified as two different objects since INLA does not allow to include two effects with `f()` that use the same index variable.

The BYM model can also be specified with the model "bym" which defines both the spatially structured random effect and the uncorrelated random effect ($u$ and $v$).

You include these two vectors (call them `idu` and `idv`) in the data frame. 
```{r}
LC.sf <- LC.sf |>
  dplyr::mutate(IDu = 1:nrow(LC.sf),
                IDv = 1:nrow(LC.sf))
LC.sf
```

Create a graph object from a neighbor list object. Write the neighbor list object to a file then read it back in with the `inla.read.graph()` function.

```{r}
nb <- spdep::poly2nb(LC.sf)
spdep::nb2INLA(file = here::here("data", "map.adj"), nb)
g <- INLA::inla.read.graph(filename = here::here("data", "map.adj"))
class(g)
```

You fit the model by calling the `inla()` function specifying the formula, the family ("poisson"), the data, and the expected counts (E). You also set `control.predictor = list(compute = TRUE)` to compute the posteriors predictions.

```{r}
model.inla <- INLA::inla(formula = f, 
                         family = "poisson",
                         data = LC.sf,
                         E = E,
                         control.predictor = list(compute = TRUE))
```

The estimates of the relative risk of lung cancer and their uncertainty for each of the counties are given by the mean posterior and the 95% credible intervals which are contained in the object `model.inla$summary.fitted.values`. Column `mean` is the mean posterior and `0.025quant` and `0.975quant` are the 2.5 and 97.5 percentiles, respectively.

You add these to the spatial data frame and then make a map of the posterior mean relative risk.

```{r}
LC.sf$RR <- model.inla$summary.fitted.values[, "mean"]
LC.sf$LL <- model.inla$summary.fitted.values[, "0.025quant"]
LC.sf$UL <- model.inla$summary.fitted.values[, "0.975quant"]

ggplot(LC.sf) + 
  geom_sf(aes(fill = RR)) +
  scale_fill_gradient2(midpoint = 1, 
                       low = "blue", 
                       mid = "white", 
                       high = "red") +
  theme_minimal()
```

These relative risk values are smoother and muted in absolute magnitude compared with the empirical SIR estimates. 

More on this topic is available from

- https://www.paulamoraga.com/book-geospatial/index.html
- https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0166895

The second source is a paper we published addressing long and short term views of tornado risk across the eastern half of the United States.

<!--chapter:end:13-Lesson.Rmd-->

# Tuesday October 18, 2022 {.unnumbered}

**"Give someone a program, you frustrate them for a day; teach them how to program, you frustrate them for a lifetime."** - David Leinweber

Today

- Spatial data as point patterns
- Working with point pattern objects using functions from the {spatstat} package
- Quantifying event intensity

## Spatial data as point patterns {-}

We now turn our attention to analyzing and modeling point pattern data. Starting with some theory, then how to work with functions from the {spatstat} package before focusing on spatial intensity.

We naturally seek to find patterns in a collection of events. The pattern that tends to catch our attention quickly is the grouping of events across space. Stars in the night sky as constellations.  A collection of events in a particular region begs for an explanation. Why do events occur more often in this particular region and not somewhere else?

Consider tornado reports over the past several years in the state of Kansas. Let the start position of a tornado be an _event location_. And let the damage rating (EF scale) provide a _mark_ on the event. Here you consider only events since 2007 with marks of 1, 2, 3, 4, and 5. More about the damage rating scale is available here https://en.wikipedia.org/wiki/Enhanced_Fujita_scale

Import and filter the data accordingly.

```{r}
Torn.sf <- sf::st_read(dsn = here::here("data", "1950-2020-torn-initpoint")) |>
  dplyr::filter(st == "KS", 
                yr >= 2007,
                mag > 0) 
```

Create a map of the event locations using functions from the {tmap} package. The state border is obtained as a simple feature data frame. The polygon geometry is plotted first with `tm_borders()` then the event locations are plotted with the `tm_bubbles()` and `size = "mag"`.

```{r}
KS.sf <- USAboundaries::us_states(states = "Kansas")

tmap::tm_shape(KS.sf) +
   tmap::tm_borders(col = "grey70") +
tmap::tm_shape(Torn.sf) +
   tmap::tm_bubbles(size = "mag", 
                    col = "red",
                    alpha = .4,
                    title.size = "EF Rating") +
tmap::tm_layout(legend.position = c("left", "top"),
                legend.outside = TRUE)
```

Based on this display of tornado genesis locations we ask: (1) Are certain areas of the state more (or less) likely to get a tornado? (2) Do tornadoes tend to cluster? (3) Are there places in the state that are safe from tornadoes?

These questions are similar but they are not identical. We will explore these canonical questions about point pattern data in the next few lessons.

To begin it is helpful to have some definitions. 

* Event: An occurrence of interest (e.g., tornado, accident, wildfire). 
* Event location: Location of event (e.g., latitude/longitude).
* Point: Any location in the study area where an event _could_ occur. Note: Event location is a particular point where an event _did_ occur. In a forest with a lake, the lake is a place where an event could not occur.
* Point pattern data: A collection of observed (or simulated) event locations together with a domain of interest.
* Domain: Study area that is often defined by data availability (e.g., state or county boundary) or by the extent of the events.
* Complete spatial randomness: Or CSR (not to be confused with CRS--coordinate reference system) defines the situation where an event has an equal chance of occurring at any point in the domain regardless of other nearby events. In this case we say they event locations have a uniform probability distribution (uniformly distributed) across space. Note: uniform chance does not mean that the events have an ordered pattern (e.g., trees in an orchard).

Consider a set of event locations that are randomly distributed within the unit plane. First create two vectors containing the x and y coordinates, then create a data frame that includes the name of the sample, and finally graph the locations using `ggplot()`.

```{r} 
library(ggplot2)

x <- runif(n = 50, min = 0, max = 1)
y <- runif(n = 50, min = 0, max = 1)
df1 <- data.frame(x, y, name = "Point Pattern 1")
ggplot(data = df1, 
       mapping = aes(x, y)) +
  geom_point(size = 2)
```

The plot shows a sample from a spatial point pattern process. A _spatial point process_ is a mechanism for producing a set of event locations across space. The pattern of locations produced by the point process is described as CSR. There are groups of event locations and some gaps. 

Let's repeat this process to create three additional samples. First you combine them into a single data frame with the `rbind()` function and then plot a four-panel figure using the `facet_wrap()` function.

```{r}
df2 <- data.frame(x = runif(n = 30, min = 0, max = 1),
                  y = runif(n = 30, min = 0, max = 1),
                  name = "Point Pattern 2")
df3 <- data.frame(x = runif(n = 30, min = 0, max = 1),
                  y = runif(n = 30, min = 0, max = 1),
                  name = "Point Pattern 3")
df4 <- data.frame(x = runif(n = 30, min = 0, max = 1),
                  y = runif(n = 30, min = 0, max = 1),
                  name = "Point Pattern 4")
df <- rbind(df1, df2, df3, df4)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_point() +
  facet_wrap(~ name)
```

Groups of nearby events illustrate that a certain degree of _clustering_ occurs by chance (without cause) making visual assessment of causal clustering difficult.

Complete spatial randomness sits on a spectrum between regularity and clustered. To illustrate this idea here you generate point pattern data that have more regularity than CSR and point pattern data that are more clustered than CSR. You do this using the `rMaternI()` and `rMaternClust()` functions from the {spatstat} package.
```{r}
m1 <- spatstat.random::rMaternI(kappa = 100, r = .02)
df1 <- data.frame(x = m1$x, y = m1$y, name = "Regular Pattern 1")
m2 <- spatstat.random::rMaternI(kappa = 100, r = .02)
df2 <- data.frame(x = m2$x, y = m2$y, name = "Regular Pattern 2")
m3 <- spatstat.random::rMatClust(kappa = 30, r = .15, mu = 4)
df3 <- data.frame(x = m3$x, y = m3$y, name = "Cluster Pattern 1")
m4 <- spatstat.random::rMatClust(kappa = 30, r = .15, mu = 4)
df4 <- data.frame(x = m4$x, y = m4$y, name = "Cluster Pattern 2")
df <- rbind(df1, df2, df3, df4)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_point() +
  facet_wrap(~ name)
```

The difference in the arrangement of event locations between a regular and a cluster process is clear. But the difference in the arrangement of event locations between a CSR and regular process and the difference in the arrangement of event locations between a CSR and cluster process is not.

And spatial scale matters. A set of event locations can be regular on a small scale but clustered on a larger scale.

Probability models for spatial patterns motivate methods for detecting event clustering. A probability model generates a point pattern process. For example, we can think of crime as a point pattern process defined by location and influenced by environmental factors. The probability of a crime occurring at a particular location is the random variable and we can estimate the probability of a crime event at any location given factors that influence crime. 

More formally, a spatial point pattern process is a _stochastic_ (statistical) process where event location is the random variable. A sample of the process is a collection of events generated under the probability model.

A spatial point process is said to be _stationary_ if the statistical properties of the events are invariant to translation. This means that the relationship between two events depends only on the relative event locations (not on where the events occur in the domain). Relative location (or spatial lag) refers to distance and orientation of the events relative to one another. 

In the case where the statistical properties are independent of the orientation of event pairs, the process is said to be _isotropic_. 

The properties of stationarity and isotropy allow for replication within a data set. Under the assumption of a stationary process, two event pairs that are separated by the same distance should have the same relatedness. This is analogous to the assumption we make when we define our weights matrix for spatially aggregated data. The assumptions of stationarity and isotropy are starting points for modeling point pattern data. 

The Poisson distribution defines a model for complete spatial randomness (CSR). A point process is said to be 'homogeneous Poisson' under the following two criteria: 

1. The number of events, N, occurring within a finite domain A is a random variable described by a Poisson distribution with mean $\lambda$|A| for some positive constant $\lambda$, with |A| denoting the area of the domain, and 
2. The locations of the N events represent a random sample where each point in A is _equally likely_ to be chosen as an event location.

The first criteria of a Poisson distribution refers to a probability model describing the number of events. It expresses the probability of a given number of events occurring in a fixed interval of space when the events occur with a known constant rate.

The Poisson parameter defines the _intensity_ of the point process. Given a set of events, an estimate for the mean (rate) parameter of the Poisson distribution is given by the number of events divided by the domain area. 

The second criteria ensures the events are scattered about the domain without clustering or regularity.

The procedure to create a homogeneous Poisson point process follows directly from its definition. Step 1: Sample the total number of events from a Poisson distribution with a mean that is proportional to the domain area. Step 2: Place each event within the domain with coordinates given by a _uniform distribution_.

For example, let area |A| = 1, and the rate of occurrence $\lambda$ = 20, then
```{r}
lambda <- 20
N <- rpois(1, lambda)
x <- runif(N)
y <- runif(N)
df <- data.frame(x, y)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_point(size = 2) 
```

The set of events represents a sample from a homogeneous Poisson point process. The intensity of the events is specified first then the locations are placed uniformly inside the domain. The domain need not be regular. The actual number of events varies from one realization to the next. On average the number of events is 20 but it could vary between 10 and 35 or more.

This point pattern is CSR by construction. However, you are typically in the opposite position. You observe a set of events and you want to know if the events are regular or clustered. The null hypothesis is CSR and you need a test statistic that will summarize the evidence against this hypothesis. The null models are simple so you can use Monte Carlo methods to generate many samples and compare summary statistics from those samples with your observed data.

In some cases the homogeneous Poisson model is not restrictive enough. This means that you can easily reject the null hypothesis but not learn anything interesting about your data. For example, with health events (locations of people with heart disease) CSR is not an appropriate model because a null hypothesis that incidences are equally likely does not consider that people cluster (locations at risk are not uniform).

Each person has the same risk of heart disease regardless of location, and you expect more cases in areas with more people at risk. Clusters of cases in high population areas violate the CSR but not necessarily the constant risk hypothesis. The constant risk hypothesis requires the intensity of the spatial process be defined as a spatially varying function. That is, you define the intensity as $\lambda(s)$, where $s$ denotes location.

The intensity (density) function is a first-order property of the random process. If intensity varies (significantly) across the domain the data-generating process is said to be heterogeneous. The intensity function describes the expected number of events at any location. Events might be independent of one another, but groups of events appear because of the changing intensity.

## Working with point pattern objects using functions from the {spatstat} package {-}

The {spatstat} package contains many functions to analyze and model point pattern data. Point pattern data are defined in {spatstat} by an object of class `ppp` (for planar point pattern) which contains the coordinates of the events (event locations), optional values attached to the events (called 'marks'), and a description of the domain or 'window' over which the events are observed. See `?ppp.object()` for details.

Spatial statistics computed on a `ppp` object will be somewhat sensitive to the choice of the window (domain), so some thought should go into deciding what window should be used.

As an example, the data `swedishpines` is available in the package as a `ppp` object.
```{r}
suppressMessages(library(spatstat))

class(swedishpines)
swedishpines
```

The data is a planar point pattern object with 71 events. 

Note: The events in a `ppp` object are called 'points' rather than events. This is in contrast to the theory that defines a point as represented by a _potential_ event not an _observed_ event.

All the events are contained within a rectangle window of size 9.6 by 10 meters.

There is a `plot()` method for `ppp` objects that provides a quick view the data and the domain window.

```{r}
plot(swedishpines)
```

Events are plotted as open circles inside a box. The plot is labeled with the name of the `ppp` object.

The function `convexhull()` from the {spatstat} package creates a convex hull around the events. A convex hull defines the minimum-area convex polygon that contains all the events. 

Here you compute the hull and add it to the plot.
```{r}
plot(swedishpines)
plot(convexhull(swedishpines), 
     add = TRUE)
```

The domain (window) for analysis and modeling should be somewhat larger than the convex hull. The function `ripras()` computes a spatial domain based on the event locations alone assuming the locations are independent and identically distributed.

Here you also overlay this polygon on the plot.
```{r}
plot(swedishpines)
plot(convexhull(swedishpines), 
     add = TRUE)
plot(ripras(swedishpines), 
     add = TRUE, lty = "dotted")
```

The window can have an arbitrary shape. A rectangle, a polygon, a collection of polygons including holes, or a binary image (mask). A window can be stored as a separate object of class `owin`. See `?owin.object()` for details.

Each event may carry information called a 'mark'. A mark can be continuous (e.g. tree height) or discrete (tree species).

A multitype point pattern is one in which the events are marked using a factor (e.g., tree species). The mark values are given in a vector of the same length as the vector of locations. That is, `marks[i]` is the mark attached to the location (`x[i]`, `y[i]`).

Consider the `ppp` object `demopat` from the {spatstat} package.

```{r}
plot(demopat)
marks(demopat)
```

Here the domain is defined as an irregular concave polygon with a hole. The distinction between inside and outside is important for spatial statistics computed using the events.

For a multitype pattern (where the marks are factors) you can use the `split()` function to separate the point pattern objects by mark type. Consider the Lansing Woods data set (`lansing`) with marks corresponding to tree species.

```{r}
data(lansing)
LW <- lansing

plot(split(LW))
```

## Quantifying event intensity {-}

The average intensity of events is defined as the number of events per unit area of the domain. The `summary()` method applied to a `ppp` object gives the average intensity.

```{r}
summary(swedishpines)
```

There are 71 events over a window area (spatial domain) of 9600 square units giving an average intensity of 71/9600 = .0074.

The average intensity might not represent the intensity of events locally. We need a way to describe the expected number of events at any location of the region. 

Counting the number of events in equal areas is one way. The quadrat method divides the domain into a grid of rectangular cells and the number of events in each cell is counted. Quadrat counting is done with the `quadratcount()` function.

```{r}
quadratcount(swedishpines)
```

By default the function divides the data into a 5 x 5 grid of cells. The event count in each cell is produced. To change the default number of cells in x and y directions you use the `nx =` and `ny =` arguments.

```{r}
quadratcount(swedishpines, 
             nx = 2, 
             ny = 3)
```

The plot method applied to the results of the `quadratcount()` functions adds the counts to a plot. Here you add the counts and include the event locations.

```{r}
plot(quadratcount(swedishpines))
plot(swedishpines, pty = 19, col = "red", 
     add = TRUE, main = "")
```

Grid cell areas will not be all equal when the domain boundaries are irregular like with the `demopat` ppp object.

```{r}
plot(quadratcount(demopat))
```

Areas near the borders are smaller than areas completely within the domain.

When the number of events is large, hexagon grid cells provide a useful alternative to rectangular grid cells. 

The process is: (1) tessellate the domain by a regular grid of hexagons, (2) count the number of events in each hexagon, and (3) use a color ramp to display the events per hexagon.

As an example here you generate 20K random values from the standard normal distribution for the x coordinate and the same number of random values for the y coordinate. You then use the `hexbin()` function from the {hexbin} package and specify 10 bins in the x direction to count the number of events in each hexagon and assign the result to the object `hbin`.
```{r}
if(!require(hexbin)) install.packages(pkgs = "hexbin", repos = "http://cran.us.r-project.org")

x <- rnorm(20000)
y <- rnorm(20000)
hbin <- hexbin::hexbin(x, y, xbins = 10) 
str(hbin)
```

The {hexbin} package uses S4 data classes so the output is stored in slots. Use the `plot()` method to make a graph.
```{r}
plot(hbin)
```

Hexagons have symmetric nearest neighbors (there is only rook contiguity). They have the most sides of any polygon that can tessellate the plane. They are generally more efficient than rectangles at covering the events. In other words it takes fewer of them to cover the same number of events. They are visually less biased for displaying local event intensity compared to squares/rectangles.

Here you generate a large number of random events in the two-dimensional plane. Use a normal distribution in the x-direction and a student t-distribution in the y-direction.
```{r}
set.seed(131)
x <- rnorm(7777)
y <- rt(7777, df = 3)
hbin2 <- hexbin::hexbin(x, y, xbins = 25)
plot(hbin2)
```

The {ggplot2} package has the `stat_binhex()` function so that also can be used for display.
```{r}
df <- data.frame(x, y)
ggplot(data = df, 
       mapping = aes(x, y)) +
  stat_binhex()
```

Another way to quantify the spatial intensity is with kernel density estimation (KDE). Let $s_i$ for $i$ = 1...$n$ be event locations, then an estimate for the intensity of the events at any location $s$ is given by

$$
\hat \lambda (s) = \frac{1}{nh}\sum_{i=1}^nK\Big(\frac{s - s_i}{h}\Big)
$$
where $K()$ is the kernel function and $h > 0$ is a smoothing parameter called the bandwidth. Typically the kernel function is a Gaussian probability density function.

To help visualize KDE you generate 25 event locations (`el`) uniformly on the real number line representing a one-dimensional spatial domain between 0 and 1 and then use kernel density estimation to get a continuous intensity function. The density estimation is is done using the function `density()` and here you compare the intensity function for increasing bandwidths specified with the `bw =` argument.
```{r}
el <- runif(25)
dd1 <- density(el, bw = .025)
dd2 <- density(el, bw = .05)
dd3 <- density(el, bw = .1)
df <- data.frame(x = c(dd1$x, dd2$x, dd3$x), 
                 y = c(dd1$y, dd2$y, dd3$y),
                bw = c(rep("h = .025", 512), 
                       rep("h = .05", 512),
                       rep("h = .1", 512)))
df2 <- data.frame(x = el, y = 0)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_line() +
  facet_wrap(~ bw, nrow = 3) +
  geom_point(mapping = aes(x, y), 
             data = df2, 
             color = "red")
```

As the bandwidth increases the curve (black line) representing the local intensity becomes smoother. The intensity is estimated at every location, not just at the location of the event. 

The density is a summation of the kernels with one kernel centered on top of each event location. Event locations are marked with a point along the x-axis and the kernel is a Gaussian probability density function. The kernel is placed on each event and the bandwidth specifies the distance between the inflection points of the kernel. The one-dimensional KDE extends to two (or more) dimensions.

Example: The distribution of trees in a tropical forest

The object `bei` is a planar point pattern object from the {spatstat} package containing the locations of trees in a tropical rain forest.

```{r}
summary(bei)
```

There are 3604 events (trees) over an area of 500,000 square meters giving an average intensity of .0073 trees per unit area.

The distribution of trees is not uniform (heterogeneous) as can be seen with a plot
```{r}
plot(bei)
```

The plot shows clusters of trees and large areas with few if any trees.

Elevation and elevation slope are factors associated with tree occurrence.

The point pattern data is accompanied by data (`bei.extra`) on elevation (`elev`) and slope of elevation (`grad`) across the region.

```{r}
plot(bei.extra)
```

These data are stored as `im` (image) objects. 
```{r}
class(bei.extra$elev)
```

The image object contains a list with 10 elements including the matrix of values (`v`).
```{r}
str(bei.extra$elev)
```

Specifying a spatial domain (window) allows focuses the analysis on a particular region. Suppose you want to model locations of a certain tree type, but only for trees located at elevations above 145 meters. The `levelset()` function creates a window from an image object using `thresh =` and `compare =` arguments.
```{r}
W <- levelset(bei.extra$elev, 
              thresh = 145, 
              compare = ">")
class(W)
```

The result is an object of class `owin`. The plot method displays the window as a mask, which is the region in black.
```{r}
plot(W)
```

You subset the `ppp` object by the window using the bracket operator (`[]`). Here you assign the reduced `ppp` object to `beiW` and then make a plot.
```{r}
beiW <- bei[W]
plot(beiW)
```

Now the analysis window is white and the event locations are plotted on top.

As another example you create a window where altitude is lower than 145 m and slope exceeds .1 degrees. In this case you use the `solutionset()` function. 
```{r}
V <- solutionset(bei.extra$elev <= 145 & 
                 bei.extra$grad > .1)
beiV <- bei[V]
plot(beiV)
```

You compute the spatial intensity function over the domain with the `density()` method using the default Gaussian kernel and fixed bandwidth determined by the window size.
```{r}
beiV |>
  density() |>
  plot()
```

The units of intensity are events per unit area (here square meters). The intensity values are computed on a grid ($v$) and are returned as a pixel image.

There are over 16K of the cells that have a value of `NA` as a result of the masking by elevation and slope.
```{r}
den <- beiV |>
  density()

sum(is.na(den$v))
```

<!--chapter:end:14-Lesson.Rmd-->

# Thursday October 20, 2022 {.unnumbered}

**"So much complexity in software comes from trying to make one thing do two things."** - Ryan Singer

Today

-   Creating `ppp` and `owin` objects from simple feature data frames
-   Estimating spatial intensity as a function of distance
-   Intensity trend as a possible confounding factor

Last time the terminology of point pattern data including the concept of complete spatial randomness (CSR) was introduced. Focus is typically on natural occurring systems where the spatial location of events is examined through the lens of statistics in an attempt to understand physical processes.

The {spatstat} package is a comprehensive set of functions for analyzing, plotting, and modeling point pattern data. The package requires the data be of spatial class `ppp`.

The typical work flow includes importing and munging data as a simple feature data frame and then converting the simple feature data frame to a `ppp` object for analysis and modeling. But it is sometimes convenient to do some of the data munging after conversion to a `ppp` object.

## Creating `ppp` and `owin` objects from simple feature data frames {.unnumbered}

Consider again Kansas tornadoes. Import the data as a simple feature data frame and transform the geographic CRS to Lambert conic conformal centered on Kansas (EPSG:6922). Keep all tornadoes (having an EF damage rating) since 1950 whose initial location occurred within Kansas.

```{r}
Torn.sf <- sf::st_read(dsn = here::here("data", "1950-2020-torn-initpoint")) |>
  sf::st_transform(crs = 6922) |>
  dplyr::filter(st == "KS", mag >= 0) |>
  dplyr::mutate(EF = factor(mag)) |>
  dplyr::select(EF)

Torn.sf |>
  head()
```

The length unit is meters. This can be seen by printing the CRS.

```{r}
sf::st_crs(Torn.sf)
```

Further you note that some tornadoes are incorrectly coded as Kansas tornadoes by plotting the event locations.

```{r}
plot(Torn.sf$geometry)
```

You recognize the large number of events within the near-rectangle shape of the Kansas border but you also see a few events clearly outside.

Instead of filtering by column name (`st == "KS"`) you can subset by geometry using the `sf::st_intersection()` function. Here, since we are using the functions in the {spatstat} package, you do this by defining the state border as an `owin` object.

You get the Kansas border as a simple feature data frame from the {USAboundaries} package transforming the CRS to that of the tornadoes.

```{r}
KS.sf <- USAboundaries::us_states(states = "Kansas") |>
  sf::st_transform(crs = sf::st_crs(Torn.sf))
```

You then create an `owin` object from the simple feature data frame using the `as.owin()` function.

```{r}
suppressMessages(library(spatstat))

KS.win <- KS.sf |>
  as.owin()
```

Next you convert the simple feature data frame of tornado reports to a `ppp` object with the EF damage rating as the marks using the `as.ppp()` function.

```{r}
T.ppp <- Torn.sf |>
  as.ppp()

plot(T.ppp)
```

Finally you subset the event locations in the `ppp` object by the Kansas border using the subset operator (`[]`).

```{r}
T.ppp <- T.ppp[KS.win]
plot(T.ppp)
```

With the `T.ppp` object you are ready to analyze the tornado locations as spatial point pattern data.

The `summary()` method summarizes information in the `ppp` object.

```{r}
summary(T.ppp)
```

The output tells you that there are 4281 events (tornado reports) with an average spatial intensity of .0000000201 (2.008277e-08) events per unit area.

The distance unit is meter since that is the length unit in the simple feature data frame (see `sf::st_crs(Torn.sf)` LENGTHUNIT["metre",1]) from which the `ppp` object was derived. So the area is in square meters making the spatial intensity (number of tornado reports per square meter) quite small.

To make it easier to interpret the intensity convert the length unit from meters to kilometers within the `ppp` object with the `rescale()` function from the {spatstat} package (spatstat.geom). The scaling factor argument is `s =` and the conversion is 1000 m = 1 km so the argument is set to 1000. You then set the unit name to `km` with the `unitname =` argument.

```{r}
T.ppp <- T.ppp |>
  spatstat.geom::rescale(s = 1000, 
                         unitname = "km")
summary(T.ppp)
```

Caution here as you are recycling the object name `T.ppp`. If you rerun the above code chunk the scale will change again by a factor of 1000 while the unit name will stay the same.

There are 4281 tornado reports with an average intensity of .02 tornadoes per square km over this time period. Nearly 60% of all Kansas tornadoes are EF0. Less than 1% of them are categorized as 'violent' (EF4 or EF5). The area of the state is 213,168 square kilometers (km).

Plot the events separated by the marks using the `plot()` method together with the `split()` function.

```{r}
T.ppp |>
  split() |>
plot()
```

Relative to the less damaging tornadoes there are far fewer EF4 and EF5 events.

Can the spatial distribution of Kansas tornadoes be described by complete spatial randomness?

The number of tornadoes varies across the state (EF4 tornadoes are rare in the far western part of the state for example) but it's difficult to say whether this is due to sampling variation. To illustrate this here you compare the EF1 tornado locations with a sample of events generated under the null hypothesis of CSR.

First create `Y` as an unmarked `ppp` object containing only the EF1 tornadoes. You do this by keeping only the events with marks equal to one with the `subset()` function. Since the marks are a factor you remove the levels with the `unmark()` function.

```{r}
( Y <- T.ppp |>
  subset(marks == 1) |>
  unmark() )
```

There were 1062 reported EF1 tornadoes originating within the state over the period 1950 through 2020.

The average intensity of the EF1 tornado events is obtained with the `intensity()` function.

```{r}
intensity(Y)
```

On average there has been .005 EF1 tornadoes per square km or 50 per 100 square km.

Make a map to check if things look right.

```{r}
plot(Y)
```

EF1 tornado reports are found throughout the state and they appear to be distributed randomly.

Formally: Is the spatial distribution of EF1 tornado reports consistent with a set of event locations that are described as complete spatial randomness?

To help answer this question you construct `X` to be a set of events generated from a homogeneous Poisson process (a model for CSR) where the intensity of the events is equal to the average intensity of the EF1 tornado reports.

You assign the average intensity to an object called `lambdaEF1` and then use `rpoispp()` (random Poisson point pattern) with lambda set to that intensity and the domain specified with the `win =` argument.

```{r}
( lambdaEF1 <- intensity(Y) )

( X <- rpoispp(lambda = lambdaEF1, 
               win = window(Y)) )
```

The average intensity of `X` matches (closely) the average intensity of `Y` by design and the `plot()` method reveals a similar looking pattern of event locations.

```{r}
intensity(X)

plot(X)
```

While the pattern is similar, there does appear to be a difference. Can you describe the difference?

To make comparisons between the two point pattern data (one observed events and the other simulated) easier you use the `superimpose()` function to create a single `ppp` object and assign to `Z` marks `Y` and `X`. Then plot the two intensity rasters split by mark type.

```{r}
Z <- superimpose(Y = Y, 
                 X = X)

Z |>
  split() |>
  density() |>
  plot()
```

The range of local intensity variations is similar. So we don't have much evidence against the null model of CSR as defined by a homogeneous Poisson process.

## Estimating spatial intensity as a function of distance {.unnumbered}

Are tornado reports more common in the vicinity of towns?

Based on domain specific knowledge of how these data were collected you suspect that tornado reports will cluster near cities and towns. This is especially true in the earlier years of the record.

This understanding is available from the literature on tornadoes (not from the data) and it is a well-known artifact of the data set, but it had never been quantified until 2013 in a paper we wrote. <http://myweb.fsu.edu/jelsner/PDF/Research/ElsnerMichaelsScheitlinElsner2013.pdf>.

How was this done? You estimate of the spatial intensity of the observed tornado reports as a function of distance from nearest town and compare that estimate with an estimate of the spatial intensity as a function of distance using randomly placed events across the state.

First get the city locations from the `us_cities()` function in the {USAboundaries} package. Exclude towns with fewer than 1000 people and transform the geometry to that of the tornado locations.

```{r}
C.sf <- USAboundaries::us_cities() |>
  dplyr::filter(population >= 1000) |>
  sf::st_transform(crs = sf::st_crs(Torn.sf))
```

Create a `ppp` object of events from the city/town locations in the simple feature data frame. Remove the marks and include only events inside the window object (`KS.own`). Convert the distance unit from meters to kilometers.

```{r}
C.ppp <- C.sf |>
  as.ppp() |>
  unmark()

C.ppp <- C.ppp[KS.win] |>
  spatstat.geom::rescale(s = 1000,
                         unitname = "km")
plot(C.ppp)
```

Next compute a 'distance map'. A distance map for a spatial domain A is a function $f(s)$ whose value is defined for any point $s$ as the shortest distance from $s$ to any event location in A.

This is done with the `distmap()` function and the points are the intersections of a 128 x 128 rectangular grid.

```{r}
Zc <- distmap(C.ppp)
plot(Zc)
```

The result is an object of class `im` (image raster). Distances are in kilometers. Most points in Kansas are less than 50 km from the nearest town (reds and blues) but some points are more than 80 km away (yellow).

Other distance functions include `pairdist()`, which is the pairwise distance between all event pairs and `crossdist()`, which is the distance between events from two point patterns. The `nndist()` computes the distance between an event and its nearest neighbor event.

The distance map (distance from any point in Kansas to the nearest town) is used to quantify the population bias in the tornado records.

This is done with `rhohat()` which estimates the smoothed spatial intensity as a function of some explanatory variable. The relationship between spatial intensity and an explanatory variable is sometimes called a 'resource selection' function (if the events are organisms and the variable is a descriptor of habitat) or a 'prospectivity index' (if the events are mineral deposits and the variable is a geological variable).

The method assumes the events are a realization from a Poisson process with intensity function $\lambda(u)$ of the form

$$
\lambda(u) = \rho[Z(u)]
$$

where $Z$ is the spatial explanatory variable (covariate) function (with continuous values) and $\rho(z)$ is a function to be estimated.

The function does not assume a particular form for the relationship between the point pattern and the variable (thus it is said to be 'non-parametric').

Here you use `rhohat()` to estimate tornado report intensity as a function of distance to nearest city.

The first argument in `rhohat()` is the `ppp` object for which you want the intensity estimate and the `covariate =` argument is the spatial variable, here as object of class `im`. By default kernel smoothing is done using a fixed bandwidth. With `method = "transform"` a variable bandwidth is used.

```{r}
rhat <- rhohat(Y, 
               covariate = Zc,
               method = "transform")

class(rhat)
```

The resulting object (`rhat`) has three classes including a data frame. The data frame contains the explanatory variable as a single vector (`Zc`), an estimate of the intensity at the distances (`rho`), the variance (`var`) and upper (`hi`) and lower (`lo`) uncertainty values (point-wise).

```{r}
rhat |>
  data.frame() |>
  head()
```

Here you put these values into a new data frame (`df`) multiplying the intensities by 10,000 (so areal units are 100 sq. km) then use `ggplot()` method with a `geom_ribbon()` layer to overlay the uncertainty band.

```{r}
df <- data.frame(dist = rhat$Zc, 
                 rho = rhat$rho * 10000, 
                 hi = rhat$hi * 10000, 
                 lo = rhat$lo * 10000)

library(ggplot2)

ggplot(data = df) +
  geom_ribbon(mapping = aes(x = dist, ymin = lo , ymax = hi), alpha = .3) +
  geom_line(mapping = aes(x = dist, y = rho), color = "red") +  
  geom_hline(yintercept = intensity(Y) * 10000, color = "blue") +
  scale_y_continuous(limits = c(0, 100)) +
  ylab("Tornado reports (EF1) per 100 sq. km") +
  xlab("Distance from nearest town center (km)") +
  theme_minimal()
```

The vertical axis on the plot is the tornado report intensity in units of number of reports per 100 square kilometers. The horizontal axis is the distance to nearest town in km. The red line is the average spatial intensity as a function of distance from nearest town. The 95% uncertainty band about this estimate is shown in gray.

At points close to the town center tornado reports are high relative to at points far from town. The blue line is the average intensity across the state computed with the `intensity()` function and scaled appropriately. At points within about 15 km the tornado report intensity is above the statewide average intensity. At points greater than about 60 km the report intensity is below the statewide average.

At zero distance from a town, this number is more than 1.7 times higher (85 tornadoes per 100 sq. km). The spatial scale is about 15 km (distance along the spatial axis where the red line falls below the blue line).

At this point in the analysis you need to think that although the plot look reasonable based on your expectations of a population bias in the tornado reports (more reports near cities/towns), could this result be an artifact of the smoothing algorithm?

You need to know how to apply statistical tools to accomplish specific tasks. But you also need to be a bit skeptical of the tool's outcome. The skepticism provides a critical check against being fooled by randomness.

As an example, the method of computing the spatial intensity as a function of a covariate should give you a different answer if events are randomly distributed. If the events are randomly distributed, what would you expect to find on a plot such as this?

You already generated a set of events from a homogeneous Poisson model so you can check simply by applying the `rhohat()` function to these events using the same set of city/town locations.

```{r}
rhat0 <- rhohat(X, 
                covariate = Zc,
                method = "transform")
df <- data.frame(dist = rhat0$Zc, 
                 rho = rhat0$rho * 10000, 
                 hi = rhat0$hi * 10000, 
                 lo = rhat0$lo * 10000)
ggplot(df) +
  geom_ribbon(aes(x = dist, ymin = lo , ymax = hi), alpha = .3) +
  geom_line(aes(x = dist, y = rho), color = "red") +  
  geom_hline(yintercept = intensity(Y) * 10000, color = "blue") +
  scale_y_continuous(limits = c(0, 100)) +
  ylab("Random events per 100 sq. km") +
  xlab("Distance from nearest town center (km)") +
  theme_minimal()
```

As expected, the number of random events near cities/towns is not higher than the number of random events at greater distances. The difference between the two point pattern data sets can be explained by the clustering of actual tornado reports in the vicinity of towns.

## Intensity trend as a possible confounding factor {.unnumbered}

Quantifying the report bias with the spatial intensity function works well for Kansas where there is no trend in the local intensity. Local tornado intensity is largely uniform across Kansas.

Things are different in Texas where a significant intensity trend makes it more difficult to estimate the report bias.

Convert the tornado reports (EF1 or worse) occurring over Texas as a `ppp` object. Use a Texas-centric Lambert conic conformal projection.

```{r}
Torn.sf <- sf::st_read(dsn = here::here("data", "1950-2020-torn-initpoint")) |>
  sf::st_transform(crs = 3082) |>
  dplyr::filter(mag >= 0)

T.ppp <- Torn.sf |>
  as.ppp()

W <- USAboundaries::us_states(states = "Texas") |>
  sf::st_transform(crs = sf::st_crs(Torn.sf)) |>
  as.owin()

( T.ppp <- T.ppp[W] |>
    spatstat.geom::rescale(s = 1000, 
                           unitname = "km") )
intensity(T.ppp)
```

There are 8,932 tornado reports. The distance unit is kilometer. The average intensity is .013 events per square kilometer over this 71-year period (1950-2020).

Next plot the local intensity using a kernel smoother.

```{r}
T.ppp |>
  density() |>
  plot()
```

There is a clear trend of tornado reports from a low number of reports in the southwest part of the state along the Rio Grande to a high number of reports in the northeast part of the state. The statewide average intensity of .013 tornado reports per square km is too high in southwest and too low in the northern.

Next compute and plot the spatial intensity as a smoothed function of distance to nearest town or city. Start by removing the marks on the tornado events assigning the unmarked `ppp` object to `Tum.ppp`. Then create a `ppp` object from the city/town locations and subset the tornado events by the window.

```{r}
Tum.ppp <- T.ppp |>
  unmark()

C.ppp <- C.sf |>
  sf::st_transform(crs = sf::st_crs(Torn.sf)) |>
  as.ppp() |>
  unmark()

C.ppp <- C.ppp[W] |>
  spatstat.geom::rescale(s = 1000,
                         unitname = "km")
```

Next create a distance map of the city/town locations using the `distmap()` function.

```{r}
Zc <- distmap(C.ppp)
plot(Zc)
```

Finally, compute the intensity of tornadoes as a smoothed function of distance to nearest town/city with the `rhohat()` function. Prepare the output and make a plot.

```{r}
rhat <- rhohat(Tum.ppp, 
               covariate = Zc,
               method = "transform")

data.frame(dist = rhat$Zc, 
           rho = rhat$rho, 
           hi = rhat$hi, 
           lo = rhat$lo) |>
ggplot() +
  geom_ribbon(aes(x = dist, ymin = lo , ymax = hi), alpha = .3) +
  geom_line(aes(x = dist, y = rho), color = "red") +  
  scale_y_continuous(limits = c(0, NA)) +
  geom_hline(yintercept = intensity(Tum.ppp), color = "blue") +
  ylab("Tornado reports per sq. km") +
  xlab("Distance from nearest town center (km)") +
  theme_minimal()
```

The plot shows that the intensity of the tornado reports is much higher than the average intensity in the vicinity of towns and cities. Yet caution needs to exercised in the interpretation because the trend of increasing tornado reports moving from southwest to northeast across the state is mirrored by the trend in the occurrence of cities/towns. There are many fewer towns in the southwestern part of Texas compared to the northern and eastern part of the state.

You can quantify this effect by specifying a function in the `covariate =` argument. Here you specify a planar surface with `x,y` as arguments and `x + y` inside the function. Here you use the `plot()` method on the output (instead of creating a data frame and using `ggplot()`).

```{r}
plot(rhohat(Tum.ppp, 
            covariate = function(x,y){x + y},
            method = "transform"),
     main = "Spatial intensity trend of tornadoes")
```

Local intensity increases along the axis labeled `X` starting at a value of 7,400. At value of `X` equal to about 8,200 the spatial intensity stops increasing.

Units along the horizontal axis are kilometers but the reference (intercept) distance is at the far left. So you interpret the increase in spatial intensity going from southwest to northeast as a change across about 800 km (8200 - 7400).

The local intensity of cities has the same property (increasing from southwest to northeast then leveling off). Here you substitute `C.ppp` for `Tum.ppp` in the `rhohat()` function.

```{r}
plot(rhohat(C.ppp, 
            covariate = function(x,y){x + y},
            method = "transform"),
     main = "Spatial intensity trend of cities")
```

So the population bias towards more reports near towns/cities is potentially confounded by the fact that there tends to be more cities and towns in areas that have conditions more favorable for tornadoes.

Thus you can only get so far by examining intensity estimates. If your interest lies in inferring the causes of spatial variation in the intensity you will need to look at second order (clustering) properties of the events.

<!--chapter:end:15-Lesson.Rmd-->

# Tuesday October 25, 2022 {.unnumbered}

**"To me programming is more than an important practical art. It is also a gigantic undertaking in the foundations of knowledge."** – Grace Hopper

Today

- Estimating the relative risk of events
- Estimating second-order properties of spatial events

## Estimating the relative risk of events {-}

Separate spatial intensity maps across two marked types provides a way to estimate the risk of one event type conditional on the other event type. More generally, the relative risk of occurrence of some event is a conditional probability. In a non-spatial context, the risk of catching a disease if you are elderly relative to the risk if you are young.

Given a tornado somewhere in Texas what is the chance that it will cause at least EF3 damage? With the historical set of all tornadoes marked by the damage rating you can make a map of all tornadoes and a map of the EF3+ tornadoes and then take the ratio.

To see this start by importing the tornado data, mutating and selecting the damage rating as a factor called `EF` before turning the resulting simple feature data frame into a planar point pattern. 
```{r}
Torn.sf <- sf::st_read(dsn = here::here("data", "1950-2020-torn-initpoint")) |>
  sf::st_transform(crs = 3082) |>
  dplyr::filter(mag >= 0) |>
  dplyr::mutate(EF = as.factor(mag)) |>
  dplyr::select(EF)

library(spatstat)
T.ppp <- Torn.sf |>
  as.ppp()
```

Then subset by the boundary of Texas.

```{r}
TX.sf <- USAboundaries::us_states(states = "Texas") |>
  sf::st_transform(crs = sf::st_crs(Torn.sf))

W <- TX.sf |>
  as.owin()

T.ppp <- T.ppp[W]
summary(T.ppp)
```

Chance that a tornado anywhere in Texas will be at least EF3 or worse is the sum of the proportions for these types: .03616 + .00537 + .00067 = .042 (or 4.2%). 

As found previously there is a spatial intensity gradient across the state with fewer tornadoes in the southwest and more in the northeast. Also the more damaging tornadoes might be more common relative to all tornadoes in some parts of the state compared with other parts.

To create a map of the relative risk of the more damaging tornadoes you start by making two `ppp` objects, one being the set of all tornado events with damage ratings 0, 1, or 2 and the other the set of all tornado locations with damage ratings 3, 4, or 5. You do this by subset the object using brackets (`[]`) and the logical operator `|` (or) and then merge the two subsets assigning names `H` and `I` as marks with the `superimpose()` function.

```{r}
H.ppp <- unmark(T.ppp[T.ppp$marks == 2 | T.ppp$marks == 1 | T.ppp$marks == 0])
I.ppp <- unmark(T.ppp[T.ppp$marks == 3 | T.ppp$marks == 4 | T.ppp$marks == 5])
T2.ppp <- superimpose(H = H.ppp, 
                      I = I.ppp)
```

See https://en.wikipedia.org/wiki/Enhanced_Fujita_scale for definitions of EF tornado rating.

The chance that a tornado chosen at random is intense (EF3+) is 4.2%. Plot the event locations for the set of intense tornadoes.

```{r}
plot(I.ppp, 
     pch = 25, 
     cols = "red", 
     main = "")
plot(T.ppp, add = TRUE, lwd = .1)
```

To get the relative risk use the `relrisk()` function. If X is a multi-type point pattern with factor marks and two levels of the factor then the events of the first type (the first level of `marks(X)`) are treated as controls (conditionals) or non-events, and events of the second type are treated as cases.

The `relrisk()` function estimates the local chance of a case (i.e. the probability $p(u)$ that a point at $u$ will be a case) using a kernel density smoother. The bandwidth for the kernel is specified or can be found through an iterative cross-validation procedure (recall the bandwidth selection procedure used in geographic regression) using the `bw.relrisk()` function. 

The bandwidth has units of length (here meters). You specify a minimum and maximum bandwidth with the `hmin =` and `hmax =` arguments. This takes a few seconds.

```{r}
( bw <- bw.relrisk(T2.ppp,
                   hmin = 1000,
                   hmax = 200000) )
```

The optimal bandwidth (`sigma`) is 119770 meters or about 120 km. 

Now estimate the relative risk at points defined by a 256 by 256 grid and using the 120 km bandwidth for the kernel smoother.

```{r}
rr <- relrisk(T2.ppp, 
              sigma = bw,
              dimyx = c(256, 256))
```

The result is an object of class `im` (image) with values you interpret as the conditional probability of an 'intense' tornado.

You retrieve the range of probabilities with the `range()` function. Note that many of the values are `NA` corresponding pixels that are outside the window so you set the `na.rm` argument to `TRUE`.

```{r}
range(rr, na.rm = TRUE)
```

The probabilities range from a low of .5% to a high of 6%. This range compares with the statewide average probability of 4.2%.

Map the probabilities with the `plot()` method.
```{r}
plot(rr)
```

Make a better map by converting the image to a raster, setting the CRS, and then using functions from the {tmap} package.

```{r}
tr.r <- raster::raster(rr)
raster::crs(tr.r) <- sf::st_crs(Torn.sf)$proj4string

tmap::tm_shape(tr.r) +
  tmap::tm_raster()
```

The chance that a tornado is more damaging peaks in the northeast part of the state.

Since the relative risk is computed for any point it is of interest to extract the probabilities for cities and towns.

You get city locations with the `us_cities()` function from the {USAboundaries} package that extracts a simple feature data frame of cities. The CRS is 4326 and you filter to keep only cities with at least 100000 in 2010.

```{r}
Cities.sf <- USAboundaries::us_cities(state = "TX") |>
  sf::st_transform(crs = raster::crs(tr.r)) |>
  dplyr::filter(population > 100000)
```

Use the `extract()` function from the {raster} package to get a single value for each city. Put these values into the simple feature data frame.

```{r}
Cities.sf$tr <- raster::extract(tr.r, 
                                Cities.sf)

Cities.sf |>
  dplyr::arrange(desc(tr)) 
```

To illustrate the results create a graph using the `geom_lollipop()` function from the {ggalt} package. Use the package {scales} to allow for labels in percent.
```{r}
library(ggalt)
library(scales)

ggplot(Cities.sf, aes(x = reorder(city, tr), y = tr)) +
    geom_lollipop(point.colour = "steelblue", point.size = 3) +
    scale_y_continuous(labels = percent, limits = c(0, .0625)) +
    coord_flip() +
    labs(x = "", y = NULL, 
         title = "Historical chance that a tornado caused at least EF3 damage",
         subtitle = "Cities in Texas with a 2010 population > 100,000",
         caption = "Data from SPC (1950-2020)") +
  theme_minimal()
```

Another example: Florida wildfires

Given a wildfire in Florida what is the probability that it was started by lightning? 

Import wildfire data (available here: https://www.fs.usda.gov/rds/archive/catalog/RDS-2013-0009.4) as a simple feature data frame and transform the native CRS to a Florida GDL Albers (EPSG 3086).
```{r}
if(!"FL_Fires" %in% list.files(here::here("data"))){
  download.file("http://myweb.fsu.edu/jelsner/temp/data/FL_Fires.zip",
                destfile = here::here("data", "FL_Fires.zip"))
unzip(zipfile = here::here("data", "FL_Fires.zip"),
      exdir = here::here("data"))
}

FL_Fires.sf <- sf::st_read(dsn = here::here("data", "FL_Fires")) |>
  sf::st_transform(crs = 3086)
dim(FL_Fires.sf)
```

Each row is a unique fire and the data spans the period 1992-2015. There are over 90K rows and 38 variables. 

To make things run faster, here you analyze only a random sample of all the data. You do this with the `dplyr::sample_n()` function where the argument `size =` specifies the number of rows to choose at random. Save the sample of events to the object `FL_FiresS.sf`. First set the seed for the random number generator so that the set of rows chosen will be the same every time you run the code.

```{r}
set.seed(78732)

FL_FiresS.sf <- FL_Fires.sf |>
  dplyr::sample_n(size = 2000)

dim(FL_FiresS.sf)
```

The result is a simple feature data frame with exactly 2000 rows.

The character variable `STAT_CAU_1` indicates the cause of the wildfire.

```{r}
FL_FiresS.sf$STAT_CAU_1 |>
  table()
```

There are 13 causes (listed in alphabetical order) with various occurrence frequencies. Lightning is the most common.

To analyze these data as spatial events, you first convert the simple feature data to a `ppp` object over a window defined by the state boundaries. Use the cause of the fire as a factor mark.
```{r}
F.ppp <- FL_FiresS.sf["STAT_CAU_1"] |>
  as.ppp()

W <- USAboundaries::us_states(states = "Florida") |>
  sf::st_transform(crs = sf::st_crs(FL_Fires.sf)) |>
  as.owin()

F.ppp <- F.ppp[W]
marks(F.ppp) <- as.factor(marks(F.ppp)) # make the character marks factor marks

summary(F.ppp)
```

Output from the `summary()` method displays a table of frequency by type including the proportion and the average spatial intensity (per square meters). 

The probability that a wildfire is caused by lightning is about 25% (`proportion` column of the frequency versus type table). How does this probability vary over the state?

Note that the window contains four separate polygons to capture the main boundary (`polygon 4`) and the Florida Keys.
```{r}
plot(W)
```

First split the object `F.ppp` on whether or not the cause was lightning and then merge the two event types and assign names `NL` (human caused) and `L` (lightning caused) as marks.

```{r}
L.ppp <- F.ppp[F.ppp$marks == "Lightning"] |>
  unmark()
NL.ppp <- F.ppp[F.ppp$marks != "Lightning"] |>
  unmark()

LNL.ppp <- superimpose(NL = NL.ppp, 
                       L = L.ppp)

summary(LNL.ppp)
```

Now the two types are `NL` and `L` composing 75% and 25% of all wildfire events.

The function `relrisk()` computes the spatially-varying probability of a case (event type), (i.e. the probability $p(u)$ that a point at location $u$ will be a case).

Here you compute the relative risk on a 256 by 256 grid.
```{r}
wfr <- relrisk(LNL.ppp, 
               dimyx = c(256, 256))
```

Create a map from the raster by first converting the image object to a raster object and assigning the CRS with the `crs()` function from the {raster} package. Add the county borders for geographic reference.
```{r}
wfr.r <- raster::raster(wfr)
raster::crs(wfr.r) <- sf::st_crs(FL_Fires.sf)$proj4string

FL.sf <- USAboundaries::us_counties(state = "FL") |>
  sf::st_transform(crs = sf::st_crs(FL_Fires.sf))

tmap::tm_shape(wfr.r) +
  tmap::tm_raster(title = "Probability") +
tmap::tm_shape(FL.sf) +
  tmap::tm_borders(col = "gray70") +
tmap::tm_legend(position = c("left", "center") ) +
tmap::tm_layout(main.title = "Chance a wildfire was started by lightning (1992-2015)",
                main.title.size = 1) +
tmap::tm_compass(position = c("right", "top")) +
tmap::tm_credits(text = "Data source: Karen Short https://doi.org/10.2737/RDS-2013-0009.4",
                 position = c("left", "bottom")) 
```

## Estimating second-moment properties of spatial events {-}

Spatial intensity is a first-moment property of event locations (like the average of a set of numbers). It answers the question: where are events more and less frequent? 

Clustering is a second-moment property of event locations (like the variance of a a set of numbers). It answers the question: is the probability of an event in the proximity of another event higher than expected by chance? 

On example of cluster occurs with the location of trees in a forest. A tree's seed dispersal mechanism leads to a greater likelihood of another tree nearby.

Let $r$ be the distance between two event locations or the distance between an event and an arbitrary point within the domain, then functions to describe clustering include:

- The nearest neighbor distance function $G(r)$: The cumulative distribution of the distances from an event to the nearest other event (event-to-event function). It summarizes the distance between events (amount of clustering).

- The empty space function $F(r)$: The cumulative distribution of the distances from a point in the domain to the nearest event (point-to-event function). It summarizes the distance gaps between events (amount of gappiness or lacunarity).

- The reduced second-moment function (Ripley $K$) $K(r)$: Defined such that $\lambda \times K(r)$ is the expected number of additional events within a distance $r$ of an event, where $\lambda$ is the average intensity of the events. It is a measure of the spatial autocorrelation among the events.

To assess the degree of clustering and significance (in a statistical sense), we estimate values of the function using our data set and compare the resulting curve (empirical curve) to a theoretical curve assuming a homogeneous Poisson process. 

The theoretical curve is well defined for homogeneous point patterns (recall: CSR--complete spatial randomness). Deviations of an 'empirical' curve from a theoretical curve provides evidence against CSR.

The theoretical functions assuming a homogeneous Poisson process are:

- $F(r) = G(r) = 1 - \exp(-\lambda \pi r^2)$
- $K(r) = \pi r^2$

where $\lambda$ is the domain average spatial intensity and $\exp()$ is the exponential function.

Recall the Swedish pine saplings data that comes with the {spatstat} package.
```{r}
data(swedishpines)
class(swedishpines)
```

Assign the data to an object called `SP` to reduce the amount of typing. 
```{r}
( SP <- swedishpines )
```

The output indicates that there are 71 events within a rectangle window 96 by 100 units where one unit is .1 meters.

You obtain the values for the nearest neighbor function using the `Gest()` function from the {spatstat} package. Use the argument `correction = "none"` so no corrections are made for events near the window borders. Assign the output to a list object called `G`.

```{r}
( G <- Gest(SP,
            correction = "none") )
```

The output includes the distance `r`, the raw uncorrected estimate of $G(r)$ (empirical estimate) at various distances, and a theoretical estimate at those same distances based on a homogeneous Poisson process. Using the `plot()` method on the saved object `G` you compare the empirical estimates with the theoretical estimates. Here two horizontal lines are added to help with the interpretation.

```{r}
plot(G)
abline(h = c(.2, .5), 
       col = "black",
       lty = 2)
```

Values of G are on the vertical axis and values of distance (lag) are on the horizontal axis starting at 0. The black curve is the uncorrected estimate of $G_{raw}(r)$ from the event locations and the red curve is $G_{pois}(r)$ estimated from a homogeneous Poisson process with the same average intensity as the pine saplings.

The horizontal dashed line at G = .2 intersects the black line at a relative distance (r) of 5 units. This means that 20% of the events have another event _within_ 5 units. This means that 20% of the saplings have another sapling withing .5 meter. 

Imagine placing a disc of radius 5 units around all 71 events then counting the number of events that have another event under the disc. That number divided by 71 is G(r).

To check this compute all pairwise distances with the `pairdist()` function.
```{r}
PDmatrix <- pairdist(SP)
PDmatrix[1:6, 1:6]
```

This creates a 71 x 71 square matrix of distances. 

Sum the number of rows whose distances are within 5 units. The minus one means you don't count the row containing event over which you are summing (an event location is not a neighbor of itself).
```{r}
sum(rowSums(PDmatrix < 5) - 1) / nrow(PDmatrix) * 100
```

Returning to the plot, the horizontal dashed line at G = .5 intersects the black line at .8 meters indicating that 50% of the pine saplings have another pine sapling within .8 meter.

You see that for a given radius the $G_{raw}$ line is _below_ the $G_{pois}(r)$ line indicating that there are _fewer_ pine saplings with another pine sapling in the vicinity than expected by chance.

For example, if the saplings were arranged under a model of CSR, you would expect 20% of the pairwise distances to be within .3 meter and 50% of them to be within .55 meter.

You make a better plot by first converting the object `G` to a data frame and then using {ggplot2} functions. Here you do this and then remove estimates for distances greater than 1.1 meter and convert the distance units to meters.
```{r}
G.df <- as.data.frame(G) |>
  dplyr::filter(r < 11) |>
  dplyr::mutate(r = r * .1)

ggplot(data = G.df, 
       mapping = aes(x = r, y = raw)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  geom_hline(yintercept = c(.2, .5), lty = 'dashed') +
  xlab("Lag distance (m)") +  ylab("G(r): Cumulative % of events having another event within a distance r") +
  theme_minimal()
```

Values for the empty space function are obtained from the `Fest()` function. Here you apply the Kaplan-Meier correction for edge effects with `correction = "km"`. The function returns the percent of the domain within a distance from any event. 

Imagine again placing the disc, but this time on top of every point in the window and counting the number of points that have an event underneath.

Make a plot and add some lines to help with interpretation. 
```{r}
F.df <- SP |>
  Fest(correction = "km") |>
  as.data.frame() |>
  dplyr::filter(r < 11) |>
  dplyr::mutate(r = r * .1)

ggplot(data = F.df, 
       mapping = aes(x = r, y = km)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  geom_hline(yintercept = c(.7, .58), lty = 'dashed') +
  geom_vline(xintercept = .61, lty = 2) +
  xlab("Lag distance (m)") +  ylab("Percent of domain within a distance r of an event") +
  theme_minimal()
```

The horizontal dashed line at F = .7 intersects the black line at a distance of .61 meter. This means that 70% of the spatial domain is less than .61 meters from a sapling. The red line is the theoretical homogeneous Poisson process model. If the process was CSR slightly less than 58% (F = .58) of the domain would be less than .6 meter from a sapling. In words, the arrangement of saplings is less "gappy" (more regular) than expected by chance.

The J function is the ratio of the F function to the G function. For a CSR processes the value of J is one. Here we see a large and systematic departure of J from one for distances greater than about .5 meter, due to the regularity in the spacing of the saplings.

```{r}
J.df <- SP |>
    Jest() |>
    as.data.frame() |>
    dplyr::filter(r < 10) |>
    dplyr::mutate(r = r * .1)

ggplot(data = J.df, 
       mapping = aes(x = r, y = km)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  xlab("Lag distance (m)") + ylab("") +
  theme_minimal()
```

A commonly used distance function for assessing clustering in point pattern data is called Ripley's K function. It is estimated with the `Kest()` function. 

Mathematically it is defined as

$$
\hat K(r) = \frac{1}{\hat \lambda} \sum_{j \ne i} \frac{I(r_{ij} < r)}{n}
$$

where $r_{ij}$ is the Euclidean distance between event $i$ and event $j$, $r$ is the search radius, and $\hat \lambda$ is an estimate of the intensity $(\hat \lambda = n/|A|)$ where $|A|$ is the window area and $n$ is the number of events. $I(.)$ is an indicator function equal to 1 when the expression $r_{ij} < r$, and 0 otherwise. If the events are homogeneous, $\hat{K}(r)$ increases at a rate proportional to $\pi r^2$.

<!--chapter:end:16-Lesson.Rmd-->

# Thursday October 27, 2022 {.unnumbered}

**"Good code is its own best documentation. As you're about to add a comment, ask yourself, 'How can I improve the code so that this comment isn't needed?' Improve the code and then document it to make it even clearer."** - Steve McConnell

Today

- Examples of spatially clustered events
- Determining the statistical significance of event clustering
- Estimating event clustering in multi-type event locations
- More about the Ripley K function

## Examples of spatially clustered events {-}

Bramble canes

The locations of bramble canes are available as a marked `ppp` object in the {spatstat} package. A bramble is a rough (usually wild) tangled prickly shrub with thorny stems.

```{r}
suppressMessages(library(spatstat))

data(bramblecanes)
summary(bramblecanes)
```

The marks represent three different ages (as an ordered factor) for the bramble canes. The unit of length is 9 meters.

```{r}
plot(bramblecanes) 
```

Consider the point pattern for all the bramble canes regardless of age and estimate the $K$ function and a corresponding plot. Plot the empirical estimate of $K$ with an 'isotropic' correction at the domain borders (`iso`). Include a line for the theoretical $K$ under the assumption of CSR.

```{r}
K.df <- bramblecanes |>
  Kest() |>
  as.data.frame() |>
  dplyr::mutate(r = r * 9)

library(ggplot2)

ggplot(data = K.df, 
       mapping = aes(x = r, y = iso)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  xlab("Lag distance (m)") + ylab("K(r)") +
  theme_minimal()
```

The $K$ estimate from the actual data (black line) lies to the left of the theoretical $K$  under CSR (red line). This means that for any distance from an event (lag distance) there tends to be _more_ events within this distance (larger $K$) than expected under CSR. You conclude that these bramble canes are more clustered than CRS.

The expected number of additional events is multiplied by the total number of events (823) so a value of .1 indicates that at a distance of 1.6 meters (where .1 value of $K(r)$ intersects the red curve) you should expect to see about 82 additional events.

Kansas tornado reports

Previously you mapped the intensity of tornadoes across Kansas using the start locations as point pattern data. Here we return to these data and consider only tornadoes since 1994.

```{r}
Torn.sf <- sf::st_read(dsn = here::here("data", "1950-2020-torn-initpoint")) |>
  sf::st_transform(crs = 3082) |>
  dplyr::filter(mag >= 0, yr >= 1994) |>
  dplyr::mutate(EF = as.factor(mag)) |>
  dplyr::select(EF)

T.ppp <- Torn.sf["EF"] |>
  as.ppp()

KS.sf <- USAboundaries::us_states(states = "Kansas") |>
  sf::st_transform(crs = sf::st_crs(Torn.sf)$proj4string)

W <- KS.sf |>
  as.owin()

T.ppp <- T.ppp[W] |>
  spatstat.geom::rescale(s = 1000, 
                         unitname = "km")

T.ppp |>
  plot()

T.ppp |>
  summary()
```

There are 2241 events with an average intensity of .01 events per square km (1 tornado per 10 square km over the 26-year period 1994--2020).

You compare the $K$ function estimated from the set of tornado reports with a theoretical $K$ function from a model of CSR.

```{r}
K.df <- T.ppp |>
  Kest(correction = "iso") |>
  as.data.frame() |>
  dplyr::mutate(Kdata = iso * sum(intensity(T.ppp)),
                Kpois = theo * sum(intensity(T.ppp)))

ggplot(data = K.df, 
       mapping = aes(x = r, y = Kdata)) +
  geom_line() +
  geom_line(mapping = aes(y = Kpois), color = "red") +
  geom_vline(xintercept = 60, lty = 'dashed') +
  geom_hline(yintercept = 129, lty = 'dashed') +
  geom_hline(yintercept = 115, lty = 'dashed') +
  xlab("Lag distance (km)") + ylab("K(r), Expected number of additional tornadoes\n within a distance r of any tornado") +
  theme_minimal()
```

Consider the lag distance of 60 km along the horizontal axis. If you draw a vertical line at that distance it intersects the black curve at a height of about 129. This value indicates that at a distance of 60 km from a random tornado report about 129 other tornado reports are in the vicinity (on average).

Imagine placing a disc with radius 60 km centered on each event and then averaging the number of events under the disc over all events.

The red line is the theoretical curve under the assumption that the tornado reports are CSR across the state. If this is the case then you would expect to see about 115 tornadoes within a distance 60 km from any tornado (on average). Since there are MORE tornadoes than expected within a given 60 km radius you conclude that there is evidence for clustering (at this spatial scale).

The black line lies above the red line across distances from 0 to greater than 100 km.

How do you interpret the results of applying the nearest neighbor function to these data? Here you create a data frame from the output of the `Gest()` function and remove distances exceeding 8 km.
```{r}
G.df <- T.ppp |>
  Gest(correction = "km") |>
  as.data.frame() |>
  dplyr::filter(r < 8)

ggplot(data = G.df, 
       mapping = aes(x = r, y = km)) +
  geom_line() + 
  geom_line(aes(y = theo), color = "red") +
  geom_hline(yintercept = .4, lty = 'dashed') +
  geom_vline(xintercept = c(3.2, 4), lty = 'dashed') +
  xlab("Lag distance (km)") + ylab("G(r): Cumulative % of tornadoes\n within a distance r of another tornado") +
  theme_minimal()
```

The interpretation is that 40% ($G$ = .4) of all tornado reports have another report within a distance of about 3.2 km on average. If the reports where homogeneous Poisson (CSR) then the distance would be 4 km. We conclude they are more clustered. 

Note: With a data set containing many events the difference between the raw and border-corrected estimates of the distance functions is typically small.

## Determining the statistical significance of event clustering {-}

The plots show a separation between the black solid line and the red line, but is this separation large relative to sampling variation? Is the above difference between the empirical and theoretical distance functions (e.g., $G$) large enough to conclude there is significant clustering?

There are two ways to approach statistical inference. 1) Compare the function computed with the observed data against the function computed data generated under the null hypothesis and ask: does the function fall outside the envelope of functions from the null cases? 2) Get estimates of uncertainty on the function and ask: does the uncertainty interval contain the null case? 

With the first approach you take a `ppp` object and then compute the function of interest (e.g., Ripley's K) for a specified number of samples under the null hypothesis of a homogeneous Poisson process.

To make things run faster you consider a subset of all the tornadoes (those that have an EF rating of 2 or higher). You create a new `ppp` object that contains only tornadoes rated at least EF2. Since the marks is a factor vector you can't use `>=`.

```{r}
ST.ppp <- unmark(T.ppp[T.ppp$marks == 2 | 
                       T.ppp$marks == 3 | 
                       T.ppp$marks == 4 |
                       T.ppp$marks == 5])
plot(ST.ppp)
```

The `envelope()` method from the {spatstat} package is used on this new `ST.ppp` object. You specify the function with the `fun = Kest` argument and the number of samples with the `nsim =` argument. You then convert the output to a data frame. It takes a few seconds to complete the computation of $K$ for all 99 samples.

```{r}
Kenv.df <- envelope(ST.ppp, 
                    fun = Kest, 
                    nsim = 99) |>
  as.data.frame()

head(Kenv.df)
```

The resulting data frame contains estimates of Ripley's $K$ as a function of lag distance (`r`) (column labeled `obs`). It also has the estimates of $K$ under the null hypothesis of CSR (`theo`) and the lowest (`lo`) and highest (`hi`) values of $K$ across the 99 samples.

You plot this information using the `geom_ribbon()` layer to include a gray ribbon around the model of CSR.

```{r}
ggplot(data = Kenv.df, 
       mapping = aes(x = r, y = obs * intensity(ST.ppp))) +
  geom_ribbon(mapping = aes(ymin = lo * intensity(ST.ppp), 
                            ymax = hi * intensity(ST.ppp)), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo * intensity(ST.ppp)), color = "red") +
  xlab("Lag distance (km)") + ylab("K(r)") +
  theme_minimal()
```

The $K$ function computed on the data is the black line and the $K$ function under CSR is the red line. The uncertainty ribbon (gray band) connects the point-wise minimum and maximum values of $K$ computed from the 99 generated point pattern samples.

Since the black line lies outside the gray band you can confidently conclude that the tornado reports are _more_ clustered than one would expect by chance.

If the specific intention is to test a null hypothesis of CSR, then a single statistic indicating the departure of $K$ computed on the observations from the theoretical $K$ is appropriate. 

One such statistic is the maximum absolute deviation (MAD) and is implemented with the `mad.test()` function from the {spatstat} package. The function performs a hypothesis test for goodness-of-fit of the observations to the theoretical model. The larger the value of the statistic, the less likely it is that the data are CSR.

```{r}
mad.test(ST.ppp, 
         fun = Kest, 
         nsim = 99)
```

The maximum absolute deviation is 7297 which is very large so the $p$-value is small and you reject the null hypothesis of CSR for these data. This is consistent with the graph. Note: Since there are 99 simulations the lowest $p$-value is .01.

Another test statistic is related to the sum of the squared deviations between the estimated and theoretical functions. It is implemented with the `dclf.test()` function.

```{r}
dclf.test(ST.ppp, 
          fun = Kest, 
          nsim = 99)
```

Again the $p$-value on the test statistic against the two-sided alternative is less than .01.

Compare these test results on tornado report clustering with test results on pine sapling clustering in the `swedishpines` data set.

```{r}
SP <- swedishpines
Kenv.df <- envelope(SP, 
                    fun = Kest, 
                    nsim = 99) |>
  as.data.frame()

ggplot(data = Kenv.df, 
       mapping = aes(x = r * .1, y = obs * intensity(SP))) +
  geom_ribbon(aes(ymin = lo * intensity(SP),
                  ymax = hi * intensity(SP)), 
              fill = "gray70") +
  geom_line() + geom_line(aes(y = theo * intensity(SP)), 
                          color = "red") +
  xlab("Lag distance (m)") + 
  ylab("K(r), Expected number of additional saplings\n within a distance r of a sapling") +
  theme_minimal()
```

At short distances (closer than about 1 m) the black line is below the red line and outside the gray ribbon which you interpret to mean that there are fewer pine saplings near other pine saplings than would be expected by chance at this scale. This 'regularity' might be the result of competition among the saplings.

At larger distances the black line is close to the red line and inside the gray ribbon which you interpret to mean that, at this larger spatial scale, the distribution of pine saplings is indistinguishable from CSR.

Based on the fact that much of the black line is within the gray envelope you might anticipate that a formal test against the null hypothesis of CSR will likely fail to reject.

```{r}
mad.test(SP, 
         fun = Kest, 
         nsim = 99)
dclf.test(SP, 
          fun = Kest, 
          nsim = 99)
```

Both return a $p$-value that is greater than .15 so you fail to reject the null hypothesis of CSR.

In the second approach to inference the procedure of re-sampling is used. Note the distinction: Re-sampling refers to generating samples from the data while sampling, as above, refers to generating samples from some theoretical model.

The bootstrap procedure is a re-sampling strategy whereby new samples are generated from the data by randomly choosing events within the domain. An event that is chosen for the 'bootstrap' sample gets the chance to be chosen again (called 'with replacement'). The number of events in each bootstrap sample must equal the number of events in the data.

Consider 15 numbers from 1 to 15. Then pick randomly from that set of numbers with replacement until the sample size is 15 to create a bootstrap sample.

```{r}
( x <- 1:15 )
sample(x, replace = TRUE)
```

Some numbers get picked more than once and some do not get picked at all.

The average of the original 15 `x` values is 8 but the average over the set of numbers in the bootstrap sample will not necessarily be 8. However, the distribution of the averages over many bootstrap samples will be centered close to this average.

```{r}
mx <- NULL
for(i in 1:99){
  mx[i] <- mean(sample(x, replace = TRUE))
}

mx.df <- as.data.frame(mx)
  ggplot(data = mx.df,
         mapping = aes(mx)) +
    geom_density() +
    geom_vline(xintercept = mean(x),
               color = "red")
```

The important thing is that the bootstrap distribution provides an estimate of the uncertainty on the computed mean through the range of possible average values.

In this way, the `lohboot()` function estimates the uncertainty on the computed spatial statistic using a bootstrap procedure. It works by computing a local version of the function (e.g., `localK()`) on the set of re-sampled events.

```{r}
Kboot.df <- ST.ppp |>
  lohboot(fun = Kest) |>
  as.data.frame()

ggplot(data = Kboot.df, 
       mapping = aes(x = r, y = iso * intensity(ST.ppp))) +
  geom_ribbon(aes(ymin = lo * intensity(ST.ppp), 
                  ymax = hi * intensity(ST.ppp)), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo * intensity(ST.ppp)), color = "red") +
  xlab("Lag distance (km)") + ylab("K(r)") +
  theme_minimal()
```

Now the uncertainty band is plotted about the black line ($K$ function computed on the observations) rather than about the null model (red line). The 95% uncertainty band does to include the CSR model so you confidently conclude that the tornadoes in Kansas are more clustered than chance.

Repeating for the Swedish pine saplings.

```{r}
Kboot.df <- SP |>
  lohboot(fun = Kest) |>
  as.data.frame()

ggplot(Kboot.df, aes(x = r * .1, y = iso * intensity(SP))) +
  geom_ribbon(aes(ymin = lo * intensity(SP), 
                  ymax = hi * intensity(SP)), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo * intensity(SP)), color = "blue", lty = 'dashed') +
  xlab("Lag distance (m)") + ylab("K(r)") +
  theme_minimal()
```

At short distances (closer than about 1.5 m) the gray ribbon is below the blue line which you interpret to mean that there are fewer pine saplings near other pine saplings than would be expected by chance at this scale indicating regularity.


## Estimating event clustering in multi-type event locations {-}

Often the interest focuses on whether the occurrence of one event type influences (or is influenced by) another event type. For example, does the occurrence of one species of tree influence the occurrence of another species?

Analogues to the $G$ and $K$ functions are available for 'multi-type' point patterns where the marks are factors.

A common statistic for examining 'cross correlation' of event type occurrences is the cross $K$ function $K_{ij}(r)$, which estimates the expected number of events of type $j$ within a distance $r$ of type $i$.

Consider the data called `lansing` from the {spatstat} package that contains the locations of 2,251 trees of various species in a wooded lot in Lansing, MI as a `ppp` object.

```{r}
data(lansing)
summary(lansing)
```

The data are a multi-type planar point pattern with the marks indicating tree species. There are 135 black oaks, 703 hickories, etc. The spatial unit is 924 feet.

Compute and plot the cross $K$ function for Maple and Hickory trees.

```{r}
Kc.df <- lansing |>
  Kcross(i = "maple",
         j = "hickory") |>
  as.data.frame()
 
ggplot(data = Kc.df, 
       mapping = aes(x = r, y = iso)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  geom_vline(xintercept = .2, lty = 'dashed') +
  geom_hline(yintercept = .093, lty = 'dashed') +
  geom_hline(yintercept = .125, lty = 'dashed') +
  xlab("Distance") + ylab("Kc(r)") +
  theme_minimal()
```

The vertical axis is the number of hickory trees within a radius r of a maple tree divided by the average intensity of the hickories. So at a distance of .2 (.2 x 924 ft = 180 ft) from a random maple there is an average of roughly 65 hickories (.093 x 703 hickories). If hickory and maple trees are CSR you would expect about 88 maples (.125 * 703) within that distance.

The presence of a hickory tree reduces the likelihood that a maple tree will be nearby.

Do the same for the EF1 and EF3 tornadoes in Kansas.

```{r}
plot(Kcross(T.ppp, 
            i = "1", 
            j = "3"))
abline(v = 70)
abline(h = 18700)
abline(h = 15500)

Kc.df <- T.ppp |>
  Kcross(i = "1", 
         j = "3") |>
  as.data.frame()
ggplot(data = Kc.df, 
       mapping = aes(x = r, y = iso)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  geom_vline(xintercept = 70, lty = 'dashed') +
  geom_hline(yintercept = 18700, lty = 'dashed') +
  geom_hline(yintercept = 15500, lty = 'dashed') +
  xlab("Distance") + ylab("Kc(r)") +
  theme_minimal()
```

The vertical axis is the number of EF3 tornadoes within a radius r of an EF1 tornado divided by the average intensity of the EF3 tornadoes. At a distance of 70 km from a random EF1 tornado there are on average 18500 x .000296 = 5.5 EF3 tornadoes. If EF1 and EF3 tornadoes are CSR then you would expect, on average, somewhat fewer EF3 tornadoes in the vicinity of EF1 tornadoes (15500 x .000296 = 4.6).

You can see this more clearly by using the `envelope()` function with the `fun = Kross`. You first use the `subset()` method with `drop = TRUE` to make a new `ppp` object with only those two groups.
```{r}
T.ppp13 <- subset(T.ppp,
                  marks == "1" |
                  marks == "3",
                  drop = TRUE)

Kcenv.df <- T.ppp13 |>
  envelope(fun = Kcross,
           nsim = 99) |>
  as.data.frame()

ggplot(data = Kcenv.df, 
       mapping = aes(x = r, y = obs)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo), color = "red", lty = 'dashed') +
  xlab("Lag distance (km)") + ylab("Kc(r)") +
  theme_minimal()
```

And you can formally test as before using the `mad.test()` function.
```{r}
mad.test(T.ppp13, fun = Kcross, nsim = 99)
dclf.test(T.ppp13, fun = Kcross, nsim = 99)
```

Both tests lead you to conclude that EF3 tornadoes are more likely near EF1 tornadoes than would be expected if they were independently CSR.

## More about the Ripley K function {-}

Compute Ripley $K$ and look at the classes of the resulting object.

```{r}
K <- Kest(T.ppp)
class(K)
```

It has two classes `fv` and `data.frame`. It is a data frame but with additional attribute information. You focus on the data frame portion. 
```{r}
K.df <- as.data.frame(K)
head(K.df)
```

In particular you  want the values of `r` and `iso`. The value of `iso` times the average spatial intensity is the number of tornadoes within a distance `r`.

You add this information to the data frame.
```{r}
K.df <- K.df |>
  dplyr::mutate(nT = summary(T.ppp)$intensity * iso)
```

Suppose you are interested in the average number of tornadoes at a distance of exactly 50 km. Use the `approx()` function to interpolate the value of `nT` at a distance of 50 km.
```{r}
approx(x = K.df$r, 
       y = K.df$nT,
       xout = 50)$y
```

Finally, the variance stabilized Ripley $K$ function called the $L$ function is often used instead of $K$.  The sample version of the $L$ function is defined as
$$
\hat{L}(r) = \Big( \hat{K}(r)/\pi\Big)^{1/2}.
$$

For data that is CSR, the $L$ function has expected value $r$ and its variance is approximately constant in $r$. A common plot is a graph of $r - \hat{L}(r)$ against $r$, which approximately follows the horizontal zero-axis with constant dispersion if the data follow a homogeneous Poisson process.

<!--chapter:end:17-Lesson.Rmd-->

# Tuesday November 1, 2022 {.unnumbered}

**"Weeks of coding can save you hours of planning."** - Unknown

Today

- Inferring event interaction from distance functions
- Removing duplicate event locations and defining the domain 
- Modeling point pattern data 
- Fitting and interpreting an inhibition model

## Inferring event interaction from distance functions {-}

The distance functions ($G$, $K$, etc) that are used to quantify clustering are defined and estimated under the assumption that the process that produced the events is stationary (homogeneous). If this is true then you can treat any sub-region of the domain as an independent and identically distributed (iid) sample from the entire set of data.

If the spatial distribution of the event locations is influenced by event interaction then the functions will deviate from the theoretical model of CSR. But a deviation from CSR does not imply event interaction. 

Moreover, the functions characterize the spatial arrangement of event locations 'on average' so variability in an interaction as a function of scale may not be detected.

As an example of the latter case, here you generate event locations at random with clustering on a small scale but with regularity on a larger scale. On average the event locations are CSR as indicated by the $K$ function.

```{r}
suppressMessages(library(spatstat))

set.seed(0112)
X <- rcell(nx = 15)
plot(X, main = "")
```

There are two 'local' clusters one in the north and one in the south. But overall the events appear to be more regular (inhibition) than CSR. 

Interpretation of the process that created the event locations based on Ripley's $K$ would be that the arrangement of events is CSR.

```{r}
library(ggplot2)

K.df <- X |>
  Kest() |>
  as.data.frame()

ggplot(K.df, aes(x = r, y = iso)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  xlab("Lag distance (km)") + ylab("K(r)") +
  theme_minimal()
```

The empirical curve (black line) coincides with the theoretical CSR line (red line) indicating CSR.

And the maximum absolute deviation test under the null hypothesis of CSR returns a large $p$-value so you fail to reject it.

```{r}
mad.test(X, fun = Kest, nsim = 99)
```

As an example of the former case, here you generate event locations that have no inter-event interaction but there is a trend in the spatial intensity.

```{r}
X <- rpoispp(function(x, y){ 300 * exp(-3 * x) })
plot(X, main = "") 
```

By design there is a clear trend toward fewer events moving toward the east.

You compute and plot the $K$ function on these event locations.

```{r}
K.df <- X |>
  Kest() |>
  as.data.frame()

ggplot(K.df, aes(x = r, y = iso)) +
  geom_line() +
  geom_line(aes(y = theo), color = "red") +
  xlab("Lag distance (km)") + ylab("K(r)") +
  theme_minimal()
```

The $K$ function indicates clustering but this is an artifact of the trend in the intensity.

In the case of a known trend in the spatial intensity, you need to use the `Kinhom()` function. For example, compare the uncertainty envelopes from a homogeneous and inhomogeneous Poisson process. 

Start by plotting the output from the `envelope()` function with `fun = Kest`. The `global = TRUE` argument indicates that the envelopes are simultaneous rather than point-wise (`global = FALSE` which is the default). Point-wise envelopes assume the estimates are independent (usually not a good assumption) across the range of distances so the standard errors will be smaller resulting in narrower bands.

```{r}
Kenv <- envelope(X, 
                 fun = Kest, 
                 nsim = 39, 
                 rank = 1, 
                 global = TRUE)
Kenv.df <- as.data.frame(Kenv)

ggplot(Kenv.df, aes(x = r, y = obs)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo), color = "red", lty = 'dashed') +
  xlab("Lag distance (km)") + ylab("K(r)") +
  theme_minimal()
```

After a distance of about .15 units the empirical curve (black line) is outside the uncertainty band indicating the events are more clustered than CSR.

However when you use the `fun = Kinhom` the empirical curve is completely inside the uncertainty band.

```{r}
Kenv <- envelope(X, 
                 fun = Kinhom, 
                 nsim = 99, 
                 rank = 1, 
                 global = TRUE)
Kenv.df <- as.data.frame(Kenv)

ggplot(Kenv.df, aes(x = r, y = obs)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo), color = "red", lty = 'dashed') +
  xlab("Lag distance (km)") + ylab("K(r), Expected number of additional events\n within a distance r of an event") +
  theme_minimal()
```

You conclude that the point pattern data are consistent with an inhomogeneous Poisson process without event interaction.

Let's return to the Kansas tornadoes (EF1+). You import the data and create a point pattern object windowed by the state borders.

```{r}
Torn.sf <- sf::st_read(dsn = here::here("data", "1950-2020-torn-initpoint")) |>
  sf::st_transform(crs = 3082) |>
  dplyr::filter(mag >= 1, yr >= 1994) |>
  dplyr::mutate(EF = as.factor(mag)) |>
  dplyr::select(EF)

ST.ppp <- Torn.sf["EF"] |>
  as.ppp()

KS.sf <- USAboundaries::us_states(states = "Kansas") |>
  sf::st_transform(crs = sf::st_crs(Torn.sf)$proj4string)

W <- KS.sf |>
  as.owin()

ST.ppp <- ST.ppp[W] |>
  spatstat.geom::rescale(s = 1000, 
                         unitname = "km")
plot(ST.ppp)
```

There are more tornado reports in the west than in the east, especially across the southern part of the state indicating the process producing the events is not homogeneous. This means there are other factors contributing to local event intensity.

Evidence for clustering must account for this inhomogeneity. Here you do this by computing the envelope around the inhomogeneous Ripley K function using the argument `fun = Kinhom`.
```{r}
Kenv <- envelope(ST.ppp,
                 fun = Kinhom,
                 nsim = 39,
                 rank = 1,
                 global = TRUE)

Kenv.df <- as.data.frame(Kenv)

ggplot(Kenv.df, aes(x = r, y = obs)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo), color = "red", lty = 'dashed') +
  xlab("Lag distance (km)") + ylab("K(r)") +
  theme_minimal()
```

The output reveals no evidence of clustering at distances less than about 70 km. At greater distances there is evidence of regularity indicated by the black line significantly below the red line. This is due to the fact that tornado reports are more common near cities and towns and cities and towns tend to be spread out more regular than CSR.

## Removing duplicate event locations and defining the domain {-}

The functions in the {spatstat} package require the event locations (as a `ppp` object) and a domain over which the spatial statistics are computed (as an `owin` object).

If no `owin` object is specified, the statistics are computed over a rectangle (bounding box) defined by the northern most, southern most, eastern most, and western most event locations.

To see this, consider the Florida wildfire data as a simple feature data frame. Extract only fires occurring in Baker County (west of Duval County--Jacksonville). Include only wildfires started by lightning and select the fire size variable.
```{r}
FL_Fires.sf <- sf::st_read(dsn = here::here("data", "FL_Fires")) |>
  sf::st_transform(crs = 3086)

Baker.sf <- USAboundaries::us_counties(states = "FL") |>
  dplyr::select(name) |>
  dplyr::filter(name == "Baker") |>
  sf::st_transform(crs = 3086)

BakerFires.sf <- FL_Fires.sf |>
  sf::st_intersection(Baker.sf) |>
  dplyr::filter(STAT_CAU_1 == "Lightning") |>
  dplyr::select(FIRE_SIZE_)
```

Create a `ppp` object and an unmarked `ppp` object. Summarize the unmarked object and make a plot.
```{r}
BF.ppp <- BakerFires.sf |>
  as.ppp() 

BFU.ppp <- unmark(BF.ppp)

summary(BFU.ppp)
plot(BFU.ppp)
```

The average intensity is 18 wildfires per 10 square km. But the intensity is based on a square domain. The lack of events in the northeast part of the domain is due to the fact that you removed wildfires outside the county border.

Further, two event locations are identical if their x,y coordinates are the same, and their marks are the same (if they carry marks).

Remove duplicate events with the `unique()` function, set the domain to be the county border, and set the name for the unit of length to meters.

```{r}
BFU.ppp <- unique(BFU.ppp)

W <- Baker.sf |>
  as.owin()

BFU.ppp <- BFU.ppp[W]

unitname(BFU.ppp) <- "meters"

summary(BFU.ppp)
plot(BFU.ppp)
```

Now the average intensity is 21 wildfires per 10 sq. km.

Apply Ripley's $K$ function and graph the results.
```{r}
K.df <- BFU.ppp |>
  Kest() |>
  as.data.frame()

ggplot(K.df, aes(x = r, y = iso * intensity(BFU.ppp))) +
  geom_line() +
  geom_line(aes(y = theo * intensity(BFU.ppp)), color = "red") +
  xlab("Lag distance (m)") + ylab("K(r), Expected number of additional wildfires\n within a distance r of any wildfire") +
  theme_minimal()
```

We see a difference indicating a cluster of event locations, but is the difference significant against a null hypothesis of a homogeneous Poisson?
```{r}
Kenv.df <- envelope(BFU.ppp, 
                    fun = Kest, 
                    nsim = 39, 
                    rank = 1, 
                    global = TRUE) |>
  as.data.frame()

ggplot(Kenv.df, aes(x = r, y = obs)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), fill = "gray70") +
  geom_line() +
  geom_line(aes(y = theo), color = "red", lty = 'dashed') +
  xlab("Lag distance (m)") + ylab("K(r)") +
  theme_minimal()
```

Yes it is.

## Modeling point pattern data {-}

Models are helpful for trying to understanding the processes leading to the event locations when event interaction is suspected. Event interaction means that an event at one location changes the probability of an event nearby.

Cluster models can be derived by starting with a Poisson model. For example, you begin with a homogeneous Poisson model $Y$ describing a set of events. A model is homogeneous Poisson when the event locations generated by the model are CSR.

Then consider each individual event $y_i$ in $Y$ to be a 'parent' that produces a set of 'offspring' events ($x_i$) according to some mechanism. The resulting set of offspring forms clustered point pattern data $X$. Said another way, the model is homogeneous Poisson at an unobserved level $Y$ (latent level) but clustered at the level of the observations ($X$).

One example of this parent-child process is the Matern cluster model. Parent events come from a homogeneous Poisson process with intensity $\kappa$ and then each parent has a Poisson ($\mu$) number of offspring that are iid within a radius $r$ centered on the parent.

For instance here you use the `rMatClust()` function from the {spatstat} package to produce a clustered `ppp` object. We use a disc radius of .1 units and an offspring rate equal to 5 (`mu = 5`).
```{r}
rMatClust(kappa = 10, 
               r = .1, 
               mu = 5) |>
  plot(main = "")
```

The result is a set of event locations and the process that produced them is  described as _doubly Poisson_. You can vary $\kappa$, $r$, and $\mu$ to generate more or fewer events.

Other clustered Poisson models include:
- Thomas model: each cluster consists of a Poisson number of random events with each event having an isotropic Gaussian displacement from its parent.  
- Gauss-Poisson model: each cluster is either a single event or a pair of events.  
- Neyman-Scott model: the cluster mechanism is arbitrary.

A Cox model is a homogeneous Poisson model with a random intensity function. Let $\Lambda(s)$ be a function with non-negative values defined at all locations $s$ inside the domain. Then, conditional on $\Lambda$ let $X$ be a Poisson model with an intensity function $\Lambda$. Then $X$ will be a sample from a Cox model.

An example of a Cox model is the mixed Poisson process in which a random variable $\Lambda$ is generated and then, conditional on $\Lambda$, a homogeneous Poisson process with intensity $\Lambda$ is generated. 

Following are two samples from a Cox point process.

```{r}
set.seed(3042)
par(mfrow = c(1, 2))
for (i in 1:2){
  lambda <- rexp(n = 1, rate = 1/100)
  X <- rpoispp(lambda)
  plot(X)
}
par(mfrow = c(1, 1))
```

The statistical moments of Cox models are defined in terms of the moments of $\Lambda$. For instance, the intensity function of $X$ is $\lambda(s)$ = E[$\Lambda(s)$], where E[] is the expected value.

Cox models are convenient for describing clustered point pattern data. A Cox model is over-dispersed relative to a Poisson model (i.e. the variance of the number of events falling in any region of size A, is greater than the mean number of events in those regions). The Matern cluster model and the Thomas models are Cox models. Another common type of a Cox model is the log-Gaussian Cox processes (LGCP) model in which logarithm of $\Lambda(s)$ is a Gaussian random function.

If you have a way of generating samples from a random function $\Lambda$ of interest, then you can use the `rpoispp()` function to generate the Cox process. The intensity argument `lambda` of `rpoispp()` can be a function of x or y or a pixel image.

Another way to generate clustered point pattern data is by 'thinning'. Thinning refers to deleting some of the events. With 'independent thinning' the fate of each event is independent of the fate of the other events. When independent thinning is applied to a homogeneous Poisson point pattern, the resulting point pattern consisting of the retained events is also Poisson. 
To simulate a inhibition process you can use a 'thinning' mechanism.

An example of this is Matern's Model I model. Here a homogeneous Poisson model first generates a point pattern $Y$, then any event in $Y$ that lies closer than a distance $r$ from another event is deleted. This results in point pattern data whereby close neighbor events do not exist.
```{r}
plot(rMaternI(kappa = 70, 
              r = .05), main = "")

X <- rMaternI(kappa = 70, 
              r = .05)

X |>
  Kest() |>
  plot()
```

Changing $\kappa$ and $r$ will change the event intensity.

The various spatial models for event locations can be described with math. For instance, expanding on the earlier notation you write that a homogeneous Poisson model with intensity $\lambda > 0$ has intensity $$\lambda(s, x) = \lambda$$ where $s$ is any location in the window W and $x$ is the set of events.

Then the inhomogeneous Poisson model has conditional intensity $$\lambda(s, x) = \lambda(s)$$. The intensity $\lambda(s)$ depends on a spatial trend or on an explanatory variable.

There is also a class of 'Markov' point process models that allow for clustering (or inhibition) due to event interaction. Markov refers to the fact that the interaction is limited to nearest neighbors. Said another way, a Markov point process generalizes a Poisson process in the case where events are pairwise dependent.

A Markov process with parameters $\beta > 0$ and $0 < \gamma < \infty$ with interaction radius $r > 0$ has conditional intensity $\lambda(s, x)$ given by

$$
\lambda(s, x) = \beta \gamma^{t(s, x)}
$$

where $t(s, x)$ is the number of events that lie within a distance $r$ of location $s$.

Three cases:
- If $\gamma = 1$, then $\lambda(s, x) = \beta$ No interaction between events,  $\beta$ can vary with $s$.
- If $\gamma < 1$, then $\lambda(s, x) < \beta$. Events inhibit nearby events.
- If $\gamma > 1$, then $\lambda(s, x) > \beta$. Events encourage nearby events.

Note the distinction between the interaction term $\gamma$ and the trend term $\beta$. Note: A similar distinction exists between autocorrelation $\rho$ and trend $\beta$ in spatial regression models.

More generally, you write the logarithm of the conditional intensity $\log[\lambda(s, x)]$ as linear expression with two components.

$$
\log\big[\lambda(s, x)\big] = \theta_1 B(s) + \theta_2 C(s, x)
$$

where the $\theta$'s are model parameters that need to be estimated.  

The term $B(s)$ depends only on location so it represents trend and explanatory variable (covariate) effects. It is the 'systematic component' of the model. The term $C(s, x)$ represents stochastic interactions (dependency) between events.

## Fitting and interpreting an inhibition model {-}

The {spatstat} package contains functions for fitting statistical models to point pattern data. Models can include trend (to account for non-stationarity), explanatory variables (covariates), _and_ event interactions of any order (in other words, interactions are not restricted to pairwise). Models are fit with the method of maximum likelihood and the method of minimum contrasts.

The method of maximum likelihood estimates the probability of the empirical $K$ curve given the theoretical curve for various parameter values. Parameter values are chosen so as to maximize the likelihood of the empirical curve.

The method of minimum contrasts derives a cost function as the difference between the theoretical and empirical $K$ curves. Parameter values for the theoretical curve are those that minimize this cost function.

The `ppm()` function is used to fit a spatial point pattern model. The syntax has the form `ppm(X, formula, interaction, ...)` where `X` is the point pattern object of class `ppp`, `formula` describes the systematic (trend and covariate) part of the model, and `interaction` describes the stochastic dependence between events (e.g., Matern process).

Recall a plot of the Swedish pine saplings. There was no indication of a trend (no systematic variation in the intensity of saplings).

```{r}
SP <- swedishpines
plot(SP)

intensity(SP)
```

There is no obvious spatial trend in the distribution of saplings and the average intensity is .0074 saplings per unit area.

A plot of the Ripley's $K$ function indicated regularity relative to CSR.

```{r}
SP |>
  Kest(correction = "iso") |>
  plot()
```

The red dashed line is the $K$ curve under CSR. The black line is the empirical curve. At lag distances of between 5 and 15 units the empirical curve is below the CSR curve indicating there are fewer events within other events at those scales than would be expected by chance.

This suggests a physical process whereby saplings tend to compete for sunlight, nutrients, etc. A process of between-event inhibition. If you suspect that the spatial distribution of event locations is influenced by inhibition you can model the process statistically.

A simple inhibition model is a Strauss process when the inhibition is constant with a fixed radius (r) around each event. The amount of inhibition ranges between zero (100% chance of a nearby event) to complete (0% chance of a nearby event). In the case of no inhibition the process is equivalent to a homogeneous Poisson process.

If you assume the inhibition process is constant across the domain with a fixed interaction radius (r), then you can fit a Strauss model to the data. You use the `ppm()` function from the {spatstat} package and include the point pattern data as the first argument. You set the trend term to a constant (implying a stationary process) with the argument `trend ~ 1` and the interaction radius to 10 units with the argument `interaction = Strauss(r = 10)`. Finally you use a border correction out to a distance of 10 units from the window with the `rbord =` argument.

Save the output in the object called `model.in` (inhibition model).

```{r}
model.in <- ppm(SP, 
                trend = ~ 1, 
                interaction = Strauss(r = 10), 
                rbord = 10)
```

The value for `r` in the `Strauss()` function is based on our visual inspection of the plot of `Kest()`. A value is chosen that represents the distance at which there is the largest departure from a CSR model. 

You inspect the model parameters by typing the object name.

```{r}
model.in
```

The first-order term (`beta`) has a value of .0757. This is the intensity of the 'proposal' events. The value of beta exceeds the average intensity by a factor of ten. 

Recall the intensity of the events is obtained as

```{r}
intensity(SP)
```

The interaction parameter (`gamma`) is .275. It is less than one, indicating an inhibition process. The logarithm of gamma, called the interaction coefficient (`Interaction`), is -1.29. Interaction coefficients less than zero imply inhibition.

A table with the coefficients including the standard errors and uncertainty ranges is obtained with the `coef()` method.
```{r}
model.in |>
  summary() |>
  coef()
```

The output includes the `Interaction` coefficient along with it's standard error (`S.E.`) and the associated 95% uncertainty interval. The ratio of the `Interaction` coefficient to its standard error is the `Zval`. A large z-value (in absolute magnitude) translates to a low $p$-value and a rejection of the null hypothesis of no interaction between events.

Output also is the estimated value for the `(Intercept)` term. It is the logarithm of the beta value, so exp(-2.58) = .0757 is the intensity of the proposal events.

You interpret the model output as follows. The process producing the spatial pattern of pine saplings is such that you should see .0757 saplings per unit area [unobserved (latent) rate]. 

But because of event inhibition, where saplings nearby other saplings fail to grow, the number of saplings is reduced to .0074 per unit area. Thus the spatial pattern is suggestive of sibling-sibling interaction. Adults have many offspring, but only some survive due to limited resources.

<!--chapter:end:18-Lesson.Rmd-->

# Thursday November 3, 2022 {.unnumbered}

**"Sometimes it pays to stay in bed on Monday, rather than spending the rest of the week debugging Monday's code."** - Christopher Thompson

Today

- Fitting and interpreting a cluster model
- Assessing how well the model fits
- Spatial logistic regression

## Fitting and interpreting a cluster model {-}

Let's compare the inhibition model fit previously to describe the Swedish pine saplings data with a cluster model for describing the Lansing Woods maple trees (in the `ppp` object called `lansing` from the {spatstat} package).

Start by extracting the events marked as `maple` and putting them in a separate `ppp` object called `MT`.

```{r}
suppressMessages(library(spatstat))
data(lansing)
summary(lansing)

MT <- lansing |>
  subset(marks == "maple") |>
  unmark()

summary(MT)
```

There are 514 maple trees over this square region (924 x 924 square feet).

Plots of the tree locations and the local intensity function help you understand the first-order property of these data.

```{r}
MT |>
  density() |>
  plot()

plot(MT, add = TRUE)
```

There are maple trees across the southern and central parts of the study domain.

A plot of the $G$ function summarizes the second-order properties under the assumption of no trend.

```{r}
library(ggplot2)

G.df <- MT |>
  Gest() |>
  as.data.frame() |>
  dplyr::filter(r < .033) |>
  dplyr::mutate(r = r * 924)

ggplot(G.df, aes(x = r, y = km)) +
  geom_line() +
  geom_line(aes(y = theo), color = "blue") +
  geom_vline(xintercept = 18, lty = 'dashed') +
  xlab("Lag distance (ft)") + ylab("G(r): Cumulative % of events within a distance r of another maple") +
  theme_minimal()
```

The plot provides evidence that the maple trees are clustered. The empirical curve is above the theoretical curve. For example about 74% of the maple trees are within 18 feet of another maple tree (vertical blue line). If the trees were arranged as CSR then only 49% of the trees would be within 18 feet of another maple.

Is the clustering due to interaction or trends (or both)?

You start the modeling process by investigating event interaction using a stationary Strauss model with interaction radius of .019 units (18 ft).

```{r}
ppm(MT, 
    trend = ~ 1, 
    interaction = Strauss(r = .019))
```

Here the first order term beta is 345. It is the 'latent' rate (intensity) of maple trees per unit area. This rate is less than the 514 actual maple trees. The fitted interaction parameter (gamma) is 1.72. It is greater than one since the trees are clustered. The logarithm of gamma is positive at .545.

The model is interpreted as follows. The process producing the maple trees is such that you expect to see about 345 maples. Because of clustering where maple trees are more likely in the vicinity of other maple trees, the number of maples increases to the observed 514 per unit area.

Here the physical explanation could be event interaction. But it also could be explained by inhibition with hickory trees. You can model this using a term for cross event type interaction.

The Strauss process is for inhibition models. So although you use it here for diagnostics, you need to fit a cluster model (thus the `*** Model is not valid ***` warning).

For a cluster model the spatial intensity $$\lambda(s) = \kappa \mu(s)$$ where $\kappa$ is the average number of clusters and where $\mu(s)$ is the spatial varying cluster size (number events per cluster).

Cluster models are fit using the `kppm()` function from the {spatstat} package. Here you specify the cluster process with `clusters = "Thomas"`. 

That means each cluster consists of a Poisson number of maple trees and where each tree in the cluster is placed randomly about the 'parent' tree with intensity that varies inversely with distance from the parent as a Gaussian function.

```{r}
( model.cl <- kppm(MT, 
                   trend = ~ 1,
                   clusters = "Thomas") )
```

Here $\kappa$ is 21.75 and $\bar \mu(s)$ (mean cluster size) is 23.6 trees. The product of kappa and the mean cluster size is the number of events. The cluster model describes a parent-child process. The number of parents is about 22. The distribution of the parents can be described as CSR. Each parent produces about 24 offspring distributed randomly about the location of the parent within a characteristic distance. Note: The physical process might be different from the statistical process used to describe it.

The cluster scale parameter indicating the characteristic size (area units) of the clusters is $\sigma^2$. 

A `plot()` method verifies that the cluster process statistically 'explains' the spatial correlation.

```{r}
plot(model.cl, 
     what = "statistic")
```

The model (black line) is very close to the cluster process line (red dashed line). Also note that it is far from the CSR model (green line).

The spatial scale of the clustering is visualized with the `what = "cluster"` argument.
```{r}
plot(model.cl, 
     what = "cluster")
```

The color ramp is the spatial intensity (number of events per unit area) about an arbitrary single event revealing the spatial scale and extent of clustering.

## Assessing how well the model fits {-}

Workflow in fitting spatial event location models

- Analyze/plot the intensity and nearest neighbor statistics
- Select a model including trend, interaction distance, etc informed by the results of step 1
- Choose an inhibition or cluster model
- Fit the model to the event pattern
- Assess how well the model fits the data by generating samples and comparing statistics from the samples with the statistics from the original data

The model should be capable of generating samples of event locations that are statistically indistinguishable from the actual event locations.

Note: The development of spatial point process methods has largely been theory driven (not by actual problems/data). More work needs to be done to apply the theory to environmental data with spatial heterogeneity, properties at the individual level (marks), and with time information.

You produce samples of event locations with the `simulate()` function applied to the model object. 

Let's return to the Swedish pine sapling data and the inhibition model.

```{r}
SP <- swedishpines
model.in <- ppm(SP, 
                trend = ~ 1, 
                interaction = Strauss(r = 10), 
                rbord = 10)
```

Here you generate three samples of the Swedish pine sapling data and plot them alongside the actual data for comparison.

```{r}
X <- model.in |>
  simulate(nsim = 3)

par(mfrow = c(2, 2))
plot(SP) 
plot(X[[1]])
plot(X[[2]])
plot(X[[3]])
```

The samples of point pattern data look similar to the actual data providing evidence that the inhibition model is adequate. 

To quantitatively assess the similarity use the `envelope()` function that computes the $K$ function on 99 samples and the actual data. The $K$ function values are averaged over all samples and a mean line represents the best model curve. Uncertainty is assessed with a band that ranges from the minimum to the maximum K at each distance.

Do this with the inhibition model for the pine saplings. This takes a few seconds to complete.

```{r}
par(mfrow = c(1, 1))

plot(envelope(model.in, 
              fun = Kest, 
              nsim = 99, 
              correction = 'border'), legend = FALSE)
```

The black line is the empirical (data) curve and the red line is the average over the 99 samples. The two lines are close and the black line falls nearly completely within the gray uncertainty band indicating the model fits the data well. The kink in the red curve is the result of specifying 10 units for the interaction distance. 

From this plot you confidently conclude that a homogeneous inhibition model is adequate for describing the pine sapling data.

What about the model for the maple trees? The model is saved as `model.cl`.

```{r}
plot(envelope(model.cl, 
              fun = Kest, 
              nsim = 99, 
              correction = 'border'), legend = FALSE)
```

In the case of the maple trees, a cluster model is adequate. However, it is not satisfying since you know about the potential for inhibition caused by the presence of hickory trees. 

Also you saw that there were more trees in the south than in the north so the stationary assumption is suspect.

You fit a second cluster model where the intensity is a linear function of distance in the north-south direction.

```{r}
model.cl2 <- kppm(MT, 
                  trend = ~ y,
                  clusters = "Thomas")
model.cl2
```

This is an inhomogeneous cluster point process model. The logarithm of the intensity depends on y (`Log intensity:  ~y`). The fitted trend coefficient is negative as expected, since there are fewer trees as you move north (increasing y direction). There is one spatial unit in the north-south direction so you interpret this coefficient to mean there are 77% fewer trees in the north than in the south. The 77% comes from the formula 1 - exp(-1.486) = .77.

The average number of clusters (`kappa`) is higher at about 27 (it was 22 for stationary model). The cluster scale parameter (`sigma`), indicating the characteristic size of the cluster (in distance units) is lower at .0536. That makes sense since some of the event-to-event distance is accounted for by the trend term.

Simulate data using the new model and compare the inhomogenous $K$ function between the simulations and the observed data.

```{r}
plot(envelope(model.cl2, 
              fun = Kinhom, 
              nsim = 99,
              correction = 'border'), legend = FALSE)
```

The black line falls within the gray band and the gray band is narrower than the simulations using the homogeneous cluster model.

Tropical trees

If the intensity of events depends on spatial location as it does with the maple trees you can include a trend and covariate term in the model.

For a trend term, the `formula ~ x` corresponds to a spatial trend of the form $\lambda(x) = \exp(a + bx)$, while `~ x + y` corresponds to $\lambda(x, y) = \exp(a + bx + cy)$ where `x`, `y` are the spatial coordinates. For a covariates, the formula is `~ covariate1 + covariate2`.

Consider the `bei` data from the {spatstat} package containing the locations of 3605 trees in a tropical rain forest.

```{r}
plot(bei)
```

Accompanied by covariate data giving the elevation (altitude) and slope of elevation in the study region. The data `bei.extra` is a list containing two pixel images, `elev` (elevation in meters) and `grad` (norm of elevation gradient). These pixel images are objects of class `im`, see `im.object`.

```{r}
image(bei.extra)
```

Compute and plot the $K$ function on the `ppp` object `bei`.
```{r}
plot(envelope(bei, 
              fun = Kest, 
              nsim = 39, 
              global = TRUE, 
              correction = "border"), 
     legend = FALSE)
```

There is significant clustering indicated by the black line sitting far above the CSR line. There are more trees in the vicinity of other trees than expected by chance.

But how much of the clustering is due to variations in terrain?

You start by fitting a model that includes elevation and gradient as covariates without clustering. This is done with the `trend =` argument naming the image variables and including the argument `covariates =` indicating a data frame or, in this case, a list whose entries are image functions.

```{r}
model1 <- ppm(bei, 
              trend = ~ elev + grad, 
              covariates = bei.extra)
```

Check to see if elevation and gradient as explanatory variables are significant in the model.

```{r}
summary(model1)
```

The output shows that both elevation and elevation gradient are significant in explaining the spatial varying intensity of the trees. 

Since the conditional intensity is on a log scale you interpret the elevation coefficient as follows: For a one meter increase in elevation the local spatial intensity increases by a amount equal to exp(.021) or 2%.

Check how well the model fits the data. Again this is done with the `envelope()` function using the model object as the first argument.
```{r}
E <- envelope(model1, 
              fun = Kest, 
              nsim = 39,
              correction = "border",
              global = TRUE)
plot(E, main = "Inhomogeneous Poisson Model", 
     legend = FALSE)
```

You conclude that although elevation and elevation slope are significant in explaining the spatial distribution of trees, they do not explain all the clustering.

An improvement is made by adding a cluster process to the model. This is done with the function `kppm()`.

```{r}
model2 <- kppm(bei, 
               trend = ~ elev + grad, 
               covariates = bei.extra, 
               clusters = "Thomas")
E <- envelope(model2, Lest, nsim = 39, 
             global = TRUE, 
             correction = "border")
plot(E, main = "Clustered Inhomogeneous Model", legend = FALSE)
```

The uncertainty band is much wider. The empirical curve fits completely inside the band so you conclude that an inhomogeneous cluster process appears to be an adequate description of the point pattern data.

Violent tornadoes

The vast majority of tornadoes have winds of less than 60 m/s (120 mph). A violent tornado, with winds exceeding 90 m/s, is rare. Most of these potentially destructive and deadly tornadoes occur from rotating thunderstorms called supercells, with formation contingent on local (storm-scale) meteorological conditions. 

The long-term risk of a tornado at a given location is assessed using historical records, however, the rarity of the most violent tornadoes make these rate estimates unstable. Here you use the more stable rate estimates from the larger set of less violent tornadoes to create more reliable estimates of violent tornado frequency.

For this exercise attention is restricted to tornadoes occurring in Kansas over the period 1954--2020.
```{r}
Torn.sf <- sf::st_read(dsn = here::here("data", "1950-2020-torn-initpoint")) |>
  sf::st_transform(crs = 3082) |>
  dplyr::filter(mag >= 0, yr >= 1954) |>
  dplyr::mutate(EF = mag,
                EFf = as.factor(EF)) |>
  dplyr::select(yr, EF, EFf)

W.sfc <- USAboundaries::us_states(states = "Kansas") |>
  sf::st_transform(crs = sf::st_crs(Torn.sf)) |>
  sf::st_geometry()

Torn.sf <- Torn.sf[W.sfc, ]
```

Create a `owin` and `ppp` objects. Note that although you already subset by Kansas tornadoes above you need to subset on the `ppp` object to assign the KS boundary as the analysis window.

```{r}
KS.win <- W.sfc |>
  as.owin()

T.ppp <- Torn.sf["EF"] |>
  as.ppp()

T.ppp <- T.ppp[KS.win]

summary(T.ppp)
```

There are 4139 tornadoes over the period with an average intensity of 192 per 100 square kilometer (multiply the average intensity in square meters by 10^10).

Separate the point pattern data into non-violent tornadoes and violent tornadoes. The non-violent tornadoes include those with an EF rating of 0, 1, 2 or 3. The violent tornadoes include those with an EF rating of 4 or 5.

```{r}
NV.ppp <- T.ppp |>
  subset(marks <= 3 & marks >= 0) |>
  unmark()

summary(NV.ppp)

V.ppp <- T.ppp |>
  subset(marks >= 4) |> 
  unmark()

summary(V.ppp)
```

The spatial intensity of the non-violent tornadoes is 190 per 100 sq km. The spatial intensity of the violent tornadoes is 1.9 per 100 square kilometer.

Plot the locations of the violent tornado events.

```{r}
plot(V.ppp)
```

Early we found that the spatial intensity of tornado reports was a function of distance to nearest city. 

So here you include this as an explanatory variable. Import the data, set the CRS, and transform the CRS to match that of the tornadoes. Exclude cities with fewer than 1000 people.

```{r}
C.sf <- USAboundaries::us_cities() |>
  dplyr::filter(population >= 1000) |>
  sf::st_transform(crs = sf::st_crs(Torn.sf))
```

Then convert the simple feature data frame to a `ppp` object. Then subset the events by the analysis window (Kansas border).

```{r}
C.ppp <- C.sf |>
  as.ppp()

C.ppp <- C.ppp[KS.win] |>
  unmark()

plot(C.ppp)
```

Next create a distance map of the city locations using the `distmap()` function.

```{r}
Zc <- distmap(C.ppp)
plot(Zc)
```

The pixel values of the `im` object are distances is meters. Blue indicates locations that are less than 20 km from a city.

Interest lies with the distance to nearest non-violent tornado. You check to see if this might be a useful variable in a model so you make a distance map for the non-violent events and then use the `rhohat()` function.

```{r}
Znv <- distmap(NV.ppp)
rhat <- rhohat(V.ppp, Znv, 
               adjust = 1.5, 
               smoother = "kernel", 
               method = "transform")

dist <- rhat$Znv
rho <- rhat$rho
hi <- rhat$hi
lo <- rhat$lo
Rho.df <- data.frame(dist = dist, rho = rho, hi = hi, lo = lo)

ggplot(Rho.df) + 
  geom_ribbon(aes(x = dist, ymin = lo, ymax = hi), alpha = .3) + 
  geom_line(aes(x = dist, y = rho), col = "black") + 
  ylab("Spatial intensity of violent tornadoes") + xlab("Distance from nearest non-violent tornado (m)") + 
  theme_minimal()
```

This shows that regions that get non-violent tornadoes also see higher rates of violent tornadoes.

So the model should include two covariates (trend terms), distance to nearest city and distance to nearest non-violent tornado.

```{r}
model1 <- ppm(V.ppp, 
              trend = ~ Zc + Znv, 
              covariates = list(Zc = Zc, Znv = Znv))

coef(summary(model1))
```

As expected the model shows fewer violent tornadoes with increasing distance from the nearest city (negative coefficient on `Zc`) and fewer violent tornadoes with increasing distance from a non-violent tornado (negative coefficient on `Znv`).

Since the spatial unit is meters the coefficient of -3.06e-05 is interpreted as a [1 - exp(-.0306)] * 100% or 3% decrease in violent tornado reports per kilometer of distance from a city. Similarly the coefficient on distance from nearest non-violent tornado is interpreted as a 23% decrease in violent tornado reports per kilometer of distance from nearest non-violent tornado.

Check if there is any residual nearest neighbor correlation.

```{r}
E <- envelope(model1, 
              fun = Kest, 
              nsim = 39,
              global = TRUE)
plot(E, main = "Inhomogeneous Poisson Model", legend = FALSE)
```

There appears to be a bit of regularity at smaller scales. The empirical curve (black line) falls slightly below the model (dashed red line). There are fewer nearby violent tornadoes than one would expect.

To see if this is statistically significant, you add an inhibition process to the model.
```{r}
model2 <- ppm(V.ppp, 
              trend = ~ Zc + Znv, 
              covariates = list(Zc = Zc, Znv = Znv),
              interaction = Strauss(r = 40000))

coef(summary(model2))
```

The interaction coefficient has a negative sign as expected from the above plot, but the standard error is relatively large so it is not significant.

Remove the inhibition process and add a trend term in the east-west direction.

```{r}
model3 <- ppm(V.ppp, 
              trend = ~ Zc + Znv + x, 
              covariates = list(Zc = Zc, Znv = Znv))

coef(summary(model3))
```

There is a significant eastward trend but it appears to confound the distance to city term. Why is this? 

Plot simulated data.
```{r}
plot(V.ppp)
plot(simulate(model1, nsim = 6))
```

Model one appears to due a good job simulating data that looks like the actual data.

## Spatial logistic regression {-}

Spatial logistic regression is a popular model for point pattern data. The study domain is divided into a grid of cells; each cell is assigned the value one if it contains at least one event, and zero otherwise.

Then a logistic regression models the presence probability $p = P(Y = 1)$ as a function of explanatory variables $X$ in the (matrix) form $$
\log \frac{p}{1-p} = \beta X
$$ where the left-hand side is the logit (log of the odds ratio) and the $\beta$ are the coefficients to be determined.

If your data are stored as `ppp` objects, a spatial logistic model can be fit directly using functions from the {spatstat} package.

Let's consider an example from the package (a good strategy in general when learning a new technique).

Consider the locations of 57 copper ore deposits (events) and 146 line segments representing geological 'lineaments.' Lineaments are linear features that consist of geological faults.

It is of interest to be able to predict the probability of a copper ore from the lineament pattern. The data are stored as a list in `copper`. The list contains a `ppp` object for the ore deposits and a `psp` object for the lineaments.

```{r}
data(copper)
plot(copper$SouthPoints)
plot(copper$SouthLines, add = TRUE)
```

For convenience you first rotate the events (points and lines) by 90 degrees in the anticlockwise direction and save them as separate objects.

```{r}
C <- rotate(copper$SouthPoints, pi/2)
L <- rotate(copper$SouthLines, pi/2)
plot(C)
plot(L, add = TRUE)
```

You summarize the planar point pattern data object `C`.

```{r}
summary(C)
```

There are 57 ore deposits over a region of size 5584 square km resulting in an intensity of about .01 ore deposits per square km.

Next you create a distance map of the lineaments to be used as a covariate.

```{r}
D <- distmap(L)
plot(D)
```

Spatial logistic regression models are fit with the `slrm()` function from the {spatstat} package.

```{r}
model.slr <- slrm(C ~ D)
model.slr
```

The model says that the odds of a copper ore along a lineament (D = 0) is exp(-4.723) = .00888. This is slightly less than the overall intensity of .01.

The model also says that for every one unit (one kilometer) increase in distance from a lineament the expected change in the log odds is .0781 [exp(.0781) = 1.0812] or an 8.1% increase in the odds. Ore deposits are more likely between the lineaments.

The fitted method produces an image (raster) of the window giving the local probability of an ore deposit. The values are the probability of a random ore deposit in each pixel.

```{r}
plot(fitted(model.slr))
plot(C, add = TRUE)
```

Integrating the predictions over the area equals the observed number of ore deposits.

```{r}
sum(fitted(model.slr))
```

<!--chapter:end:19-Lesson.Rmd-->

# Thursday November 10, 2022 {.unnumbered}

**"Beyond basic mathematical aptitude, the difference between good programmers and great programmers is verbal ability."** – Marissa Mayer

Today

- Spatial data interpolation
- Computing the sample (empirical) variogram

## Spatial data interpolation {-}

In situ observations of the natural world are made at specific locations in space (and time). But we often want estimates of the values everywhere. The temperature reported at the airport is 15C, but what is it at my house 10 miles away? 

We need to interpolate values observed at certain locations to values anywhere over the domain. To do this we assume the observations are taken from a _continuous_ field (surface). Data observed or measured at locations across a continuous field are called geostatistical data. Examples: concentrations of heavy metals across a farm field, surface air pressures in cities across the country, air temperatures within a city during the night.

Local averaging, spline functions, or inverse-distance weighting are interpolation methods. If it is 20C five miles north of here and 30C files miles to the south, then it is 25C here. This type of interpolation is a reasonable first-order assumption. But these types of interpolation methods do not take into account spatial autocorrelation and do not estimate uncertainty about the interpolated values.

Kriging is statistical spatial interpolation. It is the centerpiece of what is called 'geostatistics.' The resulting surface (kriged surface) has three parts. (1) Spatial trend: an increase or decrease in the values that depends on direction or a covariate (co-kriging); (2) Local spatial autocorrelation. (3) Random variation. This should now sound familiar. Together the three components provide a model that is used to estimate values everywhere within a specified domain.

In short, geostatistics is used to quantify spatial correlation, predict values at locations where values were not observed, estimate uncertainty on the predicted values, and simulate data.

As we've done with areal data and point pattern data (Moran's I, Ripley's K), we begin with quantifying spatial autocorrelation. To get started we need some definitions.

- Statistical interpolation assumes the observed values are spatially homogeneous. This implies stationarity and continuity
- Stationarity means that the average difference in values between pairs of observations separated by a given distance (lag) is constant across the domain
- Continuity means that the spatial autocorrelation depends only on the lag (and orientation) between observations. That is; spatial autocorrelation is independent of location and can be described by a single function
- Stationarity and continuity allow different parts of the region to be treated as "independent" samples

Stationarity can be weak or intrinsic. Both assume the average of the difference in values at observations separated by a lag distance $h$ is zero. That is, E$[z_i - z_j]$ = 0, where location $i$ and location $j$ are a (lag) distance $h$ apart. This implies that the interpolated surface $Z(s)$ is a random function with a constant mean ($m$) and a residual ($\varepsilon$).

$$
Z(s) = m + \varepsilon(s).
$$
The expected value (average across all values) in the domain is $m$.

Weak stationarity assumes that the covariance is a function of lag distance $h$.

$$
\hbox{cov}(z_i, z_j) = \hbox{cov}(h)
$$
where cov($h$) is called the covariogram.

Intrinsic stationarity assumes the variance of the difference in values is a function of the lag distance.

$$
\hbox{var}(z_i - z_j) = \gamma(h),
$$
where $\gamma(h)$ is called the variogram. This means that spatial autocorrelation is independent of location.

These assumptions are needed to get us started with statistical interpolation. If the assumptions are not met, we remove the trends in the data before spatially interpolating the residuals.

Computing the covariogram and the correlogram

In practice we focus on a model for the variogram $\gamma(h)$. But to understand the variogram it helps to first consider the covariogram. This is because of our familiarity with the idea of nearby things being more correlated than things farther away.

To make things simple without loss in generality, we start with a 4 x 6 grid of equally spaced surface air temperatures across a field in degrees C.

  21  21  20  19  18  19 
  
  26  25  26  27  29  28 
  
  32  33  34  35  30  28   
  
  34  35  35  36  32  31   

Put the values into a data vector and determine the mean and variance.

```{r}
temps <- c(21, 21, 20, 19, 18, 19, 
           26, 25, 26, 27, 29, 28, 
           32, 33, 34, 35, 30, 28, 
           34, 35, 35, 36, 32, 31)
mean(temps)
var(temps)
```

To start, you focus only on the covariance function in the north-south direction. To compute the sample covariance function you first compute the covariance between the observed values one distance unit apart. Using maths

$$
\hbox{cov}(0, 1) = 1/|N(1)| \sum (z_i - Z)(z_j - Z)
$$
where $|N(1)|$ is the number of distinct observation pairs with a distance separation of one unit in the north-south direction and where $Z$ is the average over all observations. We let zero in cov(0, 1) refer to the direction and we let one refer to the distance one unit apart. With this grid of observations $|N(1)|$ = 18.

The equation for the covariance can be simplified to

$$
\hbox{cov}(0, 1) = 1/|N(1)| \sum z_i z_j - m_{-1} m_{+1}
$$
where $m_{-1}$ is the average temperature over all rows except the first (northern most) and $m_{+1}$ is the average temperature over all rows except the last (southern most).

To simplify the notation re-index the grid of temperatures using lexicographic (reading) order.

 1   2   3   4   5   6
 
 7   8   9   10  11  12 
 
 13  14  15  16  17  18  
  
 19  20  21  22  23  24 

Then

```{r}
mp1 <- mean(temps[1:18])
mm1 <- mean(temps[7:24])
cc <- sum(temps[1:18] * temps[7:24])/18
cc - mm1 * mp1
```

Or more generally
```{r}
N <- 18
k <- 1:N
1/N * sum(temps[k] * temps[k + 6]) - mean(temps[k]) * mean(temps[k + 6])
```

The covariance has units of the observed variable squared (here $^\circ C^2$).

You also have observation pairs two units of distance apart. So you compute the cov(0, 2) in a similar way. 
$$
\hbox{cov}(0, 2) = 1/|N(2)| \sum z_i z_j - m_{-2} m_{+2}
$$
where $m_{-2}$ is the average temperature over all rows except the first two and $m_{+2}$ is the average temperature over all rows except the last two. $|N(2)|$ is the number of pairs two units apart.

```{r}
N <- 12
k <- 1:N
1/N * sum(temps[k] * temps[k + 12]) - mean(temps[k]) * mean(temps[k + 12])
```

Similarly you have observation pairs three units apart so you compute cov(0, 3) as
$$
\hbox{cov}(0, 3) = 1/|N(3)| \sum z_i z_j - m_{-3} m_{+3}
$$
```{r}
N <- 6
k <- 1:N
1/N * sum(temps[k] * temps[k + 18]) - mean(temps[k]) * mean(temps[k + 18])
```

There are no observation pairs four units apart in the north-south direction so you are finished. The covariogram is a plot of the covariance values as a function of lag distance. Let $h$ be the lag distance, then

$h$      |  cov($h$)  
-------|--------  
(0, 1) |  15  
(0, 2) |   3  
(0, 3) |   1  

It is convenient to have a measure of co-variability that is dimensionless. So you divide the covariance at lag distance $h$ by the covariance at lag zero. This is the correlogram. Values of the correlogram range from 0 to +1.

The covariogram is a decreasing function of lag distance. The _variogram_ is the inverse (multiplicative) of the covariogram. 

Mathematically: var($z_i - z_j$) for locations $i$ and $j$. The semivariogram is 1/2 the variogram. If location $i$ is near location $j$, the difference in the values will be small and so too will the variance of their differences, in general. If location $i$ is far from location $j$, the difference in values will be large and so too will the variance of their differences.

In practice you have a set of observations and we compute a variogram. This is the sample (empirical) variogram. Let $t_i = (x_i, y_i)$  be the ith location and $h_{i,j} = t_j - t_i$ be the vector connecting location $t_i$ with location $t_j$. Then the sample variogram is defined as

$$
\gamma(h) = \frac{1}{2N(h)} \sum^{N(h)} (z_i - z_j)^2
$$
where $N(h)$ is the number of observation pairs a distance of $h$ units apart.

The variogram assumes intrinsic stationarity so the raw observed values should not have a trend. If there is a trend it needs to be removed before computing the variogram.

The sample variogram is characterized by a set of points the values of which generally increase as $h$ increases before leveling off (reaching a plateau).

As an example, you compute and plot the sample variogram from the `meuse.all` data frame from the {gstat} package. First attach the data frame and look at the first six rows.

```{r}
library(gstat)
data(meuse.all)
head(meuse.all)
```

The data are locations and top soil heavy metal concentrations (ppm), along with a number of soil and landscape variables, collected in a flood plain of the river Meuse, near the village Stein, NL. Heavy metal concentrations are bulk sampled from an area of approximately 15 m x 15 m.

Next locate where the data are from. First convert the data frame to a spatial data frame and then use functions from the {tmap} package in view mode.

```{r}
meuse.sf <- meuse.all |>
  sf::st_as_sf(coords = c("x", "y"), 
               crs = 28992)

tmap::tmap_mode("view")

tmap::tm_shape(meuse.sf) +
  tmap::tm_bubbles(size = "zinc")
```

Then compute the sample variogram and save it as `meuse.v`.

```{r}
meuse.v <- variogram(zinc ~ 1,
                     data = meuse.all,
                     locations = ~ x + y)

class(meuse.v)
```

The output is an object of class `gstatVariogram` and `data.frame`. Plot the sample variogram and label the key features.

```{r}
library(ggplot2)

ggplot(data = meuse.v,
       mapping = aes(x = dist, y = gamma)) +
  geom_point(size = 2) +
  scale_y_continuous(limits = c(0, 210000)) +
  geom_hline(yintercept = c(30000, 175000), color = "red") +
  geom_vline(xintercept = 800, color = "red") +
  xlab("Lag distance (h)") + ylab(expression(paste(gamma,"(h)"))) +
  geom_segment(aes(x = 0, y = 0, xend = 0, yend = 30000,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = 100, y = 10000, label = "nugget")) +
  geom_segment(aes(x = 0, y = 10000, xend = 0, yend = 175000,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = 180, y = 150000, label = "sill (partial sill)")) +
  geom_segment(aes(x = 0, y = 190000, xend = 800, yend = 190000,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = 250, y = 190000, label = "range")) +
  theme_minimal()
```

- Lag (lag distance): Relative distance between observation locations (here units: meters)
- Nugget (nugget, nugget variance, or nugget effect): The height of the variogram at zero lag (here units ppm squared). The nugget is the variation in the values at the observation locations independent of spatial variation. It is related to the observation (or measurement) precision 
- Sill: The height of the variogram at which the values are uncorrelated
- Relative nugget effect: The ratio of the nugget to the sill expressed as a percentage
- Range: The distance beyond which the values are uncorrelated. The range is indicated on the empirical variogram as the position along the horizontal axis where values of the variogram reach a constant height

Additional terms.
- Isotropy: The condition in which spatial correlation is the same in all directions
- Anisotropy: (an-I-so-trop-y) spatial correlation is stronger or more persistent in some directions
- Directional variogram: Distance and direction are important in characterizing the spatial correlations. Otherwise the variogram is called omni-directional
- Azimuth ($\theta$): Defines the direction of the variogram in degrees. The azimuth is measured clockwise from north
- Lag spacing: The distance between successive lags is called the lag spacing or lag increment
- Lag tolerance: The distance allowable for observational pairs at a specified lag. With arbitrary observation locations there will be no observations exactly a lag distance from any observation. Lag tolerance provides a range of distances to be used for computing values of the variogram at a specified lag.

Computing the sample variogram is the first step in modeling geostatistical data. The next step is fitting a model to the variogram. The model is important since the sample variogram estimates are made only at discrete lag distances (with specified lag tolerance and azimuth). You need a continuous function that varies smoothly across all lags. In short, the statistical model replaces the discrete set of points.

Variogram models come from different families. The fitting process first requires a decision about what family to choose and then given the family, a decision about what parameters (nugget, sill, range) to choose. 

An exponential variogram model reaches the sill asymptotically. The range (a) is defined as the lag distance at which gamma reaches 95% of the sill.

```{r}
c0 <- .1
c1 <- 2.1
a <- 1.3
curve(c0 + c1*(1 - exp(-3*x/a)), 
      from = .01, to = 3, 
      xlab = "h", 
      ylab = expression(paste(hat(gamma), "(h)")), 
      las = 1)
```

A spherical variogram model reaches the sill at x = 1.

```{r}
curve(c0 + c1*(3*x/2 - x^3/2),
      from = .01, to = 1,
      xlab = "h",
      ylab = expression(paste(hat(gamma), "(h)")), 
      las = 1)
```

A Gaussian variogram model is "S"-shaped (sigmodial). It is used when the data exhibit strong correlations at the shortest lag distances.  The inflection point of the model occurs at $\sqrt{a/6}$.

```{r}
curve(c0 + c1*(1 - exp(-3*x^2/a^2)),
      from = .01, to = 3, 
      xlab = "h", 
      ylab = expression(paste(hat(gamma), "(h)")),
      las = 1)
```

Other families include

* Linear models: $\hat \gamma(h)$ = c0 + b * h.
* Power models:  $\hat \gamma(h)$ = c0 + b * h$^\lambda$.

These models have no sill.

Choosing a variogram family is largely done by looking at the shape of the sample variogram. Then, given a sample variogram computed from a set of spatial observations and a choice of family, the parameters of the variogram model are determined by weighted least-squares (WLS). Weighting is needed because the because the sample variogram estimates are computed using a varying number of point pairs.

There are other ways to determine the parameters including by eye, and by the method of maximum likelihoods, but WLS is less erratic than other methods and it requires fewer assumptions about the distribution of the data. And the process can be automated and it often is in high-level packages, but it is important to understand what is in the black box.

The final step in spatial statistical interpolation is called kriging. Kriging interpolates the observed data using the variogram model. It was developed by a South African miner (D.G. Krige) as a way to improve estimates of where ore reserves might be located. Extraction costs are reduced substantially if good predictions can be made of where the ore resides given samples taken across the mine.

A kriged estimate is a weighted average of the observations where the weights are based on the variogram model. The kriged estimates are optimal in the sense that they minimize the error variance. The type of kriging depends on the characteristics of the observations and the purpose of interpolation.

- Simple kriging assumes a known constant mean for the domain 
- Ordinary kriging assumes an unknown constant mean
- Universal kriging assumes an unknown linear or nonlinear trend in the mean

To review, the steps for spatial interpolation (statistical) are:

1. Examine the observations for trends and isotropy
2. Compute a sample (empirical) variogram
3. Fit a variogram model to the empirical variogram
4. Create an interpolated surface using the variogram model together with the data (kriging)

## Computing the sample variogram {-}

The {gstat} package contains functions for spatial interpolation that take advantage of simple feature (and S4 class) spatial data data frames.

Suppose we have the following set of observations (`zobs`) at locations (`sx`, `sy`).

```{r}
sx <- c(1.1, 3.2, 2.1, 4.9, 5.5, 7, 7.8, 9, 2.3, 6.9)
sy <- c(3, 3.5, 6, 1.5, 5.5, 3.2, 1, 4.5, 1, 7)
zobs <- c(-0.6117, -2.4232, -0.42, -0.2522, -2.0362, 0.9814, 1.842,
         0.1723, -0.0811, -0.3896)
```

Create a data frame and plot the observed values at the locations using the `geom_text()` function.
```{r}
sf <- data.frame(sx, sy, zobs) |>
  sf::st_as_sf(coords = c("sx", "sy"),
               crs = 4326)

ggplot(data = sf, 
       mapping = aes(x = sx, y = sy, label = zobs)) +
  geom_text() +
  theme_minimal()
```

Lag distance (distance between locations) is the independent variable in the variogram function. You get all pairwise distances by applying the `dist()` function to a matrix of spatial coordinates.

```{r}
dist(cbind(sx, sy))
max(dist(cbind(sx, sy)))
min(dist(cbind(sx, sy)))
```

The `dist()` function computes a pairwise distance matrix. The distance between the first and second observation is 2.16 units and so on. The largest lag distance is 8.04 units and the smallest lag distance is 2.05 units.

The functions in the {gstat} package work with simple feature objects.

As another example, consider the dataset called `topo` from the {MASS} package. The data are topographic heights (feet) within a 310 sq ft domain.

Examine the data with a series of plots.

```{r}
topo.df <- MASS::topo

p1 <- ggplot(data = topo.df,
             mapping = aes(x = x, y = y, color = z)) +
  geom_point() +
  scale_color_viridis_c() +
  theme_minimal()

p2 <- ggplot(data = topo.df,
             mapping = aes(x = z, y = y)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal()

p3 <- ggplot(data = topo.df,
             mapping = aes(x = x, y = z)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal()

p4 <- ggplot(data = topo.df,
             mapping = aes(x = z)) +
  geom_histogram(bins = 13) +
  theme_minimal()

library(patchwork)
( p1 + p2 ) / ( p3 + p4 )
```

Note the trend in the north-south direction and the skewness in the observed values. 

Examine the residuals after removing a first-order trend from the observations.

```{r}
topo.df$z1 <- residuals(lm(z ~ x + y, data = topo.df))

p1 <- ggplot(data = topo.df,
             mapping = aes(x = x, y = y, color = z1)) +
  geom_point() +
  scale_color_viridis_c() +
  theme_minimal()

p2 <- ggplot(data = topo.df,
             mapping = aes(x = z1, y = y)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal()

p3 <- ggplot(data = topo.df,
             mapping = aes(x = x, y = z1)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal()

p4 <- ggplot(data = topo.df,
             mapping = aes(x = z1)) +
  geom_histogram(bins = 13) +
  theme_minimal()

( p1 + p2 ) / ( p3 + p4 )
```

The north-south trend is removed and the observations have a more symmetric distribution. There appears to be some non-linear trend (U-shape) in the east-west direction. 

However, the residuals appear to show spatial autocorrelations (areas with above and below residuals). 

Compare the empirical variograms using first the raw values and then the residuals after removing the first-order trend.

```{r}
topo.sf <- topo.df |>
  sf::st_as_sf(coords = c("x", "y"))

topo.v1 <- variogram(z ~ 1,
                     data = topo.sf,
                     cutoff = 2.5)
topo.v2 <- variogram(z1 ~ 1,
                     data = topo.sf,
                     cutoff = 2.5)

ggplot(data = topo.v1,
       mapping = aes(x = dist, y = gamma)) +
  geom_point(color = "red") +
  geom_point(data = topo.v2,
             mapping = aes(x = dist, y = gamma),
             color = "black") +
  scale_x_continuous(breaks = seq(0, 2.5, by = .25)) +
  xlab("Lag distance (h)") + 
  ylab(expression(paste(gamma,"(h)"))) +
  theme_minimal()
```


The semivariance ($\gamma(u)$) is plotted against lag distance. Values increase with increasing lag until a lag distance of about 2. 

At large lags there are fewer estimates so the values have greater variance. A model for the semivariance is fit only for the the increasing portion of the graph.

The variogram values have units of square feet and are calculated using point pairs at lag distances within a lag tolerance. The number of point pairs depends on the lag so the variogram values are less precise at large distance.

Plot the number of point pairs used as a function of lag distance.

```{r}
ggplot(data = topo.v2,
       mapping = aes(y = np, x = dist)) +
  geom_point() +
  xlab("Lag Distance") + ylab("Number of Observation Pairs") +
  theme_minimal()
```

<!--chapter:end:20-Lesson.Rmd-->

# Tuesday November 15, 2022 {.unnumbered}

**"Statistics is such a powerful language for describing data in ways that reveal nothing about their causes. Of course statistics is powerful for revealing causes as well. But it takes some care. Like the difference between talking and making sense."** - Richard McElreath

Today

- Fitting a variogram model to the sample variogram
- Creating an interpolated surface with the method of kriging

## Fitting a variogram model to the sample variogram {-}

Some years ago there were three nuclear waste repository sites being proposed in Nevada, Texas, and Washington. The proposed site needed to be large enough for more than 68,000 high-level waste containers placed underground, about 9 m (~30 feet) apart, in trenches surrounded by salt. 

In July of 2002 the Congress approved [Yucca Mountain](https://en.wikipedia.org/wiki/Yucca_Mountain_nuclear_waste_repository), Nevada, as the nation’s first long-term geological repository for spent nuclear fuel and high-level radioactive waste.

The site must isolate the waste for 10,000 years. Leaks could occur, however, or radioactive heat could cause tiny quantities of water in the salt to migrate toward the heat until eventually each canister is surrounded by 22.5 liters of water (~6 gallons). A chemical reaction of salt and water can create hydrochloric acid that might corrode the canisters. The piezometric-head data at the site were obtained by drilling a narrow pipe into the aquifer and letting water seeks its own level in the pipe (piezometer).

The head measurements, given in units of feet above sea level, are from drill stem tests and indicate the total energy of the water in units of height. The higher the head height, the greater the potential energy.  Water flows away from areas of high potential energy with aquifer discharge proportional to the gradient of the piezometric head. The data are in `wolfcamp.csv` on my website.

Examine the observed data for trends and check to see if the observations are adequately described by a normal distribution.

Import the data as a data frame from the csv file.

```{r}
L <- "http://myweb.fsu.edu/jelsner/temp/data/wolfcamp.csv"
wca.df <- readr::read_csv(L, show_col_types = FALSE)
```

Create a simple feature data frame and make a map showing the locations and the values for the head heights.

```{r}
wca.sf <- sf::st_as_sf(x = wca.df, 
                       coords = c("lon", "lat"),
                       crs = 4326)

tmap::tmap_mode("view")

tmap::tm_shape(wca.sf) +
  tmap::tm_dots("head")
```

You will use the spatial coordinates to model the spatial autocorrelation and to remove any spatial trends. So you include them as attributes in your spatial data frame.

```{r}
XY <- wca.sf |>
  sf::st_coordinates() 
wca.sf$X <- XY[, 1]
wca.sf$Y <- XY[, 2]
```

Are all observations at different locations? Duplicate coordinates might be due to an error or they might represent multiple measurements at a location.

You check for duplicates with the {base} `duplicated()` function applied to the geometry field.

```{r}
wca.sf$geometry |>
  duplicated()
```

Observation 31 is a location that already has an observed head height. 

You remove this observation from the data frame.

```{r}
wca.sf <- wca.sf |>
  dplyr::filter(!duplicated(geometry))

wca.sf$geometry |>
  duplicated() |>
  any()
```

Summarize the information in the spatial data frame.

```{r}
wca.sf |>
  summary()
wca.sf |>
  sf::st_bbox(wca.sf)

```

There are 84 well sites bounded between longitude lines 104.55W and 100.02W and latitude lines 33.51N and 36.09N.

The data values are summarized. The values are piezometric head heights in units of feet.

```{r}
library(ggplot2)

ggplot() +
  geom_sf(data = wca.sf,
          mapping = aes(color = head)) +
  scale_color_viridis_c() +
  labs(col = "Height (ft)") +
  theme_minimal()
```

There is a clear trend in head heights with the highest potential energy (highest heights) over the southwest (yellow) and lowest over the northeast (blue). 

Compute and plot the empirical variogram using the `variogram()` function from the {gstat} package.

```{r}
library(gstat)

variogram(head ~ 1,
          data = wca.sf) |>
  plot()
```

The continuously increasing set of variances with little fluctuation results from the data trend you saw in the map. There are at least two sources of variation in any set of geostatistical data: trend and spatial autocorrelation. Trend is modeled with a smooth curve and autocorrelation is modeled with the variogram.

Note: since the spatial coordinates are unprojected (decimal latitude/longitude) great circle distances are used and the units are kilometers. 

You compute and plot the variogram this time with the trend removed. You replace the `1` with `X + Y` on the right hand side of the formula. The variogram is then computed on the residuals from the linear trend model.

```{r}
variogram(head ~ X + Y,
          data = wca.sf) |>
  plot()
```

Here you see an increase in the variance with lag distance out to about 100 km, but then the values fluctuate about a variance of about 41000 (ft$^2$).

You save the variogram object computed on the residuals.

```{r}
wca.v <- variogram(head ~ X + Y, 
                   data = wca.sf)
```

You then use this information contained in the variogram object to anticipate the type of variogram model.

```{r}
df <- wca.v |>
  as.data.frame()

( p <- ggplot(data = df, 
              mapping = aes(x = dist, y = gamma)) + 
  geom_point() + 
  geom_smooth() +
  scale_y_continuous(limits = c(0, NA)) +
  ylab(expression(paste("Variogram [", gamma,"(h)]"))) +
  xlab("Lag distance (h)") +
  theme_minimal() )
```

The blue line is a least-squares regression smoother through the variogram estimates. The fact that it is not a flat horizontal line indicates spatial autocorrelation in the residuals (distinct from the first-order trend). 

Directional variograms

The assumption of isotropy implies the same spatial autocorrelation function in all directions.

You compute variograms using observational pairs located along the same orientation to examine this assumption. Instead of considering all observational pairs within a lag distance $h$ and lag tolerance $\delta h$, you consider only pairs within a directional segment.

This is done with the `alpha =` argument specifying the direction in plane (x,y), in positive degrees clockwise from positive y (North): alpha = 0 for direction North (increasing y), alpha = 90 for direction East (increasing x).

Here you specify four directions (north-south-0, northeast-southwest-45, east-west-90, and southeast-northeast-135) and compute the corresponding directional variograms.

```{r}
wca.vd <- variogram(head ~ X + Y, 
                    data = wca.sf,
                    alpha = c(0, 45, 90, 135))
df <- wca.vd |>
  as.data.frame() |>
  dplyr::mutate(direction = factor(dir.hor))

ggplot(data = df, 
              mapping = aes(x = dist, y = gamma, color = direction)) + 
  geom_point() + 
  geom_smooth(alpha = .2) +
  scale_y_continuous(limits = c(0, NA)) +
  ylab(expression(paste("Variogram [", gamma,"(h)]"))) +
  xlab("Lag distance (h)") +
  theme_minimal()
```

The four variograms all have a similar shape and there is large overlap in the uncertainty bands surrounding the smooth curves so you conclude that the assumption of isotropy is reasonable.

Fit a variogram model to the empirical variogram

You are now ready to fit a variogram model to the empirical variogram. This amounts to fitting a parametric curve through the set of points that make up the empirical variogram.

Start by plotting the (omni-directional) empirical variogram saved in object `p`.

```{r}
p
```

The shape of the blue line gives you an idea of the type of variogram family of models you should consider.

Now you can guess at a family for the variogram model and eyeball the parameters. A _spherical_ variogram model has a nearly linear increase in variances with lag distance before an abrupt flattening so that is a good choice.

The parameters for the model can be estimated from the graph as follows.
```{r}
p +
  geom_hline(yintercept = c(12000, 45000), color = "red") +
  geom_vline(xintercept = 100, color = "red") +
  geom_segment(aes(x = 0, y = 0, xend = 0, yend = 12000,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = 15, y = 11000, label = "nugget")) +
  geom_segment(aes(x = 0, y = 12000, xend = 0, yend = 45000,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = 10, y = 44000, label = "sill")) +
  geom_segment(aes(x = 0, y = 47000, xend = 100, yend = 47000,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = 50, y = 48000, label = "range"))
```

The three parameters used in fitting a variogram model are nugget, sill, and range.

- Nugget (nugget, nugget variance, or nugget effect): The height of the variogram at zero lag. The nugget is the variation in the values at the observation locations without regard to spatial variation. Related to the observation (or measurement) precision.

- Sill: The height of the variogram at which the values are uncorrelated. The sill is indicated by the height of the plateau in the variogram. 

- Range: The distance beyond which the values are uncorrelated. The range is indicated by distance along the horizontal axis from zero lag until the plateau in the variogram.

Other terms: (1) Relative nugget effect: The ratio of the nugget to the sill expressed as a percentage. (2) Lag distance: Relative distance between observation locations.

From the figure you estimate the sill at 44000 ft^2, the nugget at 12000 ft^2 and the range at 100 km. 

To fit a model to the empirical variogram you start with the `vgm()` function that sets the curve family (here spherical) and the initial parameter values. You save result in an object called `wca.vmi`. The function needs the partial sill (`psill =` argument) as the difference between the sill and the nugget.

```{r}
wca.vmi <- vgm(model = "Sph", 
               psill = 32000, 
               range = 100, 
               nugget = 12000)
wca.vmi
```

Next you apply the function `fit.variogram()`, which uses the method of weighted least squares to improve the parameter estimates from the set of initial estimates. The function takes the empirical variogram and the set of initial estimates as `object =` and `model =`, respectively.

```{r}
wca.vm <- fit.variogram(object = wca.v, 
                        model = wca.vmi)
wca.vm
```

Note: Ordinary least squares is not an appropriate method for fitting a variogram model to the empirical variogram because the semivariances are correlated across the lag distances and the precision on the estimates depends on the number of site pairs for a given lag.

The output table shows the nugget and spherical model. The nugget is 9812 ft^2 and the partial sill for the spherical model is 34851 ft^2 with a range of 107 km. These values are close to your initial estimates.

To check the model and fit plot them together with the `plot()` method.

```{r}
plot(wca.v, wca.vm)
```

The blue line is the variogram model and the points are the empirical variogram values.

Note that the `fit.variogram()` function will find the optimal fit even if the initial values are not very good. Here you lower the partial sill to 10000 ft^2, reduce the range to 50 km and set the nugget to 8000 ft^2.

```{r}
wca.vmi2 <- vgm(model = "Sph", 
                psill = 10000, 
                range = 50, 
                nugget = 8000)
wca.vm2 <- fit.variogram(object = wca.v, 
                        model = wca.vmi2)
wca.vm2
```

The initial values are poor but good enough for the `fit.variogram()` function to find the optimal model.

Compare the spherical model to a Gaussian and an exponential model.
```{r}
wca.vmi3 <- vgm(model = "Gau", 
                psill = 30000, 
                range = 30, 
                nugget = 10000)
wca.vm3 <- fit.variogram(object = wca.v, 
                         model = wca.vmi3)
wca.vmi4 <- vgm(model = "Exp", 
                psill = 30000, 
                range = 10, 
                nugget = 10000)
wca.vm4 <- fit.variogram(object = wca.v, 
                         model = wca.vmi4)
plot(wca.v, wca.vm3)
plot(wca.v, wca.vm4)
```

The Gaussian model has a S-shaped curve (sigmodial) indicating more spatial autocorrelation at close distances. The exponential model has no plateau. Both models fit the empirical variogram values reasonably well. 

In practice, the choice often makes very little difference in the quality of the spatial interpolation.

On the other hand, it is possible to optimize over all sets of variogram models and parameters using the `autofitVariogram()` function from the {automap} package. The package requires the data to be of S4 class but uses the functions from the {gstat} package.

Here you use the function on the Wolfcamp aquifer data.

```{r}
wca.sp <- as(wca.sf, "Spatial")
wca.vm5 <- automap::autofitVariogram(formula = head ~ X + Y, 
                                     input_data = wca.sp)
plot(wca.vm5)
```

The automatic fitting results in a Matern model with Stein's parameterization. The Matern family of variogram models has an additional parameter (besides the nugget, sill, and range) kappa that allows for local smoothing. With an extra parameter these models will generally outperform models with fewer parameters.

## Creating an interpolated surface with the method of kriging {-}

Kriging uses the variogram model together with the observed data to estimate values at any location of interest. The kriged estimates are a weighted average of the neighborhood values with the weights defined by the variogram model. 

Estimates are often made at locations defined on a regular grid. Here you create a regular grid of locations within the boundary of the spatial data frame using the `sf::st_make_grid()` function.  You specify the number of locations in the x and y direction using the argument `n =`. The `what = "centers"` returns the center locations of the grid cells as points.

```{r}
grid.sfc <- sf::st_make_grid(wca.sf,
                             n = c(50, 50),
                             what = "centers")
```

The result is a simple feature column (`sfc`) of points.

Plot the grid locations together with the observation locations.

```{r}
sts <- USAboundaries::us_states()

tmap::tmap_mode("plot")
tmap::tm_shape(wca.sf) +
  tmap::tm_bubbles(size = .25) +
tmap::tm_shape(grid.sfc) +
  tmap::tm_dots(col = "red") +
tmap::tm_shape(sts) +
  tmap::tm_borders()
```

Since there is a trend term you need to add the X and Y values to the simple feature column of the grid. First make it a simple feature data frame then using then add the columns with `dplyr::mutate()`.

```{r}
XY <- grid.sfc |>
  sf::st_coordinates() 
grid.sf <- grid.sfc |>
  sf::st_as_sf() |>
  dplyr::mutate(X = XY[, 1],
                Y = XY[, 2])
```

Next interpolate the aquifer heights to the grid locations. You do this with the `krige()` function. The first argument is the formula for the trend, the locations argument is the observed data locations from the simple feature data frame, the new data argument is the locations and independent variables (in this case the trend variables) and the model argument is the variogram model that you fit above.

```{r}
wca.int <- krige(head ~ X + Y,
                 locations = wca.sf,
                 newdata = grid.sf,
                 model = wca.vm)
head(wca.int)
```

The output says `using universal kriging`. This is because there is a trend and a variogram model.

The output is a simple feature data frame containing the interpolated values at the grid locations in the column labeled `var1.pred`. The interpolated uncertainty is given in the column labeled `var1.var`.

You plot the interpolated aquifer heights at the grid locations using functions from the {ggplot2} package.

```{r}
ggplot() +
  geom_sf(data = wca.int,
          mapping = aes(col = var1.pred)) +
  scale_color_viridis_c() +
  labs(col = "Height (ft)") +
  theme_minimal()
```

The trend captures the large scale feature while the variogram captures the local spatial autocorrelation. Together they produce an interpolated surface that matches exactly the values at the observation locations (when the nugget is fixed at zero).

Plot the uncertainty in the estimated interpolated values as the square root of the variance.

```{r}
ggplot() +
  geom_sf(data = wca.int,
          mapping = aes(col = sqrt(var1.var))) +
  scale_color_viridis_c(option = "plasma") +
  labs(col = "Uncertainty (ft)") +
  theme_minimal()
```

Areas with the largest uncertainty are in areas away from observations (northwest corner). This makes sense our knowledge of the aquifer comes from these observations.

<!--chapter:end:21-Lesson.Rmd-->

# Thursday November 17, 2022 {.unnumbered}

**"The problem of nonparametric estimation consists in estimation, from the observations, of an unknown function belonging to a sufficiently large class of functions."** - A.B. Tsybakov

Today

- Comparing interpolation methods
- Evaluating the accuracy of the interpolation

## Comparing interpolation methods {-}

Here you consider a data set of monthly average surface air temperatures for April across the Midwest. The data are available on my website in the file `MidwestTemps.txt`.

Start by examining the data for spatial trends.

```{r}
L <- "http://myweb.fsu.edu/jelsner/temp/data/MidwestTemps.txt"
t.sf <- readr::read_table(L, show_col_types = FALSE) |>
  sf::st_as_sf(coords = c("lon", "lat"),
               crs = 4326)

XY <- t.sf |>
  sf::st_coordinates() 
t.sf$X <- XY[, 1]
t.sf$Y <- XY[, 2]

t.sf$geometry |>
  duplicated() |>
  any()
```

Plot the values on a map.

```{r}
sts <- USAboundaries::us_states()

tmap::tm_shape(t.sf) +
  tmap::tm_text(text = "temp", 
                size = .6) +
tmap::tm_shape(sts) +
  tmap::tm_borders() 
```

There is a clear trend in the temperature field with the coolest values to the north. Besides the trend there is some local clustering of similar values (spatial autocorrelation).

Compute and plot the empirical variogram on the residuals after removing the trend. The trend term is specified in the formula as `temp ~ X + Y`.

```{r}
library(gstat)

t.v <- variogram(temp ~ X + Y, 
                 data = t.sf)
plot(t.v)
```

Check for anisotropy. Specify four directions and compute the corresponding directional variograms.

```{r}
t.vd <- variogram(temp ~ X + Y, 
                  data = t.sf,
                  alpha = c(0, 45, 90, 135))
df <- t.vd |>
  as.data.frame() |>
  dplyr::mutate(direction = factor(dir.hor))

library(ggplot2)

ggplot(data = df, 
              mapping = aes(x = dist, y = gamma, color = direction)) + 
  geom_point() + 
  geom_smooth(alpha = .2) +
  scale_y_continuous(limits = c(0, NA)) +
  ylab(expression(paste("Variogram [", gamma,"(h)]"))) +
  xlab("Lag distance (h)") +
  theme_minimal()
```

There is no strong evidence to reject the assumption of isotropy.

Use the `autofitVariogram()` function to get initial estimates.

```{r}
t.sp <- as(t.sf, "Spatial")
t.vm <- automap::autofitVariogram(formula = temp ~ X + Y, 
                                     input_data = t.sp)
plot(t.vm)
```

Set the initial parameters for a Gaussian model then fit the model.

```{r}
t.vmi <- vgm(model = "Gau", 
             psill = 2, 
             range = 100, 
             nugget = 1)
t.vmi

t.vm <- fit.variogram(object = t.v, 
                      model = t.vmi)
t.vm

plot(t.v, t.vm)
```

Make a grid for the interpolated values and add the coordinates as attributes.

```{r}
grid.sfc <- sf::st_make_grid(t.sf,
                             n = c(100, 100),
                             what = "centers")
XY <- grid.sfc |>
  sf::st_coordinates() 
grid.sf <- grid.sfc |>
  sf::st_as_sf() |>
  dplyr::mutate(X = XY[, 1],
                Y = XY[, 2])
```

Interpolate with universal kriging.

```{r}
t.int <- krige(temp ~ X + Y,
               locations = t.sf,
               newdata = grid.sf,
               model = t.vm)
```

Map the output.

```{r}
tmap::tm_shape(t.int) +
  tmap::tm_dots(title = "°F",
                shape = 15, 
                size = 2,
                col = "var1.pred", 
                n = 9, 
                palette = "OrRd") +
tmap::tm_shape(sts) +
  tmap::tm_borders() +
tmap::tm_shape(t.sf) +
  tmap::tm_text("temp", 
                col = "white", 
                size = .5) +
tmap::tm_layout(legend.outside = TRUE)
```

The trend term captures the north-south temperature gradient and the variogram captures the local spatial autocorrelation. Together they provide the best interpolated surface.

To see this, you refit the interpolation without the variogram model.

```{r}
krige(temp ~ X + Y,
      locations = t.sf,
      newdata = grid.sf) |>
  tmap::tm_shape() +
  tmap::tm_dots(title = "°F",
                shape = 15, 
                size = 2,
                col = "var1.pred", 
                n = 9, 
                palette = "OrRd") +
tmap::tm_shape(sts) +
  tmap::tm_borders() +
tmap::tm_shape(t.sf) +
  tmap::tm_text("temp", 
                col = "white", 
                size = .5) +
tmap::tm_layout(legend.outside = TRUE)
```

The result is that the variation in temperatures is interpolated as a simple trend surface.

For another comparison, here you interpolate assuming all variation is spatial autocorrelation (no trend term). This is called ordinary kriging.

```{r}
krige(temp ~ 1,
      locations = t.sf,
      newdata = grid.sf,
      model = t.vm) |>
  tmap::tm_shape() +
  tmap::tm_dots(title = "°F",
                shape = 15, 
                size = 2,
                col = "var1.pred", 
                n = 9, 
                palette = "OrRd") +
tmap::tm_shape(sts) +
  tmap::tm_borders() +
tmap::tm_shape(t.sf) +
  tmap::tm_text("temp", 
                col = "white", 
                size = .5) +
tmap::tm_layout(legend.outside = TRUE)
```

The result is that all variation is local autocorrelation. This produces patches of higher and lower temperatures. 

The pattern obtained with ordinary kriging is similar to that obtained using inverse distance weighting. Inverse distance weighting (IDW) is a deterministic method for interpolation. The values assigned to locations are calculated with a weighted average of the values available at the known locations, where the weights are the inverse of the distance to each known location.

The `krige()` function does IDW when there is no trend and no variogram model given.

```{r}
krige(temp ~ 1,
      locations = t.sf,
      newdata = grid.sf) |>
  tmap::tm_shape() +
  tmap::tm_dots(title = "°F",
                shape = 15, 
                size = 2,
                col = "var1.pred", 
                n = 9, 
                palette = "OrRd") +
tmap::tm_shape(sts) +
  tmap::tm_borders() +
tmap::tm_shape(t.sf) +
  tmap::tm_text("temp", 
                col = "white", 
                size = .5) +
tmap::tm_layout(legend.outside = TRUE)
```

Simple kriging is ordinary kriging with a specified mean. This is done with the `beta =` argument.

```{r}
krige(temp ~ 1,
      beta = mean(t.sf$temp),
      locations = t.sf,
      newdata = grid.sf,
      model = t.vm) |>
  tmap::tm_shape() +
  tmap::tm_dots(title = "°F",
                shape = 15, 
                size = 2,
                col = "var1.pred", 
                n = 9, 
                palette = "OrRd") +
tmap::tm_shape(sts) +
  tmap::tm_borders() +
tmap::tm_shape(t.sf) +
  tmap::tm_text("temp", 
                col = "white", 
                size = .5) +
tmap::tm_layout(legend.outside = TRUE)
```

## Evaluating the accuracy of the interpolation {-}

How do you evaluate how good the interpolated surface is? If you use the variogram model to predict at the observation locations, you will get the observed values back. 

For example, here you interpolate to the observation locations by setting `newdata = t.sf` instead of `grid.sf`. You then compute the correlation between the interpolated value and the observed value.

```{r}
t.int2 <- krige(temp ~ X + Y,
                locations = t.sf,
                newdata = t.sf,
                model = t.vm)
cor(t.int2$var1.pred, t.sf$temp)
```

So this is not helpful. 

Instead you use cross validation. Cross validation is a procedure for assessing how well a model does at predicting (interpolating) values when observations specific to the prediction are removed from the model fitting procedure. Cross validation partitions the data into two disjoint subsets and the model is fit to one subset of the data (training set) and the model is validated on the other subset (testing set).

Leave-one-out cross validation (LOOCV) uses all but one observation for fitting and the left-out observation for testing. The procedure is repeated with every observation taking turns being left out. 

```{r}
krige.cv(temp ~ X + Y,
         locations = t.sf,
         model = t.vm) |>
  sf::st_drop_geometry() |>
  dplyr::summarize(r = cor(var1.pred, observed),
                   rmse = sqrt(mean((var1.pred - observed)^2)),
                   mae = mean(abs(var1.pred - observed)))

krige.cv(temp ~ 1,
         locations = t.sf,
         model = t.vm) |>
  sf::st_drop_geometry() |>
  dplyr::summarize(r = cor(var1.pred, observed),
                   rmse = sqrt(mean((var1.pred - observed)^2)),
                   mae = mean(abs(var1.pred - observed)))

krige.cv(temp ~ X + Y,
         locations = t.sf) |>
  sf::st_drop_geometry() |>
  dplyr::summarize(r = cor(var1.pred, observed),
                   rmse = sqrt(mean((var1.pred - observed)^2)),
                   mae = mean(abs(var1.pred - observed)))

krige.cv(temp ~ 1,
         locations = t.sf) |>
  sf::st_drop_geometry() |>
  dplyr::summarize(r = cor(var1.pred, observed),
                   rmse = sqrt(mean((var1.pred - observed)^2)),
                   mae = mean(abs(var1.pred - observed)))
```

All four interpolations result in high correlation between observed and interpolated values that exceed .9 and root-mean-squared errors (RMSE) less than 1.8. But the universal kriging interpolation gives the highest correlation and the lowest RMSE and mean-absolute errors.

For a visual representation of the goodness of fit you plot the observed versus interpolated values from the cross validation procedure.

```{r}
krige.cv(temp ~ X + Y,
               locations = t.sf,
               model = t.vm) |>
  dplyr::rename(interpolated = var1.pred) |>
ggplot(mapping = aes(x = observed, y = interpolated)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  geom_smooth(method = lm, color = "red") +
  ylab("Interpolated temperatures (°F)") +
  xlab("Observed temperatures (°F)") +
  theme_minimal()
```

The black line represents a perfect prediction and the red line is the best fit line when you regress the interpolated temperatures onto the observed temperatures. The fact that the two lines nearly coincide indicates the interpolation is good.

The `nfold =` argument, which by default is set to the number of observations and does a LOOCV, allows you to divide the data into different size folds (instead of N-1).

Note that these performance metrics are biased toward the sample of data because cross validation is done only on the interpolation (kriging) and not on the variogram model fitting.

That is, with kriging the data is used in two ways (1) to fit the variogram model, and (2) to interpolate the values. 

To perform a full LOOCV you need to refit the variogram after removing the observation for which you want the interpolation.
```{r, eval=FALSE}
vmi <- vgm(model = "Sph", 
           psill = 2, 
           range = 200, 
           nugget = 1)
int <- NULL
for(i in 1:nrow(t.sf)){
  t <- t.sf[-i, ]
  v <- variogram(temp ~ X + Y, 
                 data = t)
  vm <- fit.variogram(object = v, 
                      model = vmi)
  int[i] <- krige(temp ~ X + Y,
                  locations = t,
                  newdata = t[i, ],
                  model = vm)$var1.pred
}
```

The interpolation error using full cross validation will always be larger than the interpolation error using a fixed variogram model.

## Block cross validation {-}

One final note about cross validation in the context of spatial data is the observations are not independent. As such it is better to create spatial areas for training separate from the spatial areas for testing.

A nice introduction to so-called 'block' cross validation in the context of species distribution modeling is available here
https://cran.r-project.org/web/packages/blockCV/vignettes/BlockCV_for_SDM.html

<!--chapter:end:22-Lesson.Rmd-->

# Thursday November 29, 2022 {.unnumbered}

**“Practice any art, music, singing, dancing, acting, drawing, painting, sculpting, poetry, fiction, essays, reportage, no matter how well or badly, not to get money & fame, but to experience becoming, to find out what's inside you, to make your soul grow.”** - Kurt Vonnegut

Today

- Interpolating to areal units
- Simulating spatial fields
- Interpolating multiple variables
- Machine learning for spatial interpolation

## Interpolating to areal units (block kriging) {-}

In 2008 tropical cyclone (TC) Fay formed from a tropical wave near the Dominican Republic, passed over the island of Hispaniola, Cuba, and the Florida Keys, then crossed the Florida peninsula and moved westward across portions of the Panhandle producing heavy rains in parts of the state.

Rainfall is an example of geostatistical data. In principle it can be measured anywhere, but typically you have values at a sample of sites. The pattern of observation sites is not of much interest as it is a consequence of constraints (convenience, opportunity, economics, etc) unrelated to the phenomenon. Interest centers on inference about how much rain fell across the region.

Storm total rainfall amounts from stations in and around the state are in `FayRain.txt` on my website. They are compiled reports from official weather sites and many cooperative sites. The cooperative sites are the Community Collaborative Rain, Hail and Snow Network (CoCoRaHS), a community-based, high density precipitation network made up of volunteers who take measurements of precipitation in their yards. The data were obtained from NOAA/NCEP/HPC and from the Florida Climate Center.

Import the data.
```{r}
L <- "http://myweb.fsu.edu/jelsner/temp/data/FayRain.txt"
( FR.df <- readr::read_table(L) )
```

The data frame contains 803 rainfall sites. Longitude and latitude coordinates of the sites are given in the first two columns and total rainfall in inches and millimeters are given in the second two columns. 

Create a spatial points data frame by specifying columns that contain the spatial coordinates. Then assign a geographic coordinate system and convert the rainfall from millimeters to centimeters.

```{r}
FR.sf <- sf::st_as_sf(x = FR.df,
                      coords = c("lon", "lat"),
                      crs = 4326) |>
  dplyr::mutate(tpm = tpm/10)

summary(FR.sf$tpm)
```

The median rainfall across all available observations is 15.8 cm and the highest is 60.2 cm. 

Get the Florida county boundaries from the {USAboundaries} package.

```{r}
FL.sf <- USAboundaries::us_counties(states = "Florida")
```

Transform the geographic coordinates of the site locations and map polygons to projected coordinates. Here you use Florida GDL Albers (EPSG:3087) with meter as the length unit.
```{r}
FR.sf <- sf::st_transform(FR.sf, crs = 3087)
FL.sf <- sf::st_transform(FL.sf, crs = 3087)
sf::st_crs(FR.sf)
```

Start by making a map of the rainfall sites and storm total rainfall that includes the state border.

```{r}
tmap::tm_shape(FR.sf) +
  tmap::tm_dots(col = "tpm", size = .5) +
tmap::tm_shape(FL.sf) +
  tmap::tm_borders()
```

Two areas of very heavy rainfall are noted. One running north-south along the east coast and another across the north. 

Rainfall reporting sites are clustered in and around cities and are located only over land. This type of station location arrangement will make it hard for deterministic interpolation methods (e.g., IDW or splines) to produce a reasonable surface.

The empirical variogram is computed using the `variogram()` function from the {gstat} package. The first argument is the model formula specifying the rainfall column from the data frame and the second argument is the data frame name.  Here `~ 1` in the model formula indicates no covariates or trends in the data. Trends are included by specifying coordinate names through the `st_coordinates()` function.

Compute the empirical variogram for these set of rainfall values. Use a cutoff distance of 400 km (400,000 m). The cutoff is the separation distance up to which point pairs are included in the semivariogram. The smaller the cutoff value the more the variogram is focused on nearest neighbor locations.

```{r}
library(gstat)

v <- variogram(tpm ~ 1, 
               data = FR.sf,
               cutoff = 400000)
```

Plot the variogram values as a function of lag distance and add text indicating the number of point pairs for each lag distance. Save a copy of the plot for later.

```{r}
library(ggplot2)

v.df <- data.frame(dist = v$dist/1000,
                   gamma = v$gamma,
                   np = v$np)

( pv <- ggplot(v.df, aes(x = dist, y = gamma)) +
  geom_point() +
  geom_text(aes(label = np), nudge_y = -5) +
  scale_y_continuous(limits = c(0, 220)) +
  scale_x_continuous(limits = c(0, 400)) +
  xlab("Lagged distance (h) [km]") +
  ylab(expression(paste("Semivariance (", gamma, ") [", cm^2, "]"))) +
  theme_minimal() )
```

Values start low (around 50 cm$^2$) at the shortest lag distance and increase to greater than 200 cm$^2$ at lag distances of 200 km and longer.

The semivariance a lag zero is called the 'nugget' and the semivariance at a level where the variogram values no longer increase is called the 'sill.' The difference between the sill and the nugget is called the 'partial sill'.  The lag distance out to where the sill is reached is called the 'range.'  These three parameters (nugget, partial sill, and range) are used to model the variogram.

Next fit a model to the empirical variogram. The model is a mathematical relationship that defines the semivariance as a function of lag distance. First save the family and the initial parameter guesses in a variogram model (`vmi`) object.

```{r}
vmi <- vgm(model = "Gau", 
           psill = 150, 
           range = 200 * 1000, 
           nugget = 50)
vmi
```

The `psill` argument is the partial sill (the difference between the sill and the nugget) along the vertical axis. Estimate the parameter values by looking at the empirical variogram. 

Next use the `fit.variogram()` function to improve the fit over these initial values. Given a set of initial parameter values the method of weighted least squares is used to improve the parameter estimates.

```{r}
vm <- fit.variogram(object = v, 
                    model = vmi)
vm
```

The result is a variogram model with a nugget of 46.6 cm$^2$, a partial sill of 156 cm$^2$, and a range on the sill of 128 km. 

Plot the model on top of the empirical variogram. Let $r$ be the range, $c$ the partial sill and $c_o$ the nugget, then the equation defining the function over the set of lag distances $h$ is

$$
\gamma(h)=c\left(1-\exp\left(-\frac{h^2}{r^2}\right)\right)+c_o
$$

Create a data frame with values of h and gamma using this equation.

```{r}
nug <- vm$psill[1]
ps <- vm$psill[2]
r <- vm$range[2] / 1000
h <- seq(0, 400, .2)
gamma <- ps * (1 - exp(-h^2 / (r^2))) + nug

vm.df <- data.frame(dist = h,
                    gamma = gamma)

pv + geom_line(aes(x = dist, y = gamma), data = vm.df)
```

Check for anisotropy. Anisotropy refers to a dependence of the variogram shape on the direction of the location pairs used to compute semivariances. Isotropy refers to a directional independence.

```{r}
plot(variogram(tpm ~ 1, 
               data = FR.sf, 
               alpha = c(0, 45, 90, 135),
               cutoff = 400000), 
     xlab = "Lag Distance (m)")
```

The semivariance values reach the sill at a longer range  (about 300 km) in the north-south direction (0 degrees) compared to the other three directions.

Another way to look at directional dependence in the variogram is through a variogram map. Instead of classifying point pairs Z(s) and Z(s + h) by direction and distance class separately, you classify them jointly.

If h = {x, y} is the two-dimensional coordinates of the separation vector, in the variogram map the variance contribution of each point pair (Z(s) − Z(s + h))^2 is attributed to the grid cell in which h lies. The map is centered at (0, 0) and h is lag distance. Cutoff and width correspond to map extent and cell size; the semivariance map is point symmetric around (0, 0), as γ(h) = γ(−h).

The variogram map is made with the `variogram()` function by adding the `map = TRUE` argument. Here you set the cutoff to be 200 km (200,000 m) and the width (cell size) to be 20 km.

```{r}
vmap <- variogram(tpm ~ 1, 
                  data = FR.sf,
                  cutoff = 200000,
                  width = 20000,
                  map = TRUE)
plot(vmap)
```

The variogram map is centered on dx = 0 and dy = 0. Along the dx = 0 vertical line in the north-south direction (top-to-bottom on the plot) the semivariance values increase away from dy = 0, but the increase is less compared to along the dy = 0 horizontal line in the east-west direction (left-to-right on the plot) indicative of directional dependency.

You refit the variogram model defining an anisotropy ellipse with the `anis =` argument. The first parameter is the direction of longest range (here north-south) and the second parameter is the ratio of the longest to shortest ranges. Here about (200/300 = .67).

```{r}
vmi <- vgm(model = "Gau", 
           psill = 150, 
           range = 300 * 1000, 
           nugget = 50,
           anis = c(0, .67))
vm <- fit.variogram(v, vmi)
```

Use the variogram model together with the rainfall values at the observation sites to create an interpolated surface. Here you use ordinary kriging as there are no spatial trends in the rainfall.

Interpolation is done using the `krige()` function. The first argument is the model specification and the second is the data. Two other arguments are needed. One is the variogram model using the argument name `model =` and the other is a set of locations identifying where the interpolations are to be made. This is specified with the argument name `newdata =`.

Here you interpolate to locations on a regular grid. You create a grid of locations within the borders of the state using the `st_sample()` function.

```{r}
grid.sf <- sf::st_sample(FL.sf,
                         size = 5000,
                         type = "regular")
```

You specify the number of grid locations using the argument `size =`. Note that the actual number of locations will be somewhat different because of the irregular boundary.

First use the `krige()` function to interpolate the observed rainfall to the grid locations. For a given location, the interpolation is a weighted average of the rainfall across the entire region where the weights are determined by the variogram model.

```{r}
r.int <- krige(tpm ~ 1, 
               locations = FR.sf, 
               newdata = grid.sf,
               model = vm)
```

If the variogram model is not included then inverse distance-weighted interpolation is performed. The function will not work if different values share the same location.

The saved object (`r.int`) inherits the spatial geometry specified in the `newdata =` argument but extends it to a spatial data frame. The column `var1.pred` in the data frame is the interpolated rainfall and the second `var1.var` is the variability about the interpolated value.

Plot the interpolated storm-total rainfall field.

```{r}
tmap::tm_shape(r.int) +
  tmap::tm_dots("var1.pred",
                size = .1,
                palette = "Greens",
                title = "Rainfall (cm)") +
  tmap::tm_shape(FL.sf) +
  tmap::tm_borders() +
  tmap::tm_layout(legend.position = c("left", "bottom"),
                  title = "TC Fay (2008)",
                  title.position = c("left", "bottom"),
                  legend.outside = TRUE)
```

Note: a portion of the data locations are outside of the state but interest is only interpolated values within the state border as specified by the `newdata =` argument.

The spatial interpolation shows that parts of east central and north Florida were deluged by Fay with rainfall totals exceeding 30 cm (12 in).

Block kriging

The interpolation can also be done as an area average. For example what was the storm-total average rainfall for each county?

County level rainfall is relevant for water resource managers. Block kriging produces an estimate of this area average, which will differ from a simple average over all sites within the county because of the spatial autocorrelation in rainfall observations.

You use the same function to interpolate but specify the spatial polygons rather than the spatial grid as the new data. Here the spatial polygons are the county borders.

```{r}
r.int2 <- krige(tpm ~ 1, 
                locations = FR.sf, 
                newdata = FL.sf, 
                model = vm)
```

Again plot the interpolations.

```{r}
tmap::tm_shape(r.int2) +
  tmap::tm_polygons(col = "var1.pred",
                    palette = "Greens",
                    title = "Rainfall (cm)") +
  tmap::tm_layout(legend.position = c("left", "bottom"),
                  title = "TC Fay (2008)",
                  title.position = c("left", "bottom"))
```

The overall pattern of rainfall from Fay featuring the largest amounts along the central east coast and over the Big Bend region are similar in both maps but these estimates answer questions like on average how much rain fell over Leon County during TC Fay? 

You compare the kriged average with the simple average at the county level with the `aggregate()` method. The argument `FUN = mean` says to compute the average of the values in `FR.sf` across the polygons in `FL.sf`.

```{r}
r.int3 <- aggregate(FR.sf, 
                    by = FL.sf, 
                    FUN = mean)
```

The result is a simple feature data frame of the average rainfall in each county.

The state-wide mean of the kriged estimates at the county level is

```{r}
round(mean(r.int2$var1.pred), 2)
```

This compares with a state-wide mean from the simple averages.

```{r}
round(mean(r.int3$tpm), 2)
```

The correlation between the two estimates across the 67 counties is 

```{r}
round(cor(r.int3$tpm, r.int2$var1.pred), 2)
```

The variogram model reduces the standard deviation of the kriged estimate relative to the standard deviation of the simple averages because of the local smoothing.

```{r}
round(sd(r.int2$var1.pred), 2)
round(sd(r.int3$tpm), 2)
```

This can be seen with a scatter plot of simple averages versus kriged averages at the county level.

```{r}
compare.df <- data.frame(simpleAvg = r.int3$tpm,
                         krigeAvg = r.int2$var1.pred)
ggplot(compare.df, aes(x = simpleAvg,
                       y = krigeAvg)) +
  geom_point() +
  geom_abline(slope = 1) +
  geom_smooth(method = lm, se = FALSE)
```

An advantage of kriging as a method of spatial interpolation is the accompanying uncertainty estimates. The prediction variances are listed in a column in the spatial data frame saved from apply the `krige()` function. Variances are smaller in regions with more rainfall observations.  

Prediction variances are also smaller with block kriging as much of the variability within the county averages out. To compare the distribution characteristics of the prediction variances for the point and block kriging of the rainfall observations, type

```{r}
round(summary(r.int$var1.var), 1)
round(summary(r.int2$var1.var), 1)
```

The median prediction variance (in cm$^2$) for the point kriging is close to the value of the nugget.

```{r}
round(fivenum(r.int$var1.var)[3], 1)
```

In contrast, the median prediction variance for our block kriging is a much smaller.

```{r} 
round(fivenum(r.int2$var1.var)[3], 1)
```

## Simulating spatial fields {-}

Simulations use this uncertainty to provide additional data for deterministic models. Suppose for example you have a hydrology model of rainfall runoff. Given a spatial field of rain amounts the model predicts a discharge rate at some location along a river. The uncertainty in the predicted runoff rate at the location is due to the uncertainty in where and how hard the rain fell (in the rainfall field) and not due to the deterministic hydrology model.

The uncertainty in the rainfall field is simulated conditional on the observations with the same `krige()` function by adding the argument `nsim =` that specifies the number of simulations.  

For a large number it may be necessary to limit the number neighbors in the kriging. This is done using the `nmax` argument. For a given location, the weights assigned to observations far away are very small, so it is efficient to limit how many are used in the simulation.

As an example, here you generate four realizations of the county-level average storm total rainfall for Fay and limit the neighborhood to 50 of the closest observation sites. This takes a few seconds.

```{r}
r.sim <- krige(tpm ~ 1, 
               locations = FR.sf, 
               newdata = FL.sf, 
               model = vm, 
               nsim = 4, 
               nmax = 50)
```

Given the variogram model, the simulations are conditional on the observed rainfall.

```{r}
tmap::tm_shape(r.sim) +
    tmap::tm_polygons(col = c("sim1", "sim2", "sim3", "sim4"),
                palette = "Greens",
                title = "Simulated Rainfall [cm]") +
    tmap::tm_facets(free.scales = FALSE) 
```

The overall pattern of rainfall remains the same, but there are differences especially in counties with fewer observations and in counties where the rainfall gradients are sharp.

## Interpolating multiple variables {-}

Spatial interpolation can be extended to obtain surfaces of multiple variables. The idea is that if two field variables are correlated then information about the spatial correlation in one field variable can help provide information about values in the other field variable. The spatial variability of one variable is correlated with the spatial variability of the other variable. And this idea is not limited to two variables. 

Here you consider observations of heavy metal concentrations (ppm) in the top soil in the flood plain of the river Meuse near the village of Stein. The data are available in {sp} package.

```{r}
library(sp)

data(meuse)
names(meuse)
```

The metals include cadmium, copper, lead, and zinc. Observation locations are given by x and y. Other variables include elevation, soil type and distance to the river.

Create a simple feature data frame with a projected coordinate system for the Netherlands.

```{r}
meuse.sf <- sf::st_as_sf(x = meuse,
                         coords = c("x", "y"),
                         crs = 28992)
```

Interest is on the spatial distribution of all four heavy metals in the soil.

Map the concentrations at the observation locations.

```{r}
tmap::tmap_mode("view")
tmap::tm_shape(meuse.sf) +
  tmap::tm_dots(col = c("cadmium", "copper", "lead", "zinc"))
```

All observations (bulk sampled from an area of approximately 15 m x 15 m) have units of ppm. The most abundant heavy metal is zinc followed by lead and copper. For all metals highest concentrations are found nearest to the river. Thus you want to include distance to river as a covariate (trend term) and use universal kriging.

The distribution of concentrations is skewed with many locations having only low levels of heavy metals with a few having very high levels.

```{r}
ggplot(data = meuse.sf,
       mapping = aes(x = lead)) +
  geom_histogram(bins = 17) +
  theme_minimal()
```

Thus you use a logarithmic transformation.

First you organize the data as a `gstat` object. This is done with the `gstat()` function which orders (and copies) the variables into a single object. Ordering is done successively.

Here you specify the trend using the square root of the distance to river and take the natural logarithm of the heavy metal concentration. You give the dependent variable a new name with the `id =` argument.

```{r}
g <- gstat(id = "logCd", 
           formula = log(cadmium) ~ sqrt(dist), 
           data = meuse.sf)
g <- gstat(g, 
           id = "logCu", 
           formula = log(copper) ~ sqrt(dist), 
           data = meuse.sf)
g <- gstat(g, 
           id = "logPb", 
           formula = log(lead) ~ sqrt(dist), 
           data = meuse.sf)
g <- gstat(g, 
           id = "logZn",
           formula = log(zinc) ~ sqrt(dist), 
           data = meuse.sf)
g
```

Next you use the `variogram()` function to compute empirical variograms. The function, when operating on a `gstat` object, computes all direct and cross variograms.

```{r}
v <- variogram(g)
plot(v)
```

The plot method displays the set of direct and cross variograms. The direct variograms are shown in the four panels along the diagonal of the triangle of plots.

The cross variograms are shown in the six panels below the diagonal. For example, the cross variogram between the values of cadmium and copper is given in the second row of the first column and so on. 

The cross variogram is analogous to the multi-type $K$ function for analyzing point pattern data.

The cross variograms show small semivariance values at short lag distance with increasing semivariance values at longer lags. Because these variables are co-located, you can also compute direct correlations.

```{r}
cor(meuse[c("cadmium", "copper", "lead", "zinc")])
```

The direct correlation between cadmium and copper is .92 and between cadmium and lead is .8.

The correlation matrix confirms strong cross correlation among the four variables at zero lag. The cross variogram generalizes these correlations across lag distance. For instance, the cross variogram indicates the strength of the relationship between cadmium at one location and copper at nearby locations.

You use the `fit.lmc()` function to fit separate variogram models to each of the empirical variograms. You use an initial partial sill of .5, an initial nugget of zero and an initial range of 800 meters.
```{r}
vm <- fit.lmc(v, g, 
              vgm(model = "Sph", 
                  psill = .5,
                  nugget = 0,
                  range = 800))
plot(v, vm)
```

The final variogram models (blue line) fit the empirical variogram (direct and cross) well.

Given the variogram models, co-kriged maps are produced using the `predict()` method after setting the grid locations for the interpolations. The CRS for the grid locations must match the CRS of the data.

```{r}
data(meuse.grid)

grid.sf <- sf::st_as_sf(x = meuse.grid,
                        coords = c("x", "y"),
                        crs = 28992)

hm.int <- predict(vm, grid.sf)
names(hm.int)
```

Plot the interpolated for logarithm of zinc concentration are plotted.
```{r}
tmap::tmap_mode("plot")
tmap::tm_shape(hm.int) +
  tmap::tm_dots(col = c("logCd.pred", "logCu.pred", "logPb.pred", "logZn.pred"), 
                size = .2, breaks = seq(-2, 8, by = 1), palette = "Reds", midpoint = NA)
```

The pattern of heavy metal concentrations are similar with highest values along the river bank. 

Compare with predictions using only cadmium
```{r}
v2 <- variogram(log(cadmium) ~ sqrt(dist), 
                data = meuse.sf)
vm2 <- fit.variogram(v2, vgm(psill = .15, model = "Sph", 
                             range = 800, nugget = .1))
int <- krige(log(cadmium) ~ sqrt(dist), meuse.sf, newdata = grid.sf, 
                 model = vm2)

p1 <- tmap::tm_shape(int) +
        tmap::tm_dots(col = "var1.pred", 
                      size = .2, palette = "Reds", breaks = seq(-2, 3, by = .5))
p2 <- tmap::tm_shape(hm.int) +
        tmap::tm_dots(col = "logCd.pred", 
                      size = .2, palette = "Reds", breaks = seq(-2, 3, by = .5))
tmap::tmap_arrange(p1, p2)

cor(hm.int$logCu.pred, int$var1.pred)
```

Only minor differences are visible on the plot and the correlation between the two interpolations exceeds .9.

Plot the covariances between zinc and cadmium.

```{r}
tmap::tm_shape(hm.int) +
  tmap::tm_dots(col = "cov.logCd.logZn", size = .2)
```

The map shows areas of the flood plain with high (and low) correlations between cadmium and zinc. Caution: Higher values of the covariance indicate lower correlations. There is an inverse relationship between the correlogram and the covariogram.

Obtaining a quality statistical spatial interpolation is a nuanced process but with practice kriging can be an important tool in your toolbox.

Kriging is useful tool for ‘filling in the gaps’ between sampling sites. Handy if you want to make a map, or need to match up two spatial data sets that overlap in extent, but have samples at different locations.

## Machine learning for spatial interpolation {-}

See https://geocompr.robinlovelace.net/spatial-cv.html

<!--chapter:end:23-Lesson.Rmd-->

