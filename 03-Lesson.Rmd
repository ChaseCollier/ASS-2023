# Thursday January 19, 2023 {-}

**"When I'm explaining some of the tidy verse principles and philosophy in R statistics, I often break down a home baked chunk of code and illustrate that 'it says what it does and it does what it says.'** --- Diane Beldame

## Getting data into R {-}

You often start a research project by getting data into R. The file is on your computer or in the cloud. Secondary source data should be imported directly from repositories on the Web. When there is no direct link or API (application programming interface) to the repository, you need to first download the data

For example, consider the annually updated reports of tornadoes in the United States. The data repository is the Storm Prediction Center (SPC) <https://www.spc.noaa.gov/wcm/index.html#data>

Here you are interested in the file `1950-2021_actual_tornadoes.csv`. First download the file from the site with the `download.file()` function specifying the location (`url =`) and a name you want the file to be called on your computer (`destfile =`)

```{r}
download.file(url = "http://www.spc.noaa.gov/wcm/data/1950-2021_actual_tornadoes.csv",
              destfile = here::here("data", "Tornadoes.csv"))
```

A file called `Tornadoes.csv` is now be located in the directory `data`. Click on the *Files* tab in the lower-right panel, then select the `data` folder

Next you read (import) the file as a data frame using the `readr::read_csv()` function from the {tidyverse} group of packages.

```{r}
Torn.df <- readr::read_csv(file = here::here("data", "Tornadoes.csv"))
```

You preview the data frame using the `head()` function

```{r}
head(Torn.df)
```

Each row is a unique tornado report. Observations for each report include variables like the day and time, the state (`st`), the maximum EF rating (`mag`), the number of injuries (`inj`), the number of fatalities (`fat`), estimated property losses (`loss`), estimated crop losses (`closs`), start and end locations in decimal degrees longitude and latitude, length of the damage path in miles (`len`), width of the damage in yards (`wid`)

The total number of tornado reports in the data set is returned using the `nrow()` function

```{r}
nrow(Torn.df)
```

To create a subset of the data frame that contains only tornadoes in years (`yr`) since 2001, you include the logical operator `yr >= 2001` inside the subset operator (`[]`). The logical operator is placed in front of the comma since you want all *rows* where the result of the operator returns a value `TRUE`. After the column is left blank since you want all the variables

```{r}
Torn2.df <- Torn.df[Torn.df$yr >= 2001, ]
```

You see that there are fewer rows (tornado reports) in this new data frame assigned the object name `Torn2.df` as expected since you created a subset

You subset again, keeping only tornadoes with EF ratings (`mag` variable) greater than zero. Here you *recycle* the name `Torn2.df`

```{r}
Torn2.df <- Torn2.df[Torn2.df$mag > 0, ]
```

Now you compute the correlation between EF rating (`mag`) and path length (`len`) with the `cor()` function. The first argument is the vector of EF ratings and the second argument is the vector of path lengths

```{r}
cor(Torn2.df$mag, Torn2.df$len)
```

Longer tornadoes tend to be associated with worse damage. Why do you think that is the case?

Path length is recorded in miles and path width in yards and the EF damage rating variable `mag` is numeric. Since as a scientist you do all calculations using the metric system, you convert path length and width to meters, and the EF rating to a factor and include these changes as new columns in the data frame

```{r}
Torn2.df$Length_m <- Torn2.df$len * 1609.34
Torn2.df$Width_m <- Torn2.df$wid * .9144
Torn2.df$EF <- factor(Torn2.df$mag)
```

Create side-by-side box plots of path length by EF rating. Here you include a division by 1000 so the vertical scale has units of kilometers

```{r}
plot(x = Torn2.df$EF, 
     y = Torn2.df$Length_m/1000)
```

Another example: Statewide average rainfall in Florida. The data are monthly statewide average rainfall (in inches) for Florida starting in 1895. The data were obtained from <http://www.esrl.noaa.gov/psd/data/timeseries/>. I put values into a text editor and then uploaded the file to <http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt>

You read the data and assign the data object the name `FLp.df`. You type the name of the object to see that it is a tabled data frame (tibble) with 117 rows and 13 columns

```{r}
loc <- "http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt"
FLp.df <- readr::read_table(file = loc)
FLp.df
```

The first column is the year and the next 12 columns are the months

What was the statewide rainfall during June of 1900? Start by asking what happens when you check the column (vector) labeled `Year` to see what elements are equal to 1900?

```{r}
FLp.df$Year == 1900
```

Returns a vector with one element equal to `TRUE`. Next, what happens when you use this vector to select the vector of June rainfall values?

```{r}
FLp.df$Jun[FLp.df$Year == 1900]
```

What year had the wettest March?

```{r}
FLp.df$Mar
max(FLp.df$Mar)
which.max(FLp.df$Mar)
FLp.df$Year[which.max(FLp.df$Mar)]
```

What month during 1965 was the wettest? How wet was it?

```{r}
FLp.df$Year == 1965
FLp.df[FLp.df$Year == 1965, ]
which.max(FLp.df[FLp.df$Year == 1965, 2:12])
which.max(FLp.df[FLp.df$Year == 1965, 2:12])
max(FLp.df[FLp.df$Year == 1965, 2:12])
```

## Working with data frames using functions from {dplyr} {-}

The functions in the {dplyr} package (part of the tidyverse set of packages) simplify working with data frames. The functions work only on data frames

Function names are English language *verbs* so they are easier to remember. The verbs help you translate thought into code

Here we consider some of verbs using the `airquality` data frame. The data frame contains air quality measurements taken in New York City between May and September 1973 (`?airquality`) at time when pollution levels in the city were at an all time high

```{r}
dim(airquality)
head(airquality)
```

The columns include `Ozone` (ozone concentration in ppb), `Solar.R` (solar radiation in langleys), `Wind` (wind speed in mph), `Temp` (air temperature in degrees F), `Month`, and `Day`

You get summary statistics on the values in each column with the `summary()` method

```{r}
summary(airquality)
```

Note that columns that have missing values are tabulated. For example, there are 37 missing ozone measurements and 7 missing radiation measurements

Importantly for making your code more human readable you can apply the `summary()` function on the `airquality` data frame using the pipe operator (`|>`)

```{r}
airquality |> summary()
```

You read the pipe as THEN. "take the `airquality` data frame THEN summarize the columns"

The pipe operator allows you to string together functions that when read by a human makes it easy to understand what is being done. Think of it this way: suppose the object of interest is called `me` and there is a function called `wake_up()`. I could apply this function called `wake_up()` in two ways

```{r, eval=FALSE}
wake_up(me)  # classic

me |> wake_up()  # tidyverse
```

The second way involves more typing but it is easier to read (the subject comes before the predicate like simple English grammar) and thus easier to interpret. This becomes clear when stringing many functions together

Continuing, what happens to the result of `me` after the function `wake_up()` has been applied? I `get_out_of_bed()` and then `get_dressed()`

Again, you can apply these functions in two ways

```{r, eval=FALSE}
get_dressed(get_out_of_bed(wake_up(me))) # classic

me |> # tidyverse
  wake_up() |>
  get_out_of_bed() |>
  get_dressed()
```

The order of the functions often matters. I can't get dressed before I wake up but I could get dressed without getting out of bed

Note how I format the code. Each line is gets only one verb and each line ends with the pipe (except the last one). This makes it easy to read. Continuing

```{r, eval=FALSE}
me |>
  wake_up() |>
  get_out_of_bed() |>
  get_dressed() |>
  make_coffee() |>
  drink_coffee() |>
  leave_house()
```

Which is much better in terms of 'readability' then `leave_house(drink_coffee(make_coffee(get_dressed(get_out_of_bed(wake_up(me))))))`

Now lets apply the verbs to a data frame. The function `select()` chooses variables by name. For example, choose the month (`Month`), day (`Day`), and temperature (`Temp`) columns

```{r}
airquality |>
  dplyr::select(Month, Day, Temp)
```

The result is a data frame containing only the three columns with column names listed in the `select()` function

Suppose you want a new data frame with only the temperature and ozone concentrations. You include an assignment operator (`<-`) and an object name (here `df`)

```{r}
df <- airquality |>
        dplyr::select(Temp, Ozone)
df
```

The verbs take data frames as input and return data frames

The function `filter()` chooses observations based on specific values. Suppose you want only the observations where the temperature is at or above 80 F

```{r}
airquality |>
  dplyr::filter(Temp >= 80)
```

The result is a data frame with the same 6 columns but now only 73 observations. Each of the observations has a temperature of at least 80 F

Suppose you want a new data frame keeping only observations when temperature is at least 80 F and when winds are less than 5 mph

```{r}
df <- airquality |> 
  dplyr::filter(Temp >= 80 & Wind < 5)
df
```

The function `arrange()` orders the rows by values given in a particular column

```{r}
airquality |>
  dplyr::arrange(Solar.R)
```

The ordering is done from the lowest value of radiation to highest value. Here you see the first 10 rows. Note `Month` and `Day` are no longer chronological

Repeat, but order by the value of air temperature

```{r}
airquality |>
  dplyr::arrange(Temp)
```

Importantly you can string the functions together. For example select the variables radiation, wind, and temperature then filter by temperatures above 90 F and arrange by temperature

```{r}
airquality |>
  dplyr::select(Solar.R, Wind, Temp) |>
  dplyr::filter(Temp > 90) |>
  dplyr::arrange(Temp)
```

The result is a data frame with three columns and 14 rows arranged by increasing temperatures above 90 F

The `mutate()` function adds new columns to the data frame. For example, create a new column called `TempC` as the temperature in degrees Celsius. Also create a column called `WindMS` as the wind speed in meters per second

```{r}
airquality |>
  dplyr::mutate(TempC = (Temp - 32) * 5/9,
                WindMS = Wind * .44704) 
```

The resulting data frame has 8 columns (two new ones) labeled `TempC` and `WindMS`

On days when the temperature is below 60 F add a column giving the apparent temperature based on the cooling effect of the wind (wind chill) and then arrange from coldest to warmest apparent temperature

```{r}
airquality |>
  dplyr::filter(Temp < 60) |>
  dplyr::mutate(TempAp = 35.74 + .6215 * Temp - 35.75 * Wind^.16 + .4275 * Temp * Wind^.16) |>
  dplyr::arrange(TempAp)
```

The `summarize()` function reduces the data frame based on a function that computes a statistic. For example, to compute the average wind speed during July or the average temperature during June type

```{r}
airquality |>
  dplyr::filter(Month == 7) |>
  dplyr::summarize(Wavg = mean(Wind))

airquality |>
  dplyr::filter(Month == 6) |>
  dplyr::summarize(Tavg = mean(Temp))
```

You've seen functions that compute statistics including `sum()`, `sd()`, `min()`, `max()`, `var()`, `range()`, `median()`. Others include

|      Summary function | Description               |
|----------------------:|:--------------------------|
|          `dplyr::n()` | Length of the column      |
|      `dplyr::first()` | First value of the column |
|       `dplyr::last()` | Last value of the column  |
| `dplyr::n_distinct()` | Number of distinct values |

Find the maximum and median wind speed and maximum ozone concentration values during the month of May. Also determine the number of observations during May

```{r}
airquality |>
  dplyr::filter(Month == 5) |>
  dplyr::summarize(Wmax = max(Wind),
                   Wmed = median(Wind),
                   OzoneMax = max(Ozone),
                   NumDays = dplyr::n())
```

The result gives an `NA` for the maximum value of ozone (`OzoneMax`) because there is at least one missing value in the `Ozone` column. You fix this with the `na.rm = TRUE` argument in the function `max()`

```{r}
airquality |>
  dplyr::filter(Month == 5) |>
  dplyr::summarize(Wmax = max(Wind),
                   Wmed = median(Wind),
                   OzoneMax = max(Ozone, na.rm = TRUE),
                   NumDays = dplyr::n())
```

If you want to summarize separately for each month you use the `group_by()` function. You split the data frame by some variable (e.g., `Month`), apply a function to the individual data frames, and then combine the output

Find the highest ozone concentration by month. Include the number of observations (days) in the month

```{r}
airquality |>
  dplyr::group_by(Month) |>
  dplyr::summarize(OzoneMax =  max(Ozone, na.rm = TRUE),
                   NumDays = dplyr::n())
```

Find the average ozone concentration when temperatures are above and below 70 F. Include the number of observations (days) in the two groups

```{r}
airquality |>
  dplyr::group_by(Temp >= 70) |>
  dplyr::summarize(OzoneAvg =  mean(Ozone, na.rm = TRUE),
                   NumDays = dplyr::n())
```

On average ozone concentration is higher on warm days (Temp >= 70 F) days. Said another way; mean ozone concentration statistically depends on temperature

The mean is a model for the data. The statistical dependency of the mean implies that a model for ozone concentration will be improved by including temperature as an explanatory variable

In summary, the important verbs are

|          Verb | Description                                                                    |
|--------------------------------------:|:--------------------------------|
|    `select()` | selects columns; pick variables by their names                                 |
|    `filter()` | filters rows; pick observations by their values                                |
|   `arrange()` | re-orders the rows                                                             |
|    `mutate()` | creates new columns; create new variables with functions of existing variables |
| `summarize()` | summarizes values; collapse many values down to a single summary               |
|  `group_by()` | allows operations to be grouped                                                |

The six functions form the basis of a grammar for data. You can only alter a data frame by reordering the rows (`arrange()`), picking observations and variables of interest (`filter()` and `select()`), adding new variables that are functions of existing variables (`mutate()`), collapsing many values to a summary (`summarise()`), and conditioning on variables (`group_by()`)

The syntax of the functions are all the same

-   The first argument is a data frame. This argument is implicit when using the `|>` operator
-   The subsequent arguments describe what to do with the data frame. You refer to columns in the data frame directly (without using `$`)
-   The result is a new data frame

These properties make it easy to chain together many simple lines of code to do complex data manipulations and summaries all while making it easy to read by humans

Working with data frames is part of the cycle of data science, along with visualizing, and modeling. The cycle of data science

1.  Generate questions about your data
2.  Look for answers by visualizing and modeling the data after the data are in suitably arranged data frames
3.  Use what you learn to refine your questions and/or ask new ones

Questions are tools to guide your investigation. When you ask a question, the question focuses your attention on a specific part of the data and helps you decide what to do next

For more practice working with data frames using functions from the {tidyverse} set of packages

-   See <http://r4ds.had.co.nz/index.html>
-   Cheat sheets <https://www.rstudio.com/resources/cheatsheets/>

## Making graphs {-}

The {ggplot2} package is a popular graphics tool among data scientists (e.g., New York Times and 538). Functionality is built on principles of good data visualization

1.  Map data to aesthetics
2.  Layer
3.  Build in steps

You make the functions available to your current working directory by typing

```{r}
library(ggplot2)
```

Consider the following numeric vectors (`foo`, `bar` and `zaz`). Create a data frame `df` using the `data.frame()` function

```{r}
foo <- c(-122.419416,-121.886329,-71.05888,-74.005941,-118.243685,-117.161084,-0.127758,-77.036871,
         116.407395,-122.332071,-87.629798,-79.383184,-97.743061,121.473701,72.877656,2.352222,
         77.594563,-75.165222,-112.074037,37.6173)

bar <- c(37.77493,37.338208,42.360083,40.712784,34.052234,32.715738,51.507351,38.907192,39.904211,
         47.60621,41.878114,43.653226,30.267153,31.230416,19.075984,48.856614,12.971599,39.952584,
         33.448377,55.755826)

zaz <- c(6471,4175,3144,2106,1450,1410,842,835,758,727,688,628,626,510,497,449,419,413,325,318)

df <- data.frame(foo, bar, zaz)

head(df)
```

To make a scatter plot you use the `ggplot()` function. Note that the package name is {ggplot2} but the function is `ggplot()` (without the 2)

Inside the `ggplot()` function you first specify the data frame with the `data =` argument. You next specify what columns from the data frame are to be mapped to what 'aesthetics' with the `aes()` function using the `mapping =` argument. The `aes()` function is nested inside the `ggplot()` function or inside a layer function

For a scatter plot the aesthetics must include the x and y coordinates. For this example they are in the columns labeled `foo` and `bar` respectively

Then to render the scatter plot you include the function `geom_point()` as a layer with the `+` symbol. Numeric values are specified using the arguments `x =` and `y =` in the `aes()` function and are rendered as points on a plot

```{r}
ggplot(data = df, 
       mapping = aes(x = foo, y = bar)) +
  geom_point()
```

You map data values to aesthetic attributes. The *points* in the scatter plot are geometric objects that get drawn. In {ggplot2} lingo, the points are *geoms*. More specifically, the points are point *geoms* that are denoted syntactically with the function `geom_point()`

All geometric objects have aesthetic attributes (aesthetics): x-position, y-position, color, size, transparency

You create a mapping between variables in your data frame to the aesthetic attributes of geometric objects. In the scatter plot you mapped `foo` to the x-position aesthetic and `bar` to the y-position aesthetic. This may seem trivial `foo` is the x-axis and `bar` is on the y-axis

Here there is a deeper structure. Geometric objects (i.e., the things you draw in a plot, like points) don't just have position attributes, they also have a color, size, etc

Here you map a new variable to the size aesthetic.

```{r}
ggplot2::ggplot(data = df, 
                mapping = ggplot2::aes(x = foo, y = bar)) +
  ggplot2::geom_point(mapping = ggplot2::aes(size = zaz))
```

You changed the scatter plot to a bubble chart by mapping a new variable to the size aesthetic. Any visualization can be deconstructed into *geom* specifications and a mapping from data to aesthetic attributes of the geometric objects.

The principle of layering is important. To create good visualizations you often need to:

-   Plot multiple data sets
-   Plot data with additional contextual information contained in other data
-   Plot summaries or statistical transformations from the data

Let's modify the bubble chart by getting additional data and plotting it as a new layer below the bubbles. First get the data from the {ggplot2} package using the `map_data()` function and specifying the name of the map (here `"world"`) and assigning it to a data frame with the name `df2`.

```{r}
df2 <- ggplot2::map_data(map = "world") |>
  dplyr::glimpse()
```

Plot the new data as a new layer underneath the bubbles.

```{r}
ggplot(data = df, 
       mapping = aes(x = foo, y = bar)) +
  geom_polygon(data = df2, 
               mapping = aes(x = long, y = lat, group = group)) +
  geom_point(mapping = aes(size = zaz), color = "red")
```

This is the same bubble chart but now with a new layer added. You changed the bubble chart into a new visualization called a dot distribution map, which is more insightful and visually more interesting

The bubble chart is a modified scatter plot and the dot distribution map is a modified bubble chart

You used two of the data visualization principles (mapping & layering) to build this plot

-   To create the scatter plot, you mapped `foo` to the x-aesthetic and mapped `bar` to the y-aesthetic
-   To create the bubble chart, you mapped a `zaz` to the size-aesthetic
-   To create the dot distribution map, you added a layer of polygon data under the bubbles

The third principle is about process. The graphing process begins with mapping and layering but ends with iteration when you add layers that modify scales, legends, colors, etc. The syntax of `ggplot` *layerability* enables and rewards iteration

Instead of plotting the result of the above code for making a bubble chart, assign the result to an object called `p1`. Coping/paste the code from above but then include the assignment operator `p1 <-`

```{r}
p1 <- ggplot(data = df, 
             mapping = aes(x = foo, y = bar)) +
        geom_polygon(data = df2, 
                     mapping = aes(x = long, y = lat, 
                                   group = group)) +
        geom_point(mapping = aes(size = zaz), color = "red")
```

Now modify the axes labels saving the new plot to an object called `p2`

```{r}
( p2 <- p1 + xlab("Longitude") + ylab("Latitude") )
```

Next modify the scale label

```{r}
p2 + scale_size_continuous(name = "Venture Capital Investment\n(USD, Millions)\n")
```

Of course you can do these steps together

```{r}
p1 + xlab("Longitude") + 
     ylab("Latitude") +
     scale_size_continuous(name = "Venture Capital Investment\n(USD, Millions)\n")
```

The `facet_wrap()` function is a layer to iterate (repeat) the entire plot conditional on another variable. It is like the `dplyr::group_by()` function in the data grammar

Example: U.S. tornadoes: Consider the tornado records in the file `Tornadoes.csv`. Import the data using the `readr::read_csv()` function then create new columns called `Year`, `Month` and `EF` using the `dplyr::mutate()` function

```{r}
( Torn.df <- readr::read_csv(file = here::here("data", "Tornadoes.csv")) |>
  dplyr::mutate(Year = yr,
                Month = as.integer(mo),
                EF = mag) )
```

Next create a new data frame (`df`) that contains the number of tornadoes by year for the state of Kansas
  
```{r}
( df <- Torn.df |>
  dplyr::filter(st == "KS") |>
  dplyr::group_by(Year) |>
  dplyr::summarize(nT = dplyr::n()) )
```

Then use the functions from the {ggplot2} package to plot the number of tornadoes by year using lines to connect the values in order of the variable on the x-axis

```{r}
ggplot(data = df,
       mapping = aes(x = Year, y = nT)) +
  geom_line() +
  geom_point()
```

Note: In the early production stage of research, I like to break the code into steps as above: (1) Import the data, (2) manipulate the data, and (3) plot the data. It is easier to document but it  introduces the potential for mistakes because of the intermediary objects in the environment (e.g., `Torn.df`, `df`)

Below you bring together the above code to create the time series of Kansas tornado frequency without creating intermediary objects

```{r, eval=FALSE}
readr::read_csv(file = here::here("data", "Tornadoes.csv")) |>
  dplyr::mutate(Year = yr,
                Month = as.integer(mo),
                EF = mag) |>
  dplyr::filter(st == "KS") |>
  dplyr::group_by(Year) |>
  dplyr::summarize(nT = dplyr::n()) |>
ggplot(mapping = aes(x = Year, y = nT)) +
  geom_line() +
  geom_point()
```

Recall that the `group_by()` function allows you to repeat an operation depending on the value (or level) of some variable. For example to count the number of tornadoes by EF damage rating since 2007 and ignoring missing ratings

```{r}
Torn.df |>
  dplyr::filter(Year >= 2007, EF != -9) |>
  dplyr::group_by(EF) |>
  dplyr::summarize(Count = dplyr::n()) 
```

The result is a table listing the number of tornadoes grouped by EF rating

Instead of printing the table, you create a bar chart using the `geom_col()` function.

```{r}
Torn.df |>
  dplyr::filter(Year >= 2007, EF != -9) |>
  dplyr::group_by(EF) |>
  dplyr::summarize(Count = dplyr::n()) |>
ggplot(mapping = aes(x = EF, y = Count)) +
  geom_col()
```

The `geom_bar()` function counts the number of cases at each x position so you don't need the `group_by()` and `summarize()` functions

```{r}
Torn.df |>
  dplyr::filter(Year >= 2007, EF != -9) |>
ggplot(mapping = aes(x = EF)) +
  geom_bar()
```

Improve the bar chart and to make it ready for publication

```{r}
Torn.df |>
  dplyr::filter(Year >= 2007, EF != -9) |>
  dplyr::group_by(EF) |>
  dplyr::summarize(Count = dplyr::n()) |>
ggplot(mapping = aes(x = factor(EF), y = Count, fill = Count)) +
  geom_bar(stat = "identity") +
  xlab("EF Rating") + 
  ylab("Number of Tornadoes") +
  scale_fill_continuous(low = 'green', high = 'orange') +
  geom_text(aes(label = Count), vjust = -.5, size = 3) +
  theme_minimal() +
  theme(legend.position = 'none') 
```

You create a set of plots with the `facet_wrap()` function. Here you create a set of bar charts showing the frequency of tornadoes by EF rating for each year in the data set since 2004

You add the function after the `geom_bar()` layer and use the formula syntax (`~ Year`) inside the parentheses. You interpret the syntax as "plot bar charts conditioned on the variable year"

```{r}
Torn.df |>
  dplyr::filter(Year >= 2004, EF != -9) |>
ggplot(mapping = aes(x = factor(EF))) +
  geom_bar() +
  facet_wrap(~ Year)
```

Example: Comparing hot days in Tallahassee and Las Vegas. The data are [daily weather observations](http://www.ncdc.noaa.gov/cdo-web/datasets) from the Tallahassee International Airport

Import the data

```{r}
loc <- "http://myweb.fsu.edu/jelsner/temp/data/TLH_Daily1940-2021.csv"
TLH.df <- readr::read_csv(file = loc)
```

The variables of interest are the daily high (and low) temperature in the column labeled `TMAX` (`TMIN`). The values are in degrees F

Rename the columns then select only the date and temperature columns

```{r}
TLH.df <- TLH.df |>
  dplyr::rename(TmaxF = TMAX,
                TminF = TMIN,
                Date = DATE) |>
  dplyr::select(Date, TmaxF, TminF) |>
  dplyr::glimpse()
```

Q: Based on these data, is it getting hotter in Tallahassee? Let's compute the annual average high temperature and create a time series graph

You use the `year()` function from the {lubridate} package to get a column called `Year`. Since the data only has values through mid May 2022 you keep all observations with the `Year` column value less than 2021. You then use the `group_by()` function to group by `Year`, and the `summarize()` function to get the average daily maximum temperature for each year

```{r}
df <- TLH.df |>
  dplyr::mutate(Year = lubridate::year(Date)) |>
  dplyr::filter(Year < 2022) |>
  dplyr::group_by(Year) |>
  dplyr::summarize(AvgT = mean(TmaxF, na.rm = TRUE)) |>
dplyr::glimpse()
```

You now have a data frame with two columns: `Year` and `AvgT` (annual average daily high temperature in degrees F). If a day is missing a value it is skipped when computing the average because of the `na.rm = TRUE` argument in the `mean()` function

Next you use functions from the {ggplot2} package to make a time series graph. You specify the x aesthetic as `Year` and the y aesthetic as the `AvgT` and include point and line layers

```{r}
ggplot(data = df, 
       mapping = aes(x = Year, y = AvgT)) +
  geom_point(size = 3) +
  geom_line() +
  ylab("Average Annual Temperature in Tallahassee, FL (F)")
```

You can go directly to the graph without saving the resulting data frame. That is, you pipe `|>` the resulting data frame after applying the {dplyr} verbs to the `ggplot()` function. The object in the first argument of the `ggplot()` function is the result (data frame) from the code above. Here you also add a smooth curve through the set of averages with the `geom_smooth()` layer

```{r}
TLH.df |>
  dplyr::mutate(Year = lubridate::year(Date)) |>
  dplyr::filter(Year < 2022) |>
  dplyr::group_by(Year) |>
  dplyr::summarize(AvgT = mean(TmaxF, na.rm = TRUE)) |>
ggplot(mapping = aes(x = Year, y = AvgT)) +
  geom_point(size = 3) +
  geom_line() +
  ylab("Average Annual Temperature in Tallahassee, FL (F)") +
  geom_smooth() +
  theme_minimal()
```

Q: Is the frequency of extremely hot days increasing over time? Let's consider a daily high temperature of 100 F and above as extremely hot.

Here you count the number of days at or above 100F using the `summarize()` function together with the `sum()` function on the logical operator `>=`. If a day is missing a temperature, you remove it with the `na.rm = TRUE` argument in the `sum()` function

```{r}
TLH.df |>
  dplyr::mutate(Year = lubridate::year(Date)) |>
  dplyr::filter(Year < 2022) |>
  dplyr::group_by(Year) |>
  dplyr::summarize(N100 = sum(TmaxF >= 100, na.rm = TRUE)) |>
ggplot(mapping = aes(x = Year, y = N100, fill = N100)) + 
  geom_bar(stat = 'identity') + 
  scale_fill_continuous(low = 'orange', high = 'red') +
  geom_text(aes(label = N100), vjust = 1.5, size = 3) +
  scale_x_continuous(breaks = seq(1940, 2020, 10)) +
  labs(title = expression(paste("Number of days in Tallahassee, Florida at or above 100", {}^o, " F")),
       subtitle = "",
       x = "", y = "") +
#  ylab(expression(paste("Number of days in Tallahassee, FL at or above 100", {}^o, " F"))) +
  theme_minimal() +
  theme(axis.text.x  = element_text(size = 11), legend.position = "none")
```

What does the histogram of daily high temperatures look like?

```{r}
( gTLH <- ggplot(data = TLH.df, 
               mapping = aes(x = TmaxF)) + 
  geom_histogram(binwidth = 1, aes(fill = ..count..)) +
  scale_fill_continuous(low = 'green', high = 'blue') +
  scale_x_continuous() +
  scale_y_continuous() +
  ylab("Number of Days") + 
  xlab(expression(paste("Daily High Temperature in Tallahassee, FL (", {}^o, " F)"))) +
  theme_minimal() +
  theme(legend.position = "none") )
```

Q: The most common high temperatures are in the low 90s, but there are relatively few 100+ days. Why?

Compare the histogram of daily high temperatures in Tallahassee with a histogram of daily high temperatures in Las Vegas, Nevada. Here you repeat the code above but for the data frame `LVG.df`

```{r}
LVG.df <- readr::read_csv(file = "http://myweb.fsu.edu/jelsner/temp/data/LV_DailySummary.csv",
                          na = "-9999")

LVG.df <- LVG.df |>
  dplyr::mutate(TmaxF = round(9/5 * TMAX/10 + 32),
                TminF = round(9/5 * TMIN/10 + 32),
                Date = as.Date(as.character(DATE), 
                               format = "%Y%m%d")) |>
  dplyr::select(Date, TmaxF, TminF)

gLVG <- ggplot(data = LVG.df, 
               mapping = aes(x = TmaxF)) + 
  geom_histogram(binwidth = 1, aes(fill = ..count..)) +
  scale_fill_continuous(low = 'green', high = 'blue') +
  scale_x_continuous() +
  scale_y_continuous() +
  ylab("Number of Days") + 
  xlab(expression(paste("Daily High Temperature in Las Vegas, NV (", {}^o, " F)"))) +
  theme_minimal() +
  theme(legend.position = "none")
```

Then use the operators from the {patchwork} package to plot them side by side

```{r}
if(!require(patchwork)) install.packages(pkgs = "patchwork", repos = "http://cran.us.r-project.org")

library(patchwork)

gTLH / gLVG
```

Example: US population and area by state. The object `us_states` from the {spData} package is a data frame from the U.S. Census Bureau. The variables include the state `GEOID` and `NAME`, the `REGION` (`South`, `West`, etc), `AREA` (in square km), and total population in 2010 (`total_pop_10`) and in 2015 (`total_pop_15`)

```{r}
us_states <- spData::us_states
class(us_states)
head(us_states)
```

The object `us_states` has two classes: simple feature and data frame. It is a data frame that has spatial information stored in the column labeled `geometry`

Note also that the variable `AREA` is numeric with units (km\^2). Thus in order to perform some operations you need to specify units or convert the column using `as.numeric()`. For example, if you want to filter by area keeping only states with an area greater than 300,000 square km you could do the following

```{r, eval = FALSE}
us_states |> 
  dplyr::mutate(Area = as.numeric(AREA)) |>
  dplyr::filter(Area > 300000)
```

For now, suppose you want to plot area versus population for each state including state names on the plot. You note large differences between the minimum and maximum values for both variables

```{r}
us_states |>
  dplyr::summarize(rA = range(AREA),
                   rP = range(total_pop_15))
```

Start with a simple scatter plot using logarithmic scales. The variable `AREA` has units so you convert it to a numeric with the `as.numeric()` function

```{r}
ggplot(data = us_states,
       mapping = aes(x = as.numeric(AREA),
                     y = total_pop_15)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10()
```

Next use the {scales} package so the tic labels can be expressed in whole numbers with commas

```{r}
ggplot(data = us_states,
       mapping = aes(x = as.numeric(AREA),
                     y = total_pop_15)) +
  geom_point() +
  scale_x_log10(labels = scales::comma) +
  scale_y_log10(labels = scales::comma)
```

Next add text labels. You can do this with `geom_text()` or `geom_label()`

```{r}
ggplot(data = us_states,
       mapping = aes(x = as.numeric(AREA),
                     y = total_pop_15)) +
  geom_point() +
  geom_text(aes(label = NAME)) +
  scale_x_log10(labels = scales::comma) +
  scale_y_log10(labels = scales::comma)
```

The labels are centered on top of the points. To fix this you use functions from the {grepel} package

```{r}
ggplot(data = us_states,
       mapping = aes(x = as.numeric(AREA),
                     y = total_pop_15)) +
  geom_point() +
  ggrepel::geom_text_repel(aes(label = NAME)) +
  scale_x_log10(labels = scales::comma) +
  scale_y_log10(labels = scales::comma)
```

Finally, since the data object is a simple feature data frame you can make a map

```{r}
ggplot() + 
  geom_sf(data = spData::us_states, 
          mapping = aes(fill = total_pop_15)) +
  scale_fill_continuous(labels = scales::comma) +
  theme_void()
```

More resources and additional examples

-   ggplot extensions <https://exts.ggplot2.tidyverse.org/>
-   Cheat sheets: <https://rstudio.com/resources/cheatsheets/>
-   More examples: <https://geocompr.robinlovelace.net/> {spData} package.
